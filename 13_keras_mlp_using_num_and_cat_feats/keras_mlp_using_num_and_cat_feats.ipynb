{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\">Multi-Layer Perceptron (MLP) example in Keras <br> for house sales price prediction<br>using BOTH numerical and categorial features\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    " by Prof. Dr.-Ing. Jürgen Brauer, http://www.juergenbrauer.org\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. Read in the data and deal with \"NaN\" values\n",
    "2. Prepare numerical features\n",
    "3. Prepare categorial features\n",
    "4. Combine numerical and categorial feature columns\n",
    "5. Build and train MLP model\n",
    "6. Testing the trained MLP\n",
    "7. Predicting house prices for the Kaggle competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read in the data and deal with \"NaN\" values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>0</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>0</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>0</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>0</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>0</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave     0      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave     0      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave     0      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave     0      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave     0      IR1   \n",
       "\n",
       "  LandContour Utilities    ...     PoolArea PoolQC Fence MiscFeature MiscVal  \\\n",
       "0         Lvl    AllPub    ...            0      0     0           0       0   \n",
       "1         Lvl    AllPub    ...            0      0     0           0       0   \n",
       "2         Lvl    AllPub    ...            0      0     0           0       0   \n",
       "3         Lvl    AllPub    ...            0      0     0           0       0   \n",
       "4         Lvl    AllPub    ...            0      0     0           0       0   \n",
       "\n",
       "  MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0      2   2008        WD         Normal     208500  \n",
       "1      5   2007        WD         Normal     181500  \n",
       "2      9   2008        WD         Normal     223500  \n",
       "3      2   2006        WD        Abnorml     140000  \n",
       "4     12   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv(\"kaggle_dataset_house_prices/train.csv\")\n",
    "test_data  = pd.read_csv(\"kaggle_dataset_house_prices/test.csv\")\n",
    "\n",
    "train_data.fillna(0, inplace=True)\n",
    "test_data.fillna(0, inplace=True)\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "np.set_printoptions(suppress=True) # do not use scientific \"e\"-notation\n",
    "\n",
    "# 1.\n",
    "# prepare Pandas DataFrames only with numerical columns\n",
    "train_data_num_only = train_data.select_dtypes(exclude=['object'])\n",
    "test_data_num_only  = test_data.select_dtypes(exclude=['object'])\n",
    "\n",
    "# 2.\n",
    "# Throw away \"Id\" and SalePrice\" column for training data\n",
    "train_input_matrix = train_data_num_only.values[:,1:37]\n",
    "train_output_matrix = train_data_num_only.values[:,37]\n",
    "train_output_matrix = train_output_matrix.reshape(-1,1)\n",
    "\n",
    "# 3.\n",
    "# Throw away \"Id\" column for test input matrix\n",
    "test_input_matrix  = test_data_num_only.values[:,1:]\n",
    "\n",
    "# 4.\n",
    "# create a MinMaxScaler with feature range [0,1]\n",
    "# and use it to normalize the train_input_matrix\n",
    "# Then use the SAME normalization to normalize test_data_matrix\n",
    "scaler_input_features = MinMaxScaler(feature_range=(0, 1))\n",
    "normalized_train_input_matrix_feats = scaler_input_features.fit_transform(train_input_matrix)\n",
    "normalized_test_input_matrix_feats = scaler_input_features.transform(test_input_matrix)\n",
    "\n",
    "# 5.\n",
    "# Also create a MinMaxScaler for the train_output_matrix,\n",
    "# which is essentially a column with the final SalePrice\n",
    "scaler_saleprice = MinMaxScaler(feature_range=(0, 1))\n",
    "normalized_train_output_matrix = scaler_saleprice.fit_transform(train_output_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare categorial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSZoning         object\n",
      "Street           object\n",
      "Alley            object\n",
      "LotShape         object\n",
      "LandContour      object\n",
      "Utilities        object\n",
      "LotConfig        object\n",
      "LandSlope        object\n",
      "Neighborhood     object\n",
      "Condition1       object\n",
      "Condition2       object\n",
      "BldgType         object\n",
      "HouseStyle       object\n",
      "RoofStyle        object\n",
      "RoofMatl         object\n",
      "Exterior1st      object\n",
      "Exterior2nd      object\n",
      "MasVnrType       object\n",
      "ExterQual        object\n",
      "ExterCond        object\n",
      "Foundation       object\n",
      "BsmtQual         object\n",
      "BsmtCond         object\n",
      "BsmtExposure     object\n",
      "BsmtFinType1     object\n",
      "BsmtFinType2     object\n",
      "Heating          object\n",
      "HeatingQC        object\n",
      "CentralAir       object\n",
      "Electrical       object\n",
      "KitchenQual      object\n",
      "Functional       object\n",
      "FireplaceQu      object\n",
      "GarageType       object\n",
      "GarageFinish     object\n",
      "GarageQual       object\n",
      "GarageCond       object\n",
      "PavedDrive       object\n",
      "PoolQC           object\n",
      "Fence            object\n",
      "MiscFeature      object\n",
      "SaleType         object\n",
      "SaleCondition    object\n",
      "dtype: object\n",
      "Shape of fused_df is (2919, 43)\n",
      "Shape of fused_df_hot_encoded is (2919, 275)\n"
     ]
    }
   ],
   "source": [
    "# 1.\n",
    "# save the house Ids, since we need them later\n",
    "# for our Kaggle submission\n",
    "test_house_ids = test_data.values[:,0] # get the IDs from the original Pandas DataFrame\n",
    "\n",
    "\n",
    "# 2.\n",
    "# get Pandas data frames without the numerical features\n",
    "gt_saleprice = train_data[\"SalePrice\"]\n",
    "train_data_cats_only = train_data.select_dtypes(exclude=['number'])\n",
    "test_data_cats_only = test_data.select_dtypes(exclude=['number'])\n",
    "\n",
    "\n",
    "# 3.\n",
    "# map each single categorial column\n",
    "# to multiple (one-hot encoded) columns\n",
    "print(train_data_cats_only.dtypes)\n",
    "frames = [train_data_cats_only, test_data_cats_only]\n",
    "fused_df = pd.concat(frames)\n",
    "print(\"Shape of fused_df is\", fused_df.shape)\n",
    "\n",
    "# now do the one-hot encoding\n",
    "fused_df_hot_encoded = pd.get_dummies(fused_df)\n",
    "print(\"Shape of fused_df_hot_encoded is\", fused_df_hot_encoded.shape)\n",
    "\n",
    "# now split the data frame into two data frames again\n",
    "# with 1460 and 1459 rows\n",
    "train_data_hot_encoded = fused_df_hot_encoded[0:1460]\n",
    "test_data_hot_encoded  = fused_df_hot_encoded[1460:]\n",
    "\n",
    "\n",
    "# 4.\n",
    "# prepare NumPy matrices for training\n",
    "# from Pandas DataFrames\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# define how NumPy shall print matrices\n",
    "np.set_printoptions(precision=4)\n",
    "np.set_printoptions(suppress=True) # do not use scientific \"e\"-notation\n",
    "\n",
    "# convert Pandas DataFrame to NumPy matrices\n",
    "# since Keras will expect NumPy matrices\n",
    "train_input_matrix_cat = train_data_hot_encoded.values\n",
    "train_output_matrix    = gt_saleprice.values\n",
    "train_output_matrix    = train_output_matrix.reshape(-1,1)\n",
    "test_input_matrix_cat  = test_data_hot_encoded.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Combine numerical and categorial feature columns\n",
    "\n",
    "We have now a training matrix with numerical features and a training matrix with categorial one-hot encoded features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 36 many numerical features that will be used.\n",
      "\tnormalized_train_input_matrix_feats has shape (1460, 36)\n",
      "\tnormalized_test_input_matrix_feats has shape (1459, 36)\n",
      "There are 275 many categorial features that will be used.\n",
      "\ttrain_input_matrix_cat has shape (1460, 275)\n",
      "\ttrain_input_matrix_cat has shape (1460, 275)\n",
      "all_input_feats_train has shape (1460, 311)\n",
      "all_input_feats_test has shape (1459, 311)\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\", normalized_train_input_matrix_feats.shape[1],\n",
    "      \"many numerical features that will be used.\")\n",
    "print(\"\\tnormalized_train_input_matrix_feats has shape\",\n",
    "      normalized_train_input_matrix_feats.shape)\n",
    "print(\"\\tnormalized_test_input_matrix_feats has shape\",\n",
    "      normalized_test_input_matrix_feats.shape)\n",
    "\n",
    "print(\"There are\", train_input_matrix_cat.shape[1],\n",
    "      \"many categorial features that will be used.\")\n",
    "print(\"\\ttrain_input_matrix_cat has shape\",\n",
    "      train_input_matrix_cat.shape)\n",
    "print(\"\\ttrain_input_matrix_cat has shape\",\n",
    "      train_input_matrix_cat.shape)\n",
    "\n",
    "# concatenate the two input matrices horizontally\n",
    "all_input_feats_train = np.hstack((normalized_train_input_matrix_feats,\n",
    "                                   train_input_matrix_cat))\n",
    "print(\"all_input_feats_train has shape\", all_input_feats_train.shape)\n",
    "all_input_feats_test = np.hstack((normalized_test_input_matrix_feats ,\n",
    "                                   test_input_matrix_cat))\n",
    "print(\"all_input_feats_test has shape\", all_input_feats_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build and train MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input X has shape (1460, 311)\n",
      "Desired output Y has shape (1460, 1)\n",
      "Y:\n",
      " [[0.2411]\n",
      " [0.2036]\n",
      " [0.2619]\n",
      " ...\n",
      " [0.3216]\n",
      " [0.1489]\n",
      " [0.1564]]\n",
      "Train on 1095 samples, validate on 365 samples\n",
      "Epoch 1/500\n",
      "1095/1095 [==============================] - 0s 145us/step - loss: 0.0315 - val_loss: 0.0244\n",
      "Epoch 2/500\n",
      "1095/1095 [==============================] - 0s 80us/step - loss: 0.0210 - val_loss: 0.0197\n",
      "Epoch 3/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 0.0170 - val_loss: 0.0173\n",
      "Epoch 4/500\n",
      "1095/1095 [==============================] - 0s 85us/step - loss: 0.0145 - val_loss: 0.0160\n",
      "Epoch 5/500\n",
      "1095/1095 [==============================] - 0s 55us/step - loss: 0.0127 - val_loss: 0.0145\n",
      "Epoch 6/500\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 0.0113 - val_loss: 0.0134\n",
      "Epoch 7/500\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 0.0103 - val_loss: 0.0127\n",
      "Epoch 8/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 0.0096 - val_loss: 0.0120\n",
      "Epoch 9/500\n",
      "1095/1095 [==============================] - 0s 55us/step - loss: 0.0088 - val_loss: 0.0114\n",
      "Epoch 10/500\n",
      "1095/1095 [==============================] - 0s 55us/step - loss: 0.0082 - val_loss: 0.0111\n",
      "Epoch 11/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 0.0078 - val_loss: 0.0108\n",
      "Epoch 12/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 0.0074 - val_loss: 0.0105\n",
      "Epoch 13/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 0.0071 - val_loss: 0.0099\n",
      "Epoch 14/500\n",
      "1095/1095 [==============================] - 0s 68us/step - loss: 0.0067 - val_loss: 0.0096\n",
      "Epoch 15/500\n",
      "1095/1095 [==============================] - 0s 70us/step - loss: 0.0064 - val_loss: 0.0098\n",
      "Epoch 16/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 0.0062 - val_loss: 0.0094\n",
      "Epoch 17/500\n",
      "1095/1095 [==============================] - 0s 82us/step - loss: 0.0059 - val_loss: 0.0089\n",
      "Epoch 18/500\n",
      "1095/1095 [==============================] - 0s 79us/step - loss: 0.0057 - val_loss: 0.0088\n",
      "Epoch 19/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 0.0055 - val_loss: 0.0087\n",
      "Epoch 20/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 0.0053 - val_loss: 0.0084\n",
      "Epoch 21/500\n",
      "1095/1095 [==============================] - 0s 82us/step - loss: 0.0052 - val_loss: 0.0083\n",
      "Epoch 22/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0050 - val_loss: 0.0084\n",
      "Epoch 23/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0049 - val_loss: 0.0082\n",
      "Epoch 24/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0048 - val_loss: 0.0081\n",
      "Epoch 25/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0047 - val_loss: 0.0078\n",
      "Epoch 26/500\n",
      "1095/1095 [==============================] - 0s 98us/step - loss: 0.0045 - val_loss: 0.0078\n",
      "Epoch 27/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0045 - val_loss: 0.0077\n",
      "Epoch 28/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0043 - val_loss: 0.0076\n",
      "Epoch 29/500\n",
      "1095/1095 [==============================] - 0s 55us/step - loss: 0.0043 - val_loss: 0.0075\n",
      "Epoch 30/500\n",
      "1095/1095 [==============================] - 0s 56us/step - loss: 0.0041 - val_loss: 0.0073\n",
      "Epoch 31/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0041 - val_loss: 0.0073\n",
      "Epoch 32/500\n",
      "1095/1095 [==============================] - 0s 76us/step - loss: 0.0040 - val_loss: 0.0073\n",
      "Epoch 33/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0039 - val_loss: 0.0071\n",
      "Epoch 34/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0039 - val_loss: 0.0071\n",
      "Epoch 35/500\n",
      "1095/1095 [==============================] - 0s 75us/step - loss: 0.0038 - val_loss: 0.0070\n",
      "Epoch 36/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0037 - val_loss: 0.0070\n",
      "Epoch 37/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0037 - val_loss: 0.0069\n",
      "Epoch 38/500\n",
      "1095/1095 [==============================] - 0s 68us/step - loss: 0.0036 - val_loss: 0.0068\n",
      "Epoch 39/500\n",
      "1095/1095 [==============================] - 0s 81us/step - loss: 0.0036 - val_loss: 0.0068\n",
      "Epoch 40/500\n",
      "1095/1095 [==============================] - 0s 58us/step - loss: 0.0035 - val_loss: 0.0070\n",
      "Epoch 41/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 0.0035 - val_loss: 0.0068\n",
      "Epoch 42/500\n",
      "1095/1095 [==============================] - 0s 55us/step - loss: 0.0034 - val_loss: 0.0067\n",
      "Epoch 43/500\n",
      "1095/1095 [==============================] - 0s 76us/step - loss: 0.0034 - val_loss: 0.0066\n",
      "Epoch 44/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0033 - val_loss: 0.0066\n",
      "Epoch 45/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0033 - val_loss: 0.0066\n",
      "Epoch 46/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0032 - val_loss: 0.0068\n",
      "Epoch 47/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0032 - val_loss: 0.0065\n",
      "Epoch 48/500\n",
      "1095/1095 [==============================] - 0s 56us/step - loss: 0.0031 - val_loss: 0.0064\n",
      "Epoch 49/500\n",
      "1095/1095 [==============================] - 0s 57us/step - loss: 0.0031 - val_loss: 0.0065\n",
      "Epoch 50/500\n",
      "1095/1095 [==============================] - 0s 79us/step - loss: 0.0031 - val_loss: 0.0063\n",
      "Epoch 51/500\n",
      "1095/1095 [==============================] - 0s 65us/step - loss: 0.0030 - val_loss: 0.0063\n",
      "Epoch 52/500\n",
      "1095/1095 [==============================] - 0s 67us/step - loss: 0.0030 - val_loss: 0.0064\n",
      "Epoch 53/500\n",
      "1095/1095 [==============================] - 0s 69us/step - loss: 0.0030 - val_loss: 0.0062\n",
      "Epoch 54/500\n",
      "1095/1095 [==============================] - 0s 71us/step - loss: 0.0029 - val_loss: 0.0062\n",
      "Epoch 55/500\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 0.0029 - val_loss: 0.0062\n",
      "Epoch 56/500\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 0.0029 - val_loss: 0.0062\n",
      "Epoch 57/500\n",
      "1095/1095 [==============================] - 0s 70us/step - loss: 0.0028 - val_loss: 0.0062\n",
      "Epoch 58/500\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 0.0028 - val_loss: 0.0062\n",
      "Epoch 59/500\n",
      "1095/1095 [==============================] - 0s 65us/step - loss: 0.0028 - val_loss: 0.0061\n",
      "Epoch 60/500\n",
      "1095/1095 [==============================] - 0s 70us/step - loss: 0.0028 - val_loss: 0.0061\n",
      "Epoch 61/500\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 0.0027 - val_loss: 0.0062\n",
      "Epoch 62/500\n",
      "1095/1095 [==============================] - 0s 78us/step - loss: 0.0027 - val_loss: 0.0060\n",
      "Epoch 63/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 0.0027 - val_loss: 0.0059\n",
      "Epoch 64/500\n",
      "1095/1095 [==============================] - 0s 65us/step - loss: 0.0027 - val_loss: 0.0059\n",
      "Epoch 65/500\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 0.0026 - val_loss: 0.0060\n",
      "Epoch 66/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 0.0026 - val_loss: 0.0059\n",
      "Epoch 67/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 0.0026 - val_loss: 0.0059\n",
      "Epoch 68/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 0.0025 - val_loss: 0.0060\n",
      "Epoch 69/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 0.0025 - val_loss: 0.0058\n",
      "Epoch 70/500\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 0.0025 - val_loss: 0.0058\n",
      "Epoch 71/500\n",
      "1095/1095 [==============================] - 0s 69us/step - loss: 0.0025 - val_loss: 0.0058\n",
      "Epoch 72/500\n",
      "1095/1095 [==============================] - 0s 66us/step - loss: 0.0025 - val_loss: 0.0058\n",
      "Epoch 73/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 0.0024 - val_loss: 0.0057\n",
      "Epoch 74/500\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 0.0024 - val_loss: 0.0057\n",
      "Epoch 75/500\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 0.0024 - val_loss: 0.0057\n",
      "Epoch 76/500\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 0.0024 - val_loss: 0.0057\n",
      "Epoch 77/500\n",
      "1095/1095 [==============================] - 0s 70us/step - loss: 0.0024 - val_loss: 0.0057\n",
      "Epoch 78/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 0.0023 - val_loss: 0.0057\n",
      "Epoch 79/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0023 - val_loss: 0.0057\n",
      "Epoch 80/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 0.0023 - val_loss: 0.0056\n",
      "Epoch 81/500\n",
      "1095/1095 [==============================] - 0s 78us/step - loss: 0.0023 - val_loss: 0.0056\n",
      "Epoch 82/500\n",
      "1095/1095 [==============================] - 0s 78us/step - loss: 0.0023 - val_loss: 0.0056\n",
      "Epoch 83/500\n",
      "1095/1095 [==============================] - 0s 60us/step - loss: 0.0022 - val_loss: 0.0056\n",
      "Epoch 84/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0022 - val_loss: 0.0055\n",
      "Epoch 85/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0022 - val_loss: 0.0055\n",
      "Epoch 86/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0022 - val_loss: 0.0056\n",
      "Epoch 87/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 0.0022 - val_loss: 0.0055\n",
      "Epoch 88/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0022 - val_loss: 0.0055\n",
      "Epoch 89/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 0.0022 - val_loss: 0.0055\n",
      "Epoch 90/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 0.0021 - val_loss: 0.0055\n",
      "Epoch 91/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 0.0021 - val_loss: 0.0055\n",
      "Epoch 92/500\n",
      "1095/1095 [==============================] - 0s 60us/step - loss: 0.0021 - val_loss: 0.0055\n",
      "Epoch 93/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0021 - val_loss: 0.0055\n",
      "Epoch 94/500\n",
      "1095/1095 [==============================] - 0s 73us/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 95/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 96/500\n",
      "1095/1095 [==============================] - 0s 72us/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 97/500\n",
      "1095/1095 [==============================] - 0s 87us/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 98/500\n",
      "1095/1095 [==============================] - 0s 74us/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 99/500\n",
      "1095/1095 [==============================] - 0s 69us/step - loss: 0.0020 - val_loss: 0.0053\n",
      "Epoch 100/500\n",
      "1095/1095 [==============================] - 0s 67us/step - loss: 0.0020 - val_loss: 0.0053\n",
      "Epoch 101/500\n",
      "1095/1095 [==============================] - 0s 65us/step - loss: 0.0020 - val_loss: 0.0053\n",
      "Epoch 102/500\n",
      "1095/1095 [==============================] - 0s 71us/step - loss: 0.0020 - val_loss: 0.0053\n",
      "Epoch 103/500\n",
      "1095/1095 [==============================] - 0s 65us/step - loss: 0.0020 - val_loss: 0.0053\n",
      "Epoch 104/500\n",
      "1095/1095 [==============================] - 0s 55us/step - loss: 0.0019 - val_loss: 0.0053\n",
      "Epoch 105/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 0.0019 - val_loss: 0.0053\n",
      "Epoch 106/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 0.0019 - val_loss: 0.0053\n",
      "Epoch 107/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 0.0019 - val_loss: 0.0053\n",
      "Epoch 108/500\n",
      "1095/1095 [==============================] - 0s 55us/step - loss: 0.0019 - val_loss: 0.0052\n",
      "Epoch 109/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 0.0019 - val_loss: 0.0052\n",
      "Epoch 110/500\n",
      "1095/1095 [==============================] - 0s 60us/step - loss: 0.0019 - val_loss: 0.0053\n",
      "Epoch 111/500\n",
      "1095/1095 [==============================] - ETA: 0s - loss: 0.001 - 0s 59us/step - loss: 0.0019 - val_loss: 0.0052\n",
      "Epoch 112/500\n",
      "1095/1095 [==============================] - 0s 87us/step - loss: 0.0019 - val_loss: 0.0052\n",
      "Epoch 113/500\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 0.0018 - val_loss: 0.0052\n",
      "Epoch 114/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 0.0018 - val_loss: 0.0052\n",
      "Epoch 115/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 0.0018 - val_loss: 0.0052\n",
      "Epoch 116/500\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 0.0018 - val_loss: 0.0052\n",
      "Epoch 117/500\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 0.0018 - val_loss: 0.0052\n",
      "Epoch 118/500\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 0.0018 - val_loss: 0.0052\n",
      "Epoch 119/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 0.0018 - val_loss: 0.0052\n",
      "Epoch 120/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0018 - val_loss: 0.0051\n",
      "Epoch 121/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0018 - val_loss: 0.0051\n",
      "Epoch 122/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0018 - val_loss: 0.0051\n",
      "Epoch 123/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0018 - val_loss: 0.0051\n",
      "Epoch 124/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0017 - val_loss: 0.0052\n",
      "Epoch 125/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0017 - val_loss: 0.0052\n",
      "Epoch 126/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0017 - val_loss: 0.0051\n",
      "Epoch 127/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0017 - val_loss: 0.0052\n",
      "Epoch 128/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0017 - val_loss: 0.0051\n",
      "Epoch 129/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0017 - val_loss: 0.0051\n",
      "Epoch 130/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0017 - val_loss: 0.0051\n",
      "Epoch 131/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0017 - val_loss: 0.0050\n",
      "Epoch 132/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0017 - val_loss: 0.0051\n",
      "Epoch 133/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0017 - val_loss: 0.0051\n",
      "Epoch 134/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 0.0017 - val_loss: 0.0051\n",
      "Epoch 135/500\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 0.0016 - val_loss: 0.0050\n",
      "Epoch 136/500\n",
      "1095/1095 [==============================] - 0s 58us/step - loss: 0.0016 - val_loss: 0.0050\n",
      "Epoch 137/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0016 - val_loss: 0.0051\n",
      "Epoch 138/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0016 - val_loss: 0.0050\n",
      "Epoch 139/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0016 - val_loss: 0.0050\n",
      "Epoch 140/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0016 - val_loss: 0.0050\n",
      "Epoch 141/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0016 - val_loss: 0.0050\n",
      "Epoch 142/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0016 - val_loss: 0.0050\n",
      "Epoch 143/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0016 - val_loss: 0.0051\n",
      "Epoch 144/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0016 - val_loss: 0.0050\n",
      "Epoch 145/500\n",
      "1095/1095 [==============================] - 0s 60us/step - loss: 0.0016 - val_loss: 0.0050\n",
      "Epoch 146/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0016 - val_loss: 0.0050\n",
      "Epoch 147/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0016 - val_loss: 0.0052\n",
      "Epoch 148/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0016 - val_loss: 0.0050\n",
      "Epoch 149/500\n",
      "1095/1095 [==============================] - 0s 58us/step - loss: 0.0015 - val_loss: 0.0050\n",
      "Epoch 150/500\n",
      "1095/1095 [==============================] - 0s 70us/step - loss: 0.0015 - val_loss: 0.0050\n",
      "Epoch 151/500\n",
      "1095/1095 [==============================] - 0s 74us/step - loss: 0.0015 - val_loss: 0.0050\n",
      "Epoch 152/500\n",
      "1095/1095 [==============================] - 0s 68us/step - loss: 0.0015 - val_loss: 0.0050\n",
      "Epoch 153/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 0.0015 - val_loss: 0.0049\n",
      "Epoch 154/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1095/1095 [==============================] - 0s 67us/step - loss: 0.0015 - val_loss: 0.0049\n",
      "Epoch 155/500\n",
      "1095/1095 [==============================] - 0s 68us/step - loss: 0.0015 - val_loss: 0.0049\n",
      "Epoch 156/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0015 - val_loss: 0.0049\n",
      "Epoch 157/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0015 - val_loss: 0.0049\n",
      "Epoch 158/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0015 - val_loss: 0.0049\n",
      "Epoch 159/500\n",
      "1095/1095 [==============================] - 0s 56us/step - loss: 0.0015 - val_loss: 0.0049\n",
      "Epoch 160/500\n",
      "1095/1095 [==============================] - 0s 93us/step - loss: 0.0015 - val_loss: 0.0049\n",
      "Epoch 161/500\n",
      "1095/1095 [==============================] - 0s 55us/step - loss: 0.0015 - val_loss: 0.0049\n",
      "Epoch 162/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0015 - val_loss: 0.0049\n",
      "Epoch 163/500\n",
      "1095/1095 [==============================] - 0s 99us/step - loss: 0.0014 - val_loss: 0.0049\n",
      "Epoch 164/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0014 - val_loss: 0.0049\n",
      "Epoch 165/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0014 - val_loss: 0.0049\n",
      "Epoch 166/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0014 - val_loss: 0.0049\n",
      "Epoch 167/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0014 - val_loss: 0.0049\n",
      "Epoch 168/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0014 - val_loss: 0.0049\n",
      "Epoch 169/500\n",
      "1095/1095 [==============================] - 0s 73us/step - loss: 0.0014 - val_loss: 0.0049\n",
      "Epoch 170/500\n",
      "1095/1095 [==============================] - 0s 78us/step - loss: 0.0014 - val_loss: 0.0049\n",
      "Epoch 171/500\n",
      "1095/1095 [==============================] - 0s 55us/step - loss: 0.0014 - val_loss: 0.0049\n",
      "Epoch 172/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0014 - val_loss: 0.0049\n",
      "Epoch 173/500\n",
      "1095/1095 [==============================] - 0s 58us/step - loss: 0.0014 - val_loss: 0.0048\n",
      "Epoch 174/500\n",
      "1095/1095 [==============================] - 0s 81us/step - loss: 0.0014 - val_loss: 0.0048\n",
      "Epoch 175/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0014 - val_loss: 0.0049\n",
      "Epoch 176/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0014 - val_loss: 0.0048\n",
      "Epoch 177/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0014 - val_loss: 0.0048\n",
      "Epoch 178/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 0.0014 - val_loss: 0.0048\n",
      "Epoch 179/500\n",
      "1095/1095 [==============================] - 0s 78us/step - loss: 0.0014 - val_loss: 0.0048\n",
      "Epoch 180/500\n",
      "1095/1095 [==============================] - 0s 76us/step - loss: 0.0013 - val_loss: 0.0049\n",
      "Epoch 181/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0013 - val_loss: 0.0048\n",
      "Epoch 182/500\n",
      "1095/1095 [==============================] - 0s 76us/step - loss: 0.0013 - val_loss: 0.0048\n",
      "Epoch 183/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0013 - val_loss: 0.0049\n",
      "Epoch 184/500\n",
      "1095/1095 [==============================] - 0s 71us/step - loss: 0.0013 - val_loss: 0.0048\n",
      "Epoch 185/500\n",
      "1095/1095 [==============================] - 0s 55us/step - loss: 0.0013 - val_loss: 0.0049\n",
      "Epoch 186/500\n",
      "1095/1095 [==============================] - 0s 58us/step - loss: 0.0013 - val_loss: 0.0048\n",
      "Epoch 187/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0013 - val_loss: 0.0048\n",
      "Epoch 188/500\n",
      "1095/1095 [==============================] - 0s 75us/step - loss: 0.0013 - val_loss: 0.0048\n",
      "Epoch 189/500\n",
      "1095/1095 [==============================] - 0s 55us/step - loss: 0.0013 - val_loss: 0.0048\n",
      "Epoch 190/500\n",
      "1095/1095 [==============================] - 0s 58us/step - loss: 0.0013 - val_loss: 0.0048\n",
      "Epoch 191/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 0.0013 - val_loss: 0.0048\n",
      "Epoch 192/500\n",
      "1095/1095 [==============================] - 0s 82us/step - loss: 0.0013 - val_loss: 0.0048\n",
      "Epoch 193/500\n",
      "1095/1095 [==============================] - 0s 77us/step - loss: 0.0013 - val_loss: 0.0048\n",
      "Epoch 194/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0013 - val_loss: 0.0048\n",
      "Epoch 195/500\n",
      "1095/1095 [==============================] - 0s 76us/step - loss: 0.0013 - val_loss: 0.0048\n",
      "Epoch 196/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0013 - val_loss: 0.0048\n",
      "Epoch 197/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0013 - val_loss: 0.0048\n",
      "Epoch 198/500\n",
      "1095/1095 [==============================] - 0s 96us/step - loss: 0.0013 - val_loss: 0.0048\n",
      "Epoch 199/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0013 - val_loss: 0.0047\n",
      "Epoch 200/500\n",
      "1095/1095 [==============================] - 0s 55us/step - loss: 0.0013 - val_loss: 0.0047\n",
      "Epoch 201/500\n",
      "1095/1095 [==============================] - 0s 67us/step - loss: 0.0013 - val_loss: 0.0048\n",
      "Epoch 202/500\n",
      "1095/1095 [==============================] - 0s 66us/step - loss: 0.0013 - val_loss: 0.0047\n",
      "Epoch 203/500\n",
      "1095/1095 [==============================] - 0s 73us/step - loss: 0.0012 - val_loss: 0.0047\n",
      "Epoch 204/500\n",
      "1095/1095 [==============================] - 0s 79us/step - loss: 0.0012 - val_loss: 0.0047\n",
      "Epoch 205/500\n",
      "1095/1095 [==============================] - 0s 65us/step - loss: 0.0012 - val_loss: 0.0047\n",
      "Epoch 206/500\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 0.0012 - val_loss: 0.0048\n",
      "Epoch 207/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 0.0012 - val_loss: 0.0047\n",
      "Epoch 208/500\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 0.0012 - val_loss: 0.0047\n",
      "Epoch 209/500\n",
      "1095/1095 [==============================] - 0s 71us/step - loss: 0.0012 - val_loss: 0.0047\n",
      "Epoch 210/500\n",
      "1095/1095 [==============================] - 0s 79us/step - loss: 0.0012 - val_loss: 0.0047\n",
      "Epoch 211/500\n",
      "1095/1095 [==============================] - 0s 65us/step - loss: 0.0012 - val_loss: 0.0047\n",
      "Epoch 212/500\n",
      "1095/1095 [==============================] - 0s 67us/step - loss: 0.0012 - val_loss: 0.0047\n",
      "Epoch 213/500\n",
      "1095/1095 [==============================] - 0s 74us/step - loss: 0.0012 - val_loss: 0.0047\n",
      "Epoch 214/500\n",
      "1095/1095 [==============================] - 0s 74us/step - loss: 0.0012 - val_loss: 0.0048\n",
      "Epoch 215/500\n",
      "1095/1095 [==============================] - 0s 80us/step - loss: 0.0012 - val_loss: 0.0047\n",
      "Epoch 216/500\n",
      "1095/1095 [==============================] - 0s 71us/step - loss: 0.0012 - val_loss: 0.0047\n",
      "Epoch 217/500\n",
      "1095/1095 [==============================] - 0s 65us/step - loss: 0.0012 - val_loss: 0.0047\n",
      "Epoch 218/500\n",
      "1095/1095 [==============================] - 0s 73us/step - loss: 0.0012 - val_loss: 0.0047\n",
      "Epoch 219/500\n",
      "1095/1095 [==============================] - 0s 66us/step - loss: 0.0012 - val_loss: 0.0047\n",
      "Epoch 220/500\n",
      "1095/1095 [==============================] - 0s 72us/step - loss: 0.0012 - val_loss: 0.0047\n",
      "Epoch 221/500\n",
      "1095/1095 [==============================] - 0s 65us/step - loss: 0.0012 - val_loss: 0.0047\n",
      "Epoch 222/500\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 0.0012 - val_loss: 0.0047\n",
      "Epoch 223/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 0.0012 - val_loss: 0.0047\n",
      "Epoch 224/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0012 - val_loss: 0.0047\n",
      "Epoch 225/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0012 - val_loss: 0.0046\n",
      "Epoch 226/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0012 - val_loss: 0.0046\n",
      "Epoch 227/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0011 - val_loss: 0.0047\n",
      "Epoch 228/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0011 - val_loss: 0.0047\n",
      "Epoch 229/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0011 - val_loss: 0.0046\n",
      "Epoch 230/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0011 - val_loss: 0.0046\n",
      "Epoch 231/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0011 - val_loss: 0.0047\n",
      "Epoch 232/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0011 - val_loss: 0.0048\n",
      "Epoch 233/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0011 - val_loss: 0.0046\n",
      "Epoch 234/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0011 - val_loss: 0.0046\n",
      "Epoch 235/500\n",
      "1095/1095 [==============================] - 0s 93us/step - loss: 0.0011 - val_loss: 0.0048\n",
      "Epoch 236/500\n",
      "1095/1095 [==============================] - 0s 92us/step - loss: 0.0011 - val_loss: 0.0046\n",
      "Epoch 237/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0011 - val_loss: 0.0047\n",
      "Epoch 238/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0011 - val_loss: 0.0046\n",
      "Epoch 239/500\n",
      "1095/1095 [==============================] - 0s 56us/step - loss: 0.0011 - val_loss: 0.0046\n",
      "Epoch 240/500\n",
      "1095/1095 [==============================] - 0s 84us/step - loss: 0.0011 - val_loss: 0.0047\n",
      "Epoch 241/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0011 - val_loss: 0.0046\n",
      "Epoch 242/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0011 - val_loss: 0.0046\n",
      "Epoch 243/500\n",
      "1095/1095 [==============================] - 0s 72us/step - loss: 0.0011 - val_loss: 0.0046\n",
      "Epoch 244/500\n",
      "1095/1095 [==============================] - 0s 106us/step - loss: 0.0011 - val_loss: 0.0046\n",
      "Epoch 245/500\n",
      "1095/1095 [==============================] - 0s 77us/step - loss: 0.0011 - val_loss: 0.0046\n",
      "Epoch 246/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0011 - val_loss: 0.0047\n",
      "Epoch 247/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0011 - val_loss: 0.0046\n",
      "Epoch 248/500\n",
      "1095/1095 [==============================] - 0s 71us/step - loss: 0.0011 - val_loss: 0.0046\n",
      "Epoch 249/500\n",
      "1095/1095 [==============================] - 0s 72us/step - loss: 0.0011 - val_loss: 0.0046\n",
      "Epoch 250/500\n",
      "1095/1095 [==============================] - 0s 67us/step - loss: 0.0011 - val_loss: 0.0046\n",
      "Epoch 251/500\n",
      "1095/1095 [==============================] - 0s 70us/step - loss: 0.0011 - val_loss: 0.0047\n",
      "Epoch 252/500\n",
      "1095/1095 [==============================] - 0s 71us/step - loss: 0.0011 - val_loss: 0.0046\n",
      "Epoch 253/500\n",
      "1095/1095 [==============================] - 0s 77us/step - loss: 0.0011 - val_loss: 0.0046\n",
      "Epoch 254/500\n",
      "1095/1095 [==============================] - 0s 71us/step - loss: 0.0011 - val_loss: 0.0046\n",
      "Epoch 255/500\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 0.0011 - val_loss: 0.0046\n",
      "Epoch 256/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 0.0010 - val_loss: 0.0046\n",
      "Epoch 257/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0011 - val_loss: 0.0046\n",
      "Epoch 258/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0010 - val_loss: 0.0046\n",
      "Epoch 259/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0010 - val_loss: 0.0046\n",
      "Epoch 260/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0010 - val_loss: 0.0046\n",
      "Epoch 261/500\n",
      "1095/1095 [==============================] - 0s 75us/step - loss: 0.0010 - val_loss: 0.0046\n",
      "Epoch 262/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0010 - val_loss: 0.0046\n",
      "Epoch 263/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0010 - val_loss: 0.0046\n",
      "Epoch 264/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0010 - val_loss: 0.0046\n",
      "Epoch 265/500\n",
      "1095/1095 [==============================] - 0s 94us/step - loss: 0.0010 - val_loss: 0.0045\n",
      "Epoch 266/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0010 - val_loss: 0.0046\n",
      "Epoch 267/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0010 - val_loss: 0.0045\n",
      "Epoch 268/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0010 - val_loss: 0.0046\n",
      "Epoch 269/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 0.0010 - val_loss: 0.0045\n",
      "Epoch 270/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0010 - val_loss: 0.0045\n",
      "Epoch 271/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0010 - val_loss: 0.0046\n",
      "Epoch 272/500\n",
      "1095/1095 [==============================] - 0s 84us/step - loss: 0.0010 - val_loss: 0.0045\n",
      "Epoch 273/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 9.9980e-04 - val_loss: 0.0045\n",
      "Epoch 274/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0010 - val_loss: 0.0045\n",
      "Epoch 275/500\n",
      "1095/1095 [==============================] - 0s 66us/step - loss: 9.9167e-04 - val_loss: 0.0046\n",
      "Epoch 276/500\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 9.9630e-04 - val_loss: 0.0046\n",
      "Epoch 277/500\n",
      "1095/1095 [==============================] - 0s 74us/step - loss: 9.9535e-04 - val_loss: 0.0045\n",
      "Epoch 278/500\n",
      "1095/1095 [==============================] - 0s 75us/step - loss: 9.8547e-04 - val_loss: 0.0045\n",
      "Epoch 279/500\n",
      "1095/1095 [==============================] - 0s 78us/step - loss: 9.8152e-04 - val_loss: 0.0045\n",
      "Epoch 280/500\n",
      "1095/1095 [==============================] - 0s 58us/step - loss: 9.8010e-04 - val_loss: 0.0045\n",
      "Epoch 281/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 9.8295e-04 - val_loss: 0.0045\n",
      "Epoch 282/500\n",
      "1095/1095 [==============================] - 0s 69us/step - loss: 9.7450e-04 - val_loss: 0.0045\n",
      "Epoch 283/500\n",
      "1095/1095 [==============================] - 0s 60us/step - loss: 9.7353e-04 - val_loss: 0.0045\n",
      "Epoch 284/500\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 9.7017e-04 - val_loss: 0.0046\n",
      "Epoch 285/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 9.7489e-04 - val_loss: 0.0046\n",
      "Epoch 286/500\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 9.7206e-04 - val_loss: 0.0045\n",
      "Epoch 287/500\n",
      "1095/1095 [==============================] - 0s 60us/step - loss: 9.6602e-04 - val_loss: 0.0045\n",
      "Epoch 288/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 9.5539e-04 - val_loss: 0.0045\n",
      "Epoch 289/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 9.5775e-04 - val_loss: 0.0045\n",
      "Epoch 290/500\n",
      "1095/1095 [==============================] - 0s 58us/step - loss: 9.5506e-04 - val_loss: 0.0045\n",
      "Epoch 291/500\n",
      "1095/1095 [==============================] - 0s 58us/step - loss: 9.5387e-04 - val_loss: 0.0045\n",
      "Epoch 292/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 9.5429e-04 - val_loss: 0.0045\n",
      "Epoch 293/500\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 9.4254e-04 - val_loss: 0.0045\n",
      "Epoch 294/500\n",
      "1095/1095 [==============================] - 0s 65us/step - loss: 9.4644e-04 - val_loss: 0.0045\n",
      "Epoch 295/500\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 9.4221e-04 - val_loss: 0.0045\n",
      "Epoch 296/500\n",
      "1095/1095 [==============================] - 0s 67us/step - loss: 9.4039e-04 - val_loss: 0.0045\n",
      "Epoch 297/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 9.3784e-04 - val_loss: 0.0045\n",
      "Epoch 298/500\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 9.3569e-04 - val_loss: 0.0045\n",
      "Epoch 299/500\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 9.4070e-04 - val_loss: 0.0045\n",
      "Epoch 300/500\n",
      "1095/1095 [==============================] - 0s 60us/step - loss: 9.3807e-04 - val_loss: 0.0045\n",
      "Epoch 301/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 9.3294e-04 - val_loss: 0.0045\n",
      "Epoch 302/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 9.2317e-04 - val_loss: 0.0045\n",
      "Epoch 303/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 9.1877e-04 - val_loss: 0.0045\n",
      "Epoch 304/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 9.2603e-04 - val_loss: 0.0045\n",
      "Epoch 305/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 9.2407e-04 - val_loss: 0.0045\n",
      "Epoch 306/500\n",
      "1095/1095 [==============================] - 0s 60us/step - loss: 9.1661e-04 - val_loss: 0.0045\n",
      "Epoch 307/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1095/1095 [==============================] - 0s 62us/step - loss: 9.1712e-04 - val_loss: 0.0045\n",
      "Epoch 308/500\n",
      "1095/1095 [==============================] - 0s 56us/step - loss: 9.1476e-04 - val_loss: 0.0045\n",
      "Epoch 309/500\n",
      "1095/1095 [==============================] - 0s 69us/step - loss: 9.0845e-04 - val_loss: 0.0045\n",
      "Epoch 310/500\n",
      "1095/1095 [==============================] - 0s 56us/step - loss: 9.1465e-04 - val_loss: 0.0045\n",
      "Epoch 311/500\n",
      "1095/1095 [==============================] - 0s 55us/step - loss: 9.0685e-04 - val_loss: 0.0045\n",
      "Epoch 312/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 9.1326e-04 - val_loss: 0.0045\n",
      "Epoch 313/500\n",
      "1095/1095 [==============================] - 0s 67us/step - loss: 9.0504e-04 - val_loss: 0.0045\n",
      "Epoch 314/500\n",
      "1095/1095 [==============================] - 0s 89us/step - loss: 8.9912e-04 - val_loss: 0.0045\n",
      "Epoch 315/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 8.9525e-04 - val_loss: 0.0045\n",
      "Epoch 316/500\n",
      "1095/1095 [==============================] - 0s 76us/step - loss: 8.9642e-04 - val_loss: 0.0044\n",
      "Epoch 317/500\n",
      "1095/1095 [==============================] - 0s 55us/step - loss: 8.9463e-04 - val_loss: 0.0044\n",
      "Epoch 318/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 8.8646e-04 - val_loss: 0.0045\n",
      "Epoch 319/500\n",
      "1095/1095 [==============================] - 0s 67us/step - loss: 8.9244e-04 - val_loss: 0.0045\n",
      "Epoch 320/500\n",
      "1095/1095 [==============================] - 0s 65us/step - loss: 8.8715e-04 - val_loss: 0.0045\n",
      "Epoch 321/500\n",
      "1095/1095 [==============================] - 0s 72us/step - loss: 8.9324e-04 - val_loss: 0.0044\n",
      "Epoch 322/500\n",
      "1095/1095 [==============================] - 0s 85us/step - loss: 8.8078e-04 - val_loss: 0.0045\n",
      "Epoch 323/500\n",
      "1095/1095 [==============================] - 0s 79us/step - loss: 8.7875e-04 - val_loss: 0.0044\n",
      "Epoch 324/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 8.7764e-04 - val_loss: 0.0044\n",
      "Epoch 325/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 8.8118e-04 - val_loss: 0.0044\n",
      "Epoch 326/500\n",
      "1095/1095 [==============================] - 0s 84us/step - loss: 8.7396e-04 - val_loss: 0.0044\n",
      "Epoch 327/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 8.7099e-04 - val_loss: 0.0044\n",
      "Epoch 328/500\n",
      "1095/1095 [==============================] - 0s 75us/step - loss: 8.7174e-04 - val_loss: 0.0044\n",
      "Epoch 329/500\n",
      "1095/1095 [==============================] - 0s 72us/step - loss: 8.6389e-04 - val_loss: 0.0045\n",
      "Epoch 330/500\n",
      "1095/1095 [==============================] - 0s 71us/step - loss: 8.6329e-04 - val_loss: 0.0044\n",
      "Epoch 331/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 8.6339e-04 - val_loss: 0.0044\n",
      "Epoch 332/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 8.6258e-04 - val_loss: 0.0044\n",
      "Epoch 333/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 8.6135e-04 - val_loss: 0.0044\n",
      "Epoch 334/500\n",
      "1095/1095 [==============================] - 0s 76us/step - loss: 8.5459e-04 - val_loss: 0.0045\n",
      "Epoch 335/500\n",
      "1095/1095 [==============================] - 0s 96us/step - loss: 8.6496e-04 - val_loss: 0.0044\n",
      "Epoch 336/500\n",
      "1095/1095 [==============================] - 0s 78us/step - loss: 8.5056e-04 - val_loss: 0.0044\n",
      "Epoch 337/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 8.5379e-04 - val_loss: 0.0044\n",
      "Epoch 338/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 8.4768e-04 - val_loss: 0.0044\n",
      "Epoch 339/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 8.4304e-04 - val_loss: 0.0044\n",
      "Epoch 340/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 8.4813e-04 - val_loss: 0.0044\n",
      "Epoch 341/500\n",
      "1095/1095 [==============================] - 0s 74us/step - loss: 8.4437e-04 - val_loss: 0.0044\n",
      "Epoch 342/500\n",
      "1095/1095 [==============================] - 0s 72us/step - loss: 8.4207e-04 - val_loss: 0.0044\n",
      "Epoch 343/500\n",
      "1095/1095 [==============================] - 0s 78us/step - loss: 8.4059e-04 - val_loss: 0.0044\n",
      "Epoch 344/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 8.3493e-04 - val_loss: 0.0044\n",
      "Epoch 345/500\n",
      "1095/1095 [==============================] - 0s 75us/step - loss: 8.3698e-04 - val_loss: 0.0044\n",
      "Epoch 346/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 8.3228e-04 - val_loss: 0.0044\n",
      "Epoch 347/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 8.3013e-04 - val_loss: 0.0044\n",
      "Epoch 348/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 8.3154e-04 - val_loss: 0.0044\n",
      "Epoch 349/500\n",
      "1095/1095 [==============================] - 0s 92us/step - loss: 8.2866e-04 - val_loss: 0.0044\n",
      "Epoch 350/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 8.2440e-04 - val_loss: 0.0044\n",
      "Epoch 351/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 8.2404e-04 - val_loss: 0.0044\n",
      "Epoch 352/500\n",
      "1095/1095 [==============================] - 0s 77us/step - loss: 8.2153e-04 - val_loss: 0.0044\n",
      "Epoch 353/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 8.2053e-04 - val_loss: 0.0044\n",
      "Epoch 354/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 8.1846e-04 - val_loss: 0.0044\n",
      "Epoch 355/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 8.1275e-04 - val_loss: 0.0046\n",
      "Epoch 356/500\n",
      "1095/1095 [==============================] - 0s 74us/step - loss: 8.2079e-04 - val_loss: 0.0044\n",
      "Epoch 357/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 8.1445e-04 - val_loss: 0.0044\n",
      "Epoch 358/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 8.1513e-04 - val_loss: 0.0044\n",
      "Epoch 359/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 8.0819e-04 - val_loss: 0.0044\n",
      "Epoch 360/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 8.0739e-04 - val_loss: 0.0044\n",
      "Epoch 361/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 8.0461e-04 - val_loss: 0.0044\n",
      "Epoch 362/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 8.0211e-04 - val_loss: 0.0044\n",
      "Epoch 363/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 7.9989e-04 - val_loss: 0.0044\n",
      "Epoch 364/500\n",
      "1095/1095 [==============================] - 0s 74us/step - loss: 7.9581e-04 - val_loss: 0.0044\n",
      "Epoch 365/500\n",
      "1095/1095 [==============================] - 0s 88us/step - loss: 8.0009e-04 - val_loss: 0.0044\n",
      "Epoch 366/500\n",
      "1095/1095 [==============================] - 0s 58us/step - loss: 7.9624e-04 - val_loss: 0.0044\n",
      "Epoch 367/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 7.9996e-04 - val_loss: 0.0044\n",
      "Epoch 368/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 7.8943e-04 - val_loss: 0.0044\n",
      "Epoch 369/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 7.8822e-04 - val_loss: 0.0044\n",
      "Epoch 370/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 7.9044e-04 - val_loss: 0.0044\n",
      "Epoch 371/500\n",
      "1095/1095 [==============================] - 0s 75us/step - loss: 7.9247e-04 - val_loss: 0.0044\n",
      "Epoch 372/500\n",
      "1095/1095 [==============================] - 0s 58us/step - loss: 7.8530e-04 - val_loss: 0.0044\n",
      "Epoch 373/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 7.8434e-04 - val_loss: 0.0044\n",
      "Epoch 374/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 7.8588e-04 - val_loss: 0.0044\n",
      "Epoch 375/500\n",
      "1095/1095 [==============================] - 0s 78us/step - loss: 7.7902e-04 - val_loss: 0.0044\n",
      "Epoch 376/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 7.7976e-04 - val_loss: 0.0044\n",
      "Epoch 377/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 7.7868e-04 - val_loss: 0.0044\n",
      "Epoch 378/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 7.7073e-04 - val_loss: 0.0044\n",
      "Epoch 379/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 7.7360e-04 - val_loss: 0.0044\n",
      "Epoch 380/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 7.6969e-04 - val_loss: 0.0044\n",
      "Epoch 381/500\n",
      "1095/1095 [==============================] - 0s 83us/step - loss: 7.7482e-04 - val_loss: 0.0044\n",
      "Epoch 382/500\n",
      "1095/1095 [==============================] - 0s 71us/step - loss: 7.7642e-04 - val_loss: 0.0044\n",
      "Epoch 383/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 7.6659e-04 - val_loss: 0.0044\n",
      "Epoch 384/500\n",
      "1095/1095 [==============================] - 0s 93us/step - loss: 7.6530e-04 - val_loss: 0.0044\n",
      "Epoch 385/500\n",
      "1095/1095 [==============================] - 0s 66us/step - loss: 7.6390e-04 - val_loss: 0.0044\n",
      "Epoch 386/500\n",
      "1095/1095 [==============================] - 0s 88us/step - loss: 7.6118e-04 - val_loss: 0.0044\n",
      "Epoch 387/500\n",
      "1095/1095 [==============================] - 0s 65us/step - loss: 7.6041e-04 - val_loss: 0.0044\n",
      "Epoch 388/500\n",
      "1095/1095 [==============================] - 0s 57us/step - loss: 7.5893e-04 - val_loss: 0.0044\n",
      "Epoch 389/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 7.5554e-04 - val_loss: 0.0044\n",
      "Epoch 390/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 7.5643e-04 - val_loss: 0.0044\n",
      "Epoch 391/500\n",
      "1095/1095 [==============================] - 0s 57us/step - loss: 7.5940e-04 - val_loss: 0.0044\n",
      "Epoch 392/500\n",
      "1095/1095 [==============================] - 0s 58us/step - loss: 7.5288e-04 - val_loss: 0.0043\n",
      "Epoch 393/500\n",
      "1095/1095 [==============================] - 0s 79us/step - loss: 7.4898e-04 - val_loss: 0.0044\n",
      "Epoch 394/500\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 7.4510e-04 - val_loss: 0.0044\n",
      "Epoch 395/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 7.4903e-04 - val_loss: 0.0044\n",
      "Epoch 396/500\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 7.4976e-04 - val_loss: 0.0043\n",
      "Epoch 397/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 7.4564e-04 - val_loss: 0.0043\n",
      "Epoch 398/500\n",
      "1095/1095 [==============================] - 0s 81us/step - loss: 7.4412e-04 - val_loss: 0.0043\n",
      "Epoch 399/500\n",
      "1095/1095 [==============================] - 0s 65us/step - loss: 7.3928e-04 - val_loss: 0.0044\n",
      "Epoch 400/500\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 7.3795e-04 - val_loss: 0.0043\n",
      "Epoch 401/500\n",
      "1095/1095 [==============================] - 0s 65us/step - loss: 7.3306e-04 - val_loss: 0.0044\n",
      "Epoch 402/500\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 7.3798e-04 - val_loss: 0.0043\n",
      "Epoch 403/500\n",
      "1095/1095 [==============================] - 0s 65us/step - loss: 7.3649e-04 - val_loss: 0.0043\n",
      "Epoch 404/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 7.3248e-04 - val_loss: 0.0043\n",
      "Epoch 405/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 7.3001e-04 - val_loss: 0.0043\n",
      "Epoch 406/500\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 7.3113e-04 - val_loss: 0.0043\n",
      "Epoch 407/500\n",
      "1095/1095 [==============================] - 0s 66us/step - loss: 7.2842e-04 - val_loss: 0.0044\n",
      "Epoch 408/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 7.2785e-04 - val_loss: 0.0044\n",
      "Epoch 409/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 7.2936e-04 - val_loss: 0.0043\n",
      "Epoch 410/500\n",
      "1095/1095 [==============================] - 0s 70us/step - loss: 7.2438e-04 - val_loss: 0.0043\n",
      "Epoch 411/500\n",
      "1095/1095 [==============================] - 0s 60us/step - loss: 7.2210e-04 - val_loss: 0.0043\n",
      "Epoch 412/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 7.1904e-04 - val_loss: 0.0043\n",
      "Epoch 413/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 7.1896e-04 - val_loss: 0.0043\n",
      "Epoch 414/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 7.1310e-04 - val_loss: 0.0043\n",
      "Epoch 415/500\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 7.1839e-04 - val_loss: 0.0043\n",
      "Epoch 416/500\n",
      "1095/1095 [==============================] - 0s 66us/step - loss: 7.1286e-04 - val_loss: 0.0043\n",
      "Epoch 417/500\n",
      "1095/1095 [==============================] - 0s 65us/step - loss: 7.1507e-04 - val_loss: 0.0043\n",
      "Epoch 418/500\n",
      "1095/1095 [==============================] - 0s 57us/step - loss: 7.0936e-04 - val_loss: 0.0044\n",
      "Epoch 419/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 7.0806e-04 - val_loss: 0.0043\n",
      "Epoch 420/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 7.1002e-04 - val_loss: 0.0043\n",
      "Epoch 421/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 7.0917e-04 - val_loss: 0.0044\n",
      "Epoch 422/500\n",
      "1095/1095 [==============================] - 0s 55us/step - loss: 7.1172e-04 - val_loss: 0.0043\n",
      "Epoch 423/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 7.0508e-04 - val_loss: 0.0043\n",
      "Epoch 424/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 7.0289e-04 - val_loss: 0.0043\n",
      "Epoch 425/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 7.0037e-04 - val_loss: 0.0044\n",
      "Epoch 426/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 7.0209e-04 - val_loss: 0.0044\n",
      "Epoch 427/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 7.0242e-04 - val_loss: 0.0043\n",
      "Epoch 428/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 7.0080e-04 - val_loss: 0.0043\n",
      "Epoch 429/500\n",
      "1095/1095 [==============================] - 0s 66us/step - loss: 6.9806e-04 - val_loss: 0.0043\n",
      "Epoch 430/500\n",
      "1095/1095 [==============================] - 0s 66us/step - loss: 6.9533e-04 - val_loss: 0.0043\n",
      "Epoch 431/500\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 6.9440e-04 - val_loss: 0.0043\n",
      "Epoch 432/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 6.9226e-04 - val_loss: 0.0044\n",
      "Epoch 433/500\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 6.9206e-04 - val_loss: 0.0043\n",
      "Epoch 434/500\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 6.8782e-04 - val_loss: 0.0043\n",
      "Epoch 435/500\n",
      "1095/1095 [==============================] - 0s 72us/step - loss: 6.8799e-04 - val_loss: 0.0043\n",
      "Epoch 436/500\n",
      "1095/1095 [==============================] - 0s 65us/step - loss: 6.8758e-04 - val_loss: 0.0043\n",
      "Epoch 437/500\n",
      "1095/1095 [==============================] - 0s 70us/step - loss: 6.8451e-04 - val_loss: 0.0043\n",
      "Epoch 438/500\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 6.8403e-04 - val_loss: 0.0043\n",
      "Epoch 439/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 6.8205e-04 - val_loss: 0.0043\n",
      "Epoch 440/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 6.8310e-04 - val_loss: 0.0043\n",
      "Epoch 441/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 6.7988e-04 - val_loss: 0.0043\n",
      "Epoch 442/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 6.8086e-04 - val_loss: 0.0043\n",
      "Epoch 443/500\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 6.7952e-04 - val_loss: 0.0043\n",
      "Epoch 444/500\n",
      "1095/1095 [==============================] - 0s 65us/step - loss: 6.7727e-04 - val_loss: 0.0043\n",
      "Epoch 445/500\n",
      "1095/1095 [==============================] - 0s 72us/step - loss: 6.7563e-04 - val_loss: 0.0043\n",
      "Epoch 446/500\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 6.7768e-04 - val_loss: 0.0043\n",
      "Epoch 447/500\n",
      "1095/1095 [==============================] - 0s 74us/step - loss: 6.7035e-04 - val_loss: 0.0043\n",
      "Epoch 448/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 6.7127e-04 - val_loss: 0.0044\n",
      "Epoch 449/500\n",
      "1095/1095 [==============================] - 0s 79us/step - loss: 6.7192e-04 - val_loss: 0.0043\n",
      "Epoch 450/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 6.7055e-04 - val_loss: 0.0043\n",
      "Epoch 451/500\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 6.6620e-04 - val_loss: 0.0043\n",
      "Epoch 452/500\n",
      "1095/1095 [==============================] - 0s 60us/step - loss: 6.6739e-04 - val_loss: 0.0044\n",
      "Epoch 453/500\n",
      "1095/1095 [==============================] - 0s 66us/step - loss: 6.7158e-04 - val_loss: 0.0043\n",
      "Epoch 454/500\n",
      "1095/1095 [==============================] - 0s 60us/step - loss: 6.6289e-04 - val_loss: 0.0043\n",
      "Epoch 455/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1095/1095 [==============================] - 0s 61us/step - loss: 6.6086e-04 - val_loss: 0.0043\n",
      "Epoch 456/500\n",
      "1095/1095 [==============================] - 0s 55us/step - loss: 6.6249e-04 - val_loss: 0.0043\n",
      "Epoch 457/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 6.5991e-04 - val_loss: 0.0043\n",
      "Epoch 458/500\n",
      "1095/1095 [==============================] - 0s 85us/step - loss: 6.5525e-04 - val_loss: 0.0043\n",
      "Epoch 459/500\n",
      "1095/1095 [==============================] - 0s 60us/step - loss: 6.6279e-04 - val_loss: 0.0043\n",
      "Epoch 460/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 6.5719e-04 - val_loss: 0.0043\n",
      "Epoch 461/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 6.5224e-04 - val_loss: 0.0043\n",
      "Epoch 462/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 6.5575e-04 - val_loss: 0.0043\n",
      "Epoch 463/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 6.5040e-04 - val_loss: 0.0043\n",
      "Epoch 464/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 6.4691e-04 - val_loss: 0.0043\n",
      "Epoch 465/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 6.4858e-04 - val_loss: 0.0043\n",
      "Epoch 466/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 6.4369e-04 - val_loss: 0.0043\n",
      "Epoch 467/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 6.4708e-04 - val_loss: 0.0043\n",
      "Epoch 468/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 6.4778e-04 - val_loss: 0.0043\n",
      "Epoch 469/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 6.4503e-04 - val_loss: 0.0043\n",
      "Epoch 470/500\n",
      "1095/1095 [==============================] - 0s 71us/step - loss: 6.4697e-04 - val_loss: 0.0043\n",
      "Epoch 471/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 6.4124e-04 - val_loss: 0.0043\n",
      "Epoch 472/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 6.4102e-04 - val_loss: 0.0043\n",
      "Epoch 473/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 6.4268e-04 - val_loss: 0.0043\n",
      "Epoch 474/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 6.3985e-04 - val_loss: 0.0043\n",
      "Epoch 475/500\n",
      "1095/1095 [==============================] - 0s 93us/step - loss: 6.3948e-04 - val_loss: 0.0043\n",
      "Epoch 476/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 6.3623e-04 - val_loss: 0.0043\n",
      "Epoch 477/500\n",
      "1095/1095 [==============================] - 0s 72us/step - loss: 6.3717e-04 - val_loss: 0.0043\n",
      "Epoch 478/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 6.2879e-04 - val_loss: 0.0043\n",
      "Epoch 479/500\n",
      "1095/1095 [==============================] - 0s 71us/step - loss: 6.3469e-04 - val_loss: 0.0043\n",
      "Epoch 480/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 6.3357e-04 - val_loss: 0.0044\n",
      "Epoch 481/500\n",
      "1095/1095 [==============================] - 0s 69us/step - loss: 6.3277e-04 - val_loss: 0.0043\n",
      "Epoch 482/500\n",
      "1095/1095 [==============================] - 0s 74us/step - loss: 6.3197e-04 - val_loss: 0.0043\n",
      "Epoch 483/500\n",
      "1095/1095 [==============================] - 0s 69us/step - loss: 6.3121e-04 - val_loss: 0.0043\n",
      "Epoch 484/500\n",
      "1095/1095 [==============================] - 0s 67us/step - loss: 6.2750e-04 - val_loss: 0.0043\n",
      "Epoch 485/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 6.2656e-04 - val_loss: 0.0043\n",
      "Epoch 486/500\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 6.2426e-04 - val_loss: 0.0043\n",
      "Epoch 487/500\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 6.2454e-04 - val_loss: 0.0043\n",
      "Epoch 488/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 6.2200e-04 - val_loss: 0.0043\n",
      "Epoch 489/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 6.2262e-04 - val_loss: 0.0043\n",
      "Epoch 490/500\n",
      "1095/1095 [==============================] - 0s 60us/step - loss: 6.2158e-04 - val_loss: 0.0043\n",
      "Epoch 491/500\n",
      "1095/1095 [==============================] - 0s 67us/step - loss: 6.1647e-04 - val_loss: 0.0043\n",
      "Epoch 492/500\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 6.1846e-04 - val_loss: 0.0043\n",
      "Epoch 493/500\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 6.1805e-04 - val_loss: 0.0043\n",
      "Epoch 494/500\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 6.1340e-04 - val_loss: 0.0043\n",
      "Epoch 495/500\n",
      "1095/1095 [==============================] - 0s 66us/step - loss: 6.1465e-04 - val_loss: 0.0043\n",
      "Epoch 496/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 6.1485e-04 - val_loss: 0.0043\n",
      "Epoch 497/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 6.1287e-04 - val_loss: 0.0043\n",
      "Epoch 498/500\n",
      "1095/1095 [==============================] - 0s 66us/step - loss: 6.1279e-04 - val_loss: 0.0043\n",
      "Epoch 499/500\n",
      "1095/1095 [==============================] - 0s 67us/step - loss: 6.0982e-04 - val_loss: 0.0043\n",
      "Epoch 500/500\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 6.1103e-04 - val_loss: 0.0043\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2beb16635c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers.core import Dense, Dropout\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(160, activation=\"relu\"))\n",
    "model.add(Dense(80, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "\n",
    "X = all_input_feats_train\n",
    "Y = normalized_train_output_matrix\n",
    "print(\"Input X has shape\", X.shape)\n",
    "print(\"Desired output Y has shape\", Y.shape)\n",
    "print(\"Y:\\n\", Y)\n",
    "model.fit(X,Y, validation_split=0.25, epochs=500)\n",
    "#model.fit(X,Y, epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing the trained MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds_train_houses:\n",
      " [[0.2309]\n",
      " [0.219 ]\n",
      " [0.2739]\n",
      " ...\n",
      " [0.2903]\n",
      " [0.0844]\n",
      " [0.2053]]\n",
      "preds_train_houses_dollar:\n",
      " [[201183.02 ]\n",
      " [192620.5  ]\n",
      " [232109.92 ]\n",
      " ...\n",
      " [243954.03 ]\n",
      " [ 95687.664]\n",
      " [182715.42 ]]\n",
      "Shape of preds_train_houses is (1460, 1)\n",
      "Shape of preds_train_houses_dollar is (1460, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAooAAAJQCAYAAAAJ0UXFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3X+UXOV95/nPVy0kJAS21PqxHOFuwcBRLHNiJziYcSZZ\nJchANE7weoFVtgVtDFaM2ESexJkxR9khY7tnTA4ZBnZXIjogLFBnbBknA0mwHUmkvWe9C5h4kmDA\nimSbFigYhW6MLAskIX33j/tcurr6uVW3qutW14/365w6VfVU3Vv3Nv7x4Xme7/OYuwsAAAAoN2um\nLwAAAACtiaAIAACAKIIiAAAAogiKAAAAiCIoAgAAIIqgCAAAgKhCg6KZbTKz75rZs2b2qdC2yMx2\nm9n+8Lyw5Pu3mdkBM9tnZleWtF9iZs+Ez+4xMwvtc83sy6H9STNbUXLMYPiN/WY2WOR9AgAAdKLC\ngqKZXSzpE5IulfReSR82swslfUbSXne/SNLe8F5mtkrSOknvkXSVpC1m1hNOtzWc66LwuCq03yTp\nNXe/UNJdku4I51ok6XZJHwi/f3tpIAUAAEB1RfYovlvSk+5+zN3fkvRNSR+VdLWkHeE7OyR9JLy+\nWtKX3P24u/9Q0gFJl5rZuZLOcfcnPFkd/MGyY9JzPSzp8tDbeKWk3e4+7u6vSdqtiXAJAACAHGYX\neO7vShoys15Jb0haK+lpScvc/eXwnR9JWhZeL5f0RMnxL4W2k+F1eXt6zIuS5O5vmdnrknpL2yPH\nvM3MNkjaIElnnnnmJX19fXXdaDs7ffq0Zs3qvqmq3Hd34b67C/fdXbr1vv/xH//xVXdfUvTvFBYU\n3f15M7tD0l9L+qmkv5N0quw7bmYztoegu2+TtE2SVq5c6fv27ZupS5kxIyMjWr169UxfRtNx392F\n++4u3Hd36db7NrPRZvxOoRHc3e9390vc/ZclvSbpHyW9EoaTFZ4Ph68fkvSuksPPC22Hwuvy9knH\nmNlsSe+QNFbhXAAAAMip6KrnpeG5T8n8xD+V9KiktAp5UNIj4fWjktaFSubzlRStPBWGqY+Y2WVh\n/uENZcek57pG0uNhHuM3JF1hZgtDEcsVoQ0AAAA5FTlHUZK+GuYonpR0q7v/2My+IGmXmd0kaVTS\ndZLk7s+a2S5Jz0l6K3w/HareKOmLkuZJ+lp4SNL9kh4yswOSxpVUTcvdx83sc5K+Hb73WXcfL/ZW\nAQAAOkuhQdHdfynSNibp8ozvD0kairQ/LeniSPubkq7NONd2SdtrvGQAAAAE3VcmBAAAgFwIigAA\nAIgiKAIAACCKoAgAAIAogiIAAACiCIoAAACIIigCAAAgiqAIAACAKIIiAAAAogiKAAAAiCIoAgAA\nIIqgCAAAgCiCIgAAAKIIigAAAIgiKAIAACCKoAgAAIAogiIAAACiCIoAAACIIigCAAAgiqAIAACA\nKIIiAAAAogiKAAAAiCIoAgAAIIqgCAAAgCiCIgAAAKIIigAAAIgiKAIAACCKoAgAAIAogiIAAACi\nCIoAAACIIigCAAAgiqAIAACAKIIiAAAAogiKAAAAiCIoAgAAIIqgCAAAgCiCIgAAAKIIigAAAIgi\nKAIAACCKoAgAAIAogiIAAACiCIoAAACIIigCAAAgiqAIAACAKIIiAAAAogiKAAAAiCIoAgAAIIqg\nCAAAgCiCIgAAAKIIigAAAIgiKAIAACCKoAgAAICoQoOimf0bM3vWzL5rZv/VzM40s0VmttvM9ofn\nhSXfv83MDpjZPjO7sqT9EjN7Jnx2j5lZaJ9rZl8O7U+a2YqSYwbDb+w3s8Ei7xMAAKATFRYUzWy5\npN+R9H53v1hSj6R1kj4jaa+7XyRpb3gvM1sVPn+PpKskbTGznnC6rZI+Iemi8LgqtN8k6TV3v1DS\nXZLuCOdaJOl2SR+QdKmk20sDKQAAAKoreuh5tqR5ZjZb0nxJ/yTpakk7wuc7JH0kvL5a0pfc/bi7\n/1DSAUmXmtm5ks5x9yfc3SU9WHZMeq6HJV0eehuvlLTb3cfd/TVJuzURLgEAAJDD7KJO7O6HzOxO\nSQclvSHpr939r81smbu/HL72I0nLwuvlkp4oOcVLoe1keF3enh7zYvi9t8zsdUm9pe2RY95mZhsk\nbZCkJUuWaGRkpL6bbWNHjx7lvrsI991duO/uwn2jCIUFxTDUe7Wk8yX9WNJXzGx96Xfc3c3Mi7qG\natx9m6RtkrRy5UpfvXr1TF3KjBkZGRH33T247+7CfXcX7htFKHLoeY2kH7r7P7v7SUl/JumDkl4J\nw8kKz4fD9w9JelfJ8eeFtkPhdXn7pGPC8PY7JI1VOBcAAAByKjIoHpR0mZnND/MGL5f0vKRHJaVV\nyIOSHgmvH5W0LlQyn6+kaOWpMEx9xMwuC+e5oeyY9FzXSHo8zGP8hqQrzGxh6Nm8IrQBAAAgpyLn\nKD5pZg9L+o6ktyT9dyXDvAsk7TKzmySNSroufP9ZM9sl6bnw/Vvd/VQ43UZJX5Q0T9LXwkOS7pf0\nkJkdkDSupGpa7j5uZp+T9O3wvc+6+3hR9woAANCJCguKkuTutytZpqbUcSW9i7HvD0kairQ/Leni\nSPubkq7NONd2SdtrvGQAAAAE7MwCAACAKIIiAAAAogiKAAAAiCIoAgAAIIqgCAAAgCiCIgAAAKII\nigAAAIgiKAIAACCKoAgAAIAogiIAAACiCIoAAACIIigCAAAgiqAIAACAKIIiAAAAogiKAAAAiCIo\nAgAAIIqgCAAAgCiCIgAAAKIIigAAAIgiKAIAACCKoAgAAIAogiIAAACiCIoAAACIIigCAAAgiqAI\nAACAKIIiAAAAogiKAAAAiCIoAgAAIIqgCAAAgCiCIgAAAKIIigAAAIgiKAIAACCKoAgAAIAogiIA\nAACiCIoAAACIIigCAAAgiqAIAACAKIIiAAAAogiKAAAAiCIoAgAAIIqgCAAAgCiCIgAAAKIIigAA\nAIgiKAIAACCKoAgAAIAogiIAAACiCIoAAACIIigCAAAgiqAIAACAKIIiAAAAogiKAAAAiCosKJrZ\nSjP7u5LHETP7lJktMrPdZrY/PC8sOeY2MztgZvvM7MqS9kvM7Jnw2T1mZqF9rpl9ObQ/aWYrSo4Z\nDL+x38wGi7pPAACATlVYUHT3fe7+Pnd/n6RLJB2T9OeSPiNpr7tfJGlveC8zWyVpnaT3SLpK0hYz\n6wmn2yrpE5IuCo+rQvtNkl5z9wsl3SXpjnCuRZJul/QBSZdKur00kAIAAKC6Zg09Xy7p++4+Kulq\nSTtC+w5JHwmvr5b0JXc/7u4/lHRA0qVmdq6kc9z9CXd3SQ+WHZOe62FJl4fexisl7Xb3cXd/TdJu\nTYRLAAAA5DC7Sb+zTtJ/Da+XufvL4fWPJC0Lr5dLeqLkmJdC28nwurw9PeZFSXL3t8zsdUm9pe2R\nY95mZhskbZCkJUuWaGRkpI5ba29Hjx7lvrsI991duO/uwn2jCIUHRTObI+k3JN1W/pm7u5l50deQ\nxd23SdomSStXrvTVq1fP1KXMmJGREXHf3YP77i7cd3fhvlGEZgw9/5qk77j7K+H9K2E4WeH5cGg/\nJOldJcedF9oOhdfl7ZOOMbPZkt4haazCuQAAAJBTM4Lib2pi2FmSHpWUViEPSnqkpH1dqGQ+X0nR\nylNhmPqImV0W5h/eUHZMeq5rJD0e5jF+Q9IVZrYwFLFcEdoAAACQU6FDz2Z2lqQPSfqtkuYvSNpl\nZjdJGpV0nSS5+7NmtkvSc5LeknSru58Kx2yU9EVJ8yR9LTwk6X5JD5nZAUnjSuZCyt3Hzexzkr4d\nvvdZdx8v5CYBAAA6VKFB0d1/qqS4pLRtTEkVdOz7Q5KGIu1PS7o40v6mpGszzrVd0vbarxoAAAAS\nO7MAAAAgA0ERAAAAUQRFAAAARBEUAQBAYwwPSytWSLNmJc/DwzN9RZimZu3MAgAAOtnwsLRhg3Ts\nWPJ+dDR5L0kDAzN3XZgWehQBAMD0bd48ERJTx44l7WhbBEUAADB9Bw/W1o62QFAEAADT19dXWzva\nAkERAABM39CQNH/+5Lb585N2tC2CIgAAmL6BAWnbNqm/XzJLnrdto5ClzVH1DAAAGmNggGDYYehR\nBAAAQBRBEQAAAFEERQAAAEQRFAEAABBFUAQAAEAUQREAAABRBEUAAABEERQBAAAQRVAEAABAFEER\nAAAAUQRFAAAARBEUAQAAEEVQBAAAQBRBEQAAAFEERQAAAEQRFAEAABBFUAQAAEAUQREAAABRBEUA\nAABEERQBAAAQRVAEAABAFEERAAAAUQRFAAAARBEUAQAAEEVQBAAAQBRBEQAAAFEERQAAAEQRFAEA\nABBFUAQAAEAUQREAAABRBEUAAABEERQBAAAQRVAEAABAFEERAAAAUQRFAAAARBEUAQAAEEVQBAAA\nQBRBEQAAAFEERQAAAEQRFAEAABBVaFA0s3ea2cNm9j0ze97M/qWZLTKz3Wa2PzwvLPn+bWZ2wMz2\nmdmVJe2XmNkz4bN7zMxC+1wz+3Jof9LMVpQcMxh+Y7+ZDRZ5nwAAAJ2o6B7FuyV93d1/RtJ7JT0v\n6TOS9rr7RZL2hvcys1WS1kl6j6SrJG0xs55wnq2SPiHpovC4KrTfJOk1d79Q0l2S7gjnWiTpdkkf\nkHSppNtLAykAAACqKywomtk7JP2ypPslyd1PuPuPJV0taUf42g5JHwmvr5b0JXc/7u4/lHRA0qVm\ndq6kc9z9CXd3SQ+WHZOe62FJl4fexisl7Xb3cXd/TdJuTYRLAAAA5DC7wHOfL+mfJT1gZu+V9LeS\nNkla5u4vh+/8SNKy8Hq5pCdKjn8ptJ0Mr8vb02NelCR3f8vMXpfUW9oeOeZtZrZB0gZJWrJkiUZG\nRuq5z7Z29OhR7ruLcN/dhfvuLtw3ilBkUJwt6ecl/ba7P2lmdysMM6fc3c3MC7yGitx9m6RtkrRy\n5UpfvXr1TF3KjBkZGRH33T247+7CfXcX7htFKHKO4kuSXnL3J8P7h5UEx1fCcLLC8+Hw+SFJ7yo5\n/rzQdii8Lm+fdIyZzZb0DkljFc4FAACAnAoLiu7+I0kvmtnK0HS5pOckPSoprUIelPRIeP2opHWh\nkvl8JUUrT4Vh6iNmdlmYf3hD2THpua6R9HiYx/gNSVeY2cJQxHJFaAMAAEBORQ49S9JvSxo2szmS\nfiDpRiXhdJeZ3SRpVNJ1kuTuz5rZLiVh8i1Jt7r7qXCejZK+KGmepK+Fh5QUyjxkZgckjSupmpa7\nj5vZ5yR9O3zvs+4+XuSNAgAAdJpCg6K7/52k90c+ujzj+0OShiLtT0u6ONL+pqRrM861XdL2Wq4X\nAAAAE9iZBQAAAFEERQAAAEQRFAEAABBFUAQAAEAUQREAAABRBEUAAABEERQBAAAQRVAEAABAFEER\nAAAAUQRFAAAARBEUAQAAEEVQBAAAQBRBEQAAAFEERQAAAEQRFAEAABBFUAQAAEAUQREAAABRBEUA\nAABEERQBAAAQRVAEAABAFEERAAAAUQRFAAAARBEUAQAAEEVQBAAAQBRBEQAAAFEERQAAAEQRFAEA\nABBFUAQAAEAUQREAAABRBEUAAABEERQBAAAQRVBEaxkellaskGbNSp6Hh2f6igAA6Fqz837RzOa7\n+7EiLwZdbnhY2rBBOhb+YzY6mryXpIGBmbsuAAC6VNUeRTP7oJk9J+l74f17zWxL4VeG7rN580RI\nTB07lrQDwHQxYgHULM/Q812SrpQ0Jknu/veSfrnIi0KXOniwtnYAyCsdsRgdldwnRiwIi0BFueYo\nuvuLZU2nCrgWdLu+vtraASAvRiyAuuQJii+a2QcluZmdYWaflvR8wdeFbjQ0JM2fP7lt/vykHUDr\naMchXEYsgLrkCYqflHSrpOWSDkl6X3gPNNbAgLRtm9TfL5klz9u2UcgCtJJ2HcJlxAKoS9Wg6O6v\nuvuAuy9z96Xuvt7dx5pxcehCAwPSCy9Ip08nz4REoLW06xAuIxZAXfJUPe8ws3eWvF9oZtuLvSwA\nQEtq1yFcRiyAuuRZR/Fn3f3H6Rt3f83Mfq7AawIAtKq+vmS4Odbe6gYGCIZAjfLMUZxlZgvTN2a2\nSDUs1A0A6CAM4QJdJU/g+2NJ/5+ZfUWSSbpGEv+LAADdKO2R27w5GW7u60tCIj11QEeqGhTd/UEz\ne1rSr4amj7r7c8VeFgCgZTGEC3SNzKBoZue4+5Ew1PwjSX9a8tkidx9vxgUCAABgZlTqUfxTSR+W\n9LeSvKTdwvsLCrwuAAAAzLDMoOjuHzYzk/Q/unuLr3sAAACARqtY9ezuLumvmnQtAAAAaCF5lsf5\njpn9QuFXAgAAgJaSZ3mcD0gaMLNRST9VmKPo7j9b6JUBAABgRuUJilcWfhUAAABoOXnWURw1s5+X\n9K+UVDt/y92/U/iVAQAAYEZVnaNoZv9e0g5JvZIWS3rAzP4gz8nN7AUze8bM/i4s2i0zW2Rmu81s\nf3gu3R7wNjM7YGb7zOzKkvZLwnkOmNk9oRpbZjbXzL4c2p80sxUlxwyG39hvZoP5/hwAAABI5Slm\nGZD0C+5+u7vfLukySdfX8Bu/4u7vc/f3h/efkbTX3S+StDe8l5mtkrRO0nskXSVpi5n1hGO2SvqE\npIvC46rQfpOk19z9Qkl3SbojnGuRpNuVzK+8VNLtpYEUAAAA1eUJiv8k6cyS93MlHZrGb16tpIdS\n4fkjJe1fcvfj7v5DSQckXWpm50o6x92fCMv1PFh2THquhyVdHnobr5S0293H3f01Sbs1ES4BAACQ\nQ55iltclPWtmu5XMUfyQpKfM7B5JcvffqXCsS9pjZqck/Ym7b5O0zN1fDp//SNKy8Hq5pCdKjn0p\ntJ0Mr8vb02NeDNfxlpm9rmSI/O32yDFvM7MNkjZI0pIlSzQyMlLhVjrT0aNHue8uwn13l06576V7\n9uiC++7T3MOHdXzpUv3g5pt1eM2azO93yn3XivtGEfIExT8Pj9RIDef/V+5+yMyWStptZt8r/dDd\n3cw849jCheC6TZJWrlzpq1evnqlLmTEjIyPivrsH991dOuK+h4elu+6Sjh2TJJ35yitaddddWvXu\nd0sDA9FDOuK+68B9owhVh57dfUelR5VjD4Xnw0rC5qWSXgnDyQrPh8PXD0l6V8nh54W2Q+F1efuk\nY8xstqR3SBqrcC4AQDMND0srVkizZiXPw8O1Hb9589sh8W3HjiXtAAqXZ45iXczsLDM7O30t6QpJ\n35X0qKS0CnlQ0iPh9aOS1oVK5vOVFK08FYapj5jZZWH+4Q1lx6TnukbS42Ee4zckXWFmC0MRyxWh\nDQDQLMPD0oYN0uio5J48b9hQW1g8eLC2dgANlWfouV7LJP15WMlmtqQ/dfevm9m3Je0ys5skjUq6\nTpLc/Vkz2yXpOUlvSbrV3U+Fc22U9EVJ8yR9LTwk6X5JD5nZAUnjSqqm5e7jZvY5Sd8O3/usu48X\neK8AgHKVegMzho2n6OtLAmasHUDhcgdFM5vv7seqfzPh7j+Q9N5I+5ikyzOOGZI0FGl/WtLFkfY3\nJV2bca7tkrbnvV4AQIM1ojdwaCjphSwNnPPnJ+0ACpdnwe0Pmtlzkr4X3r/XzLYUfmUAgPaW1etX\nS2/gwIC0bZvU3y+ZJc/btuXvkQQwLXnmKN6lZF3CMUly97+X9MtFXhQAoAMMDSW9f6Xq6Q0cGJBe\neEE6fTp5JiQCTZOrmMXdXyxrOhX9IgAAqbQ3sLd3om3evJm7HgA1yzNH8UUz+6AkN7MzJG2S9Hyx\nlwUA6BhvvDHxemwsmXMo0TMItIE8PYqflHSrkp1NDkl6X3gPAEBlrIMItLWqPYru/qok/rUPAFA7\n1kEE2lqequc/MrNzzOwMM9trZv9sZuubcXEAgDbXiMpnADMmz9DzFe5+RNKHJb0g6UJJv1/kRQEA\npm/pnj3T2z6vERpV+QxgRuQJiunw9L+W9BV3f73A6wEANMLwsFbeeef0ts9rBNZBBNpanqD4l2b2\nPUmXSNprZkskvVnsZQEApmXzZvUcPz65baaKSFgHEWhbVYOiu39G0gclvd/dT0r6qaSri74wAMA0\nUEQCoAGqVj2b2Q0lr0s/erCICwIANEBfXzLcHGsHgJzyDD3/QsnjlyT9oaTfKPCaAADDw9MrRBka\n0qm5cye3UUQCoEZ51lH87dL3ZvZOSV8q7IoAoNsNDyeFJ+lC1WkhipR/ft/AgPY9/7xW7dyZDDf3\n9SUhkfmBAGqQa6/nMj+VdH6jLwQAEDRoN5PDa9ZQRAJgWvIsuP0XZvZoePyVpH2S/rz4SwOALkUh\nysyY7nA/0IHy9CjeKemPw+M/SvrlUAkNAChCpd1MCDPFSIf7Z3rdSaDF5Fke55uSvifpbEkLJZ0o\n+qIAoKtl7Waydi1hpigNGu4HOk2eoefrJD0l6VpJ10l60syuKfrCAKBrZe1m8thjhJmiMNwPRFWt\nepa0WdIvuPthSQo7s+yR9HCRFwYAXW1gYGrxyfXXx79LmJk+1p0EovLMUZyVhsRgLOdxAIBGqjR3\nEdOTNdzPupPocnkC39fN7Btm9jEz+5ikv5L0WLGXBQBdJG+BCmGmOFnD/SwphC6XZ8Ht3zez/1nS\nL4ambe7O8jgA0Ai1LK6dvt+8mUW0ixAb7ge6XJ45inL3r0r6asHXAgDdp1K1bSy0EGYANFGequeP\nmtl+M3vdzI6Y2U/M7EgzLg4AOh7VtgBaWJ45in8k6Tfc/R3ufo67n+3u5xR9YQDQ8hqx+DUFKgBa\nWJ6g+Iq7P1/4lQBAO2nUTh4UqABoYZlBMQw5f1TS02b2ZTP7zbQttANA92rUTh5U2wJoYZWKWX69\n5PUxSVeUvHdJf1bIFQFAO4gtzizVN7eQAhUALSozKLr7jc28EABoG8PDSe+f+9TPmFsIoIOwwwoA\n1Grz5nhINGNuIYCOQlAEgFplDS+71z6E3IjKaQAoCEERAGqVNbzc3z+1rVIQbFTlNAAUJHOOopn9\nbqUD3f0/N/5yAKANDA1N3nZPii9pU217vlp3ZQGAJqvUo3h2eLxf0i2SlofHJyX9fPGXBgBNUuvw\nb94lbaotocOuLABaXKWq5/8gSWb2f0v6eXf/SXj/h5L+qilXBwBFq9brlyXPkjbVgmBfX3yZHSqn\nAbSIPHMUl0k6UfL+RGgDgNZTa+9goxbOjqm2PR+7sgBocXmC4oOSnjKzPwy9iU9K2lHoVQFAPTKK\nQ5bu2ZN9TLVev+lUJVcLguzKAqDFVQ2K7j4k6UZJr4XHje7+H4u+MACoWUbv4AX33Rf//vBwEgBj\n+vqmX5WcJwgODEgvvCCdPp08ExIBtJC8y+PMl3TE3e+W9JKZnV/gNQFAfTJ6B+cePjy1MQ2Bp05N\n/Szt9WvEsDRBEEAbqxoUzex2Sf9O0m2h6QxJO4u8KACoS8acwONLl05tjIVASerpmej1q6cqmQW0\nAXSQPD2K/5Ok35D0U0ly939SsmwOALSWjDmBP7j55qnfzQp7p09P9PpVK0YpxwLaADpMnqB4wt1d\nkkuSmZ1V7CUBQJ0y5gQeXrNm6nfzhMBaq5KLrKAGgBmQJyjuMrM/kfROM/uEpD2SMmaGA2gqhjnr\nlycE1lqVzALaADpMnqrnOyU9LOmrklZK+vfufk/RFwagCoY5p6pleZy8IbCWYpRah6oBoMXlKWa5\nw913u/vvu/un3X23md3RjIsDUAHDnFPVujxOoyuSWUAbQIfJM/T8oUjbrzX6QgDUiGHOqUPvse3w\nlLE8ThFYQBtAh8kMimZ2i5k9I+lnzOwfSh4/lPRM8y4RQFQdFbmXrVvXOfMZY8PMZtGvRpfHKQrr\nJgLoIJV6FP9U0q9LeiQ8p49L3J3/5QNmWi3DnCFUnfnKK50znzE2zOw+NSxmLY9TKwqHAHShzKDo\n7q+7+wuS7pY07u6j7j4q6S0z+0CzLhBAhlqGOTtxPmPWELt7vuVxakHhEIAulWeO4lZJR0veHw1t\nAGZa3mHOTpzPmDXE3t/f+KHfTgzaAJBDnqBoYcFtSZK7n5Y0O+8PmFmPmf13M/vL8H6Rme02s/3h\neWHJd28zswNmts/Mrixpv8TMngmf3WOWjC2Z2Vwz+3Jof9LMVpQcMxh+Y7+ZDea9XqAjdeKyLc2s\nMO7EoA0AOeQJij8ws98xszPCY5OkH9TwG5skPV/y/jOS9rr7RZL2hvcys1WS1kl6j6SrJG0xs55w\nzFZJn5B0UXhcFdpvkvSau18o6S5Jd4RzLZJ0u6QPSLpU0u2lgRToOp24bEszK4w7MWgDQA55guIn\nJX1Q0iFJLykJXxvynNzMzpP0rzV5J5erJe0Ir3dI+khJ+5fc/bi7/1DSAUmXmtm5ks5x9ydCz+aD\nZcek53pY0uWht/FKSbvdfdzdX5O0WxPhEug+IVS9uWxZZy3b0qwK404M2kC9KOzqKlYyqtz4k5s9\nLOk/STpb0qfd/cNm9mN3f2f43JT0CL7TzP5PSU+4+87w2f2SvibpBUlfcPc1of2XJP27cK7vSrrK\n3V8Kn31fSZD9mKQz3f3zof1/l/RG2GWm9Po2KITeJUuWXLJr167C/hat6ujRo1qwYMFMX0bTcd/d\npRH3vXTPHl1w332ae/iwji9dqh/cfPP0i2QKxj/v7tKM+166Z49W3nmneo4ff7vt1Ny52vfpT8/Y\nfx+69Z/3r/zKr/ytu7+/6N/JnGtoZv/W3f/IzP4PSVPSpLv/TqUTm9mHJR129781s9Wx77i7m1lx\nSbUKd98maZskrVy50levXj1TlzJjRkZGxH13kOHhpMDi4MFkWHRoaFIvW8fedxUNue/Vq6XPf16S\ndKakVeHRyvjn3V2act8f+5hUEhIlqef4ca3auVOrwn8/mq1b/3k3S6WilHRe4dN1nvsXJf2Gma1V\n8r+r55jZTkmvmNm57v5yGFZOt0w4JOldJcefF9oOhdfl7aXHvGRmsyW9Q9JYaF9ddsxInfcBtId0\nCZe0OjddwkVq/yFmAK2Bwq6uU2kdxb8Izztij2ondvfb3P08d1+hpEjlcXdfL+lRSWkV8qCSBb0V\n2teFSuYUIrfmAAAgAElEQVTzlRStPOXuL0s6YmaXhaHqG8qOSc91TfgNl/QNSVeY2cJQxHJFaAM6\nF0u4ACgahV1dp9IWfn9hZo9mPabxm1+Q9CEz2y9pTXgvd39W0i5Jz0n6uqRb3f1UOGajkoKYA5K+\nr2TuoiTdL6nXzA5I+l2FCmp3H5f0OUnfDo/Phjagc3Xyv+kzeR5oDRR2dZ1KQ89p4cdHJf0PknaG\n978p6ZVafsTdRxSGft19TNLlGd8bkjTlP23u/rSkiyPtb0q6NuNc2yVtr+U6gbbW15cMN8fa29Hw\nsLRpkzQ2NrmdIXVg5qT/naswFxqdpdLQ8zfd/ZuSftHd/xd3/4vw+F8l/VLzLhFALp30b/rDw9KN\nN04NiSmG1IGZ06xlqdAS8qyjeJaZXZC+CfMHzyrukgDUpZkLUBdt82bp5MnK3+mEIXUAaHF5guK/\nkTRiZiNm9k1JfyPpU8VeFoC6tOO/6cfmH+YJgTUMqV/4X/6LNHt2EqBnz5Y2bqzvugCgy1QNiu7+\ndSUVyJsk/Y6kle5OBTHQKYoIRHnPmS7pMzoquU/MP1y0qPL5axlS37hRyx95RDoVauNOnZK2bq0c\nFrOui7AIoMtUDYpmNl/S70v639z97yX1hcW0AbS7IgJRLefMWtJHks44I37+3t7ahtS3bZNltGdi\nqSEAkJRv6PkBSSck/cvw/pCkmVl+HUBjFRGIajln1hDz+Lj0wANJKEz19ko7d0qvvlrbkHrak5i3\nvdJ1MS8SQJfJExT/hbv/kaSTkuTux6T4v6ADaDNFBKJazpk1xOyeBMu7705euyevN2+ufYi8p6e2\ndolFhQEgyBMUT5jZPIX9ns3sX0g6XvkQAG2hiECU95zDw9KRI9nnKR2yns4Q+YYNUzerD+2ZOmmp\nIQCYhjxB8XYlO6W8y8yGJe2V9G8LvSoAzVFEIMp7zjxL4KRD1tMZIt+yRYeuvnqiB7GnR7rlFmnL\nluxjOmmpIQCYhko7syjsrfw9JbuzXKZkyHmTu7/ahGsDULQidlnIe868w9uVvpfzHAc+9Smd99/+\nW77fSw0MEAwBdL2KPYru7pIec/cxd/8rd/9LQiIwTa22Pl8Ray/mOWfe4e2+vuzvzprVOn9HAOhA\neYaev2Nmv1D4lQDdoFPX56sn/MaGqMulQ9ZZ3z11qrP+jgDQYvIExQ9IesLMvm9m/2Bmz5jZPxR9\nYUBH6rT1+YaHpcWLpfXraw+/sXmAt9ySPEvJXMLSv03pd2MVy+38dwSAFpUnKF4p6QJJvyrp1yV9\nODwDqFUnrc+X9o6OjU39LG9oKx+i3rJlovcwXecwDZ7SxHdPn46fr9F/x1abJgAATZYZFM3sTDP7\nlJJdWa6SdMjdR9NH064Q6CStvj5fLcEo1jtaqt7QltXrun79xDU14+/YqdMEAKAGlXoUd0h6v6Rn\nJP2apD9uyhUBnayV1+erNRhVC4L1hrZK5x0dla6/Pnm2snX/G/137LRpAgBQh0pBcZW7r3f3P5F0\njaRfatI1AZ2rldfnqzUYVQqC0wlt1QKm+8RzGhaL+Dt20jQBAKhTpaD49kq47v5WE64F6A5FLEfT\nCLUGo6xK5N7e6YW2WgKmR/dcaYxWnyYAAE1QKSi+18yOhMdPJP1s+trMKuy7BaAt1RqMyntHe3uT\nx/h40gtZ71y+gQFpwYLajili/mDeaQIUvADoYJlB0d173P2c8Djb3WeXvD6nmRcJoAnqmT+Z9o4+\n9JD0xhtJBfR0Cz+Gh6XjdWwn3+j5g3mmCVDwAqDD5VkeB0A3mM78yUYWflTaA7q8gKVco+cPVpsm\nQMELgA5HUAQwoVIwqjTE2sjCj6xjzJKey3RB7phmzx+k4AVAhyMoAqiu2hBrpfmNtc7hq3SuNMju\n3NkaywxR8AKgwxEUAVS3aVPlIdas+Y1r10of//jkgPnxj1cOi3nmSjZimaFGFKG08rqYANAABEUA\nlQ0Px7fpk5Lgt2JFsgj2vHlJ1XNpcNu1SzpxYvIxJ05M3mWlXFYIlCYHO6n+ZYYaVYTSyutiAkAD\nEBQBVFapMMNsImyNjSWVzw89NBHcsgKmVDmclc+VlKoHu+luP1hvEUqrrosJAA1AUARQWaXCjPIF\nr48dS4ap08BWTd5wVi3YNWr7QYpQAGASgiKAymotzBgbmwhseeQJZ9WCXVaQHByUhoe1dM+eyb2N\nixbFz0cRCgBMMnumLwBAixsaSgpQSucazpkjnX125aHlvPKEs76+JHxmHZsVJE+dkj7+cf3MqVPJ\nayk5zxlnJPdQek8UoQDAFPQoApiqdL7fpk3SW2XbvbtL110X3+u5kvIFs/OGs2rVxZXC5okTmpWG\nxNTJk0nQpQgFACoiKAKdrKzAY+mePdW/v3hxUpVcWqRy+vTk7508KT32WDK0W223FCn5zs6dEwtm\n1xrOqlUXx4JkNePjFKEAQBUMPQOdKi3wSOfujY5q5Z13Su9+dzwUlX+/moMHk7CYZy6iezKPcDqB\nbGAg+9i0/YYbpobaLMxHBICq6FEEOlWkwKPn+PGkFzC2hEysIKSSvr7aqoSLrigeGJAWLox+NCXK\nMh8RAHIhKAKdqlKBR2wJmVixSJY0aNXSK9eMHrzx8ezPmI8IADUjKAKdKk8wK12LsKcn33l7eyeC\nVt65gUX04MUW2M645+PLljEfEQDqQFAEOlXeEJf2PJZXBmcZH5e+9a3kdVpkUilk9vQ0vgcva4Ht\ntWuj1dE/uPnmxv02AHQRgiLQLmrZok6aWimcFebSXrj+/nzX4S7de+/E7w8MVC4g2bGj8T14WQts\nP/ZYtDr68Jo1jf19AOgSBEWgHdS6RV2qdB/iHTt0au7cyZ+nQ8LDw9LRo/mvJ61iTmUNc/f2FjPM\nW2mnFvZeBoCGISgC7aDSFnU19DDu+/SnpxZ1SEnorHWXldLil6wFse++u7Zz5pUVTFnyBgAaiqAI\ntINaK5gzHF6zZmpv26ZNtS2Lkyodyq62IHZMrUPppart1AIAaAiCItAOaq1gzmt4uP79msuLX2oZ\n8q13KL30t2oNpgCAmhEUgXawdm2+76U9j3l762oNlqXyFr9k/W5sKL2W64kF0+n0UgIApmALP6Ad\nPPZYvu/19UW37tOGDcnr5csnf7/e3VKmO8xbqRilXrXcNwAgF3oUgXaQJ0Cl4a2W3rp6ij8aMcxb\nRDFKI3opAQCTEBSBVlFp2DQrQPX0TJ2jV0tvXd5FuVNmjVlypohilCJ6KQGgyxEUgVZQrbgjK1jt\n2DG1eKTW3rp58/JfZ+k5pjMfsIhiFJbMAYCGIygCraDasGneYJW1cHasty4Np3mrnkvPEQu269dL\nixfXVrncyIWxWTIHABqOoAi0gjzDptWCVVbw6+2Nh8pYOK1kcHDiHFnHjo3VtsxNI7FkDgA0HEER\naAV5hk2zhnrT9vXr4+FtwYJ4WKp17t6OHRO/WenYdMeYmQqLbN8HAA1DUARaQbVh06w5jBs3TrRn\nKQl1S/fsmQibs2r8r/+xY0kYXbFCWrSo8ndPnZq4PtY1BIC2RVBE52qnxZerDZtmzWHcurX68PGi\nRcn9m+ndQ0MTYbN8Z5W8RkelI0ekOXMqf+/YMenee+vffQUAMONYcBudqdLiy606HDkwkH1t9S7x\nMmdOEurCvEWr89KmOHkymfsoVS6GcZ/8Pi3QadV/BgCASQrrUTSzM83sKTP7ezN71sz+Q2hfZGa7\nzWx/eF5YcsxtZnbAzPaZ2ZUl7ZeY2TPhs3vMzEL7XDP7cmh/0sxWlBwzGH5jv5kNFnWfaFGdtvhy\nvUu8nDyZPIowPi69+qq0c2eynmNerGsIAG2jyKHn45J+1d3fK+l9kq4ys8skfUbSXne/SNLe8F5m\ntkrSOknvkXSVpC1mlv6/z1ZJn5B0UXhcFdpvkvSau18o6S5Jd4RzLZJ0u6QPSLpU0u2lgRRdoNMW\nXx4aqj7UG1Peo9dIs2YlPbcDA0mhS/kcS8vov2RdQwBoG4UFRU+kC7qdER4u6WpJO0L7DkkfCa+v\nlvQldz/u7j+UdEDSpWZ2rqRz3P0Jd3dJD5Ydk57rYUmXh97GKyXtdvdxd39N0m5NhEt0g05ZfDmd\nZ3n99cX1DNYrLVhJw2L5HMtPfpJ1DQGgzRU6RzH0CP6tpAsl/V/u/qSZLXP3l8NXfiRpWXi9XNIT\nJYe/FNpOhtfl7ekxL0qSu79lZq9L6i1tjxxTen0bJG2QpCVLlmhkZKS+G21jR48e7cj7Xrp+vVbe\nead6jh9/u+3U3Lnat369Do+MtMV9L92zZ8o9FOV0T4/eOussnXHkiKT4XEaPtR87pjd/7/f0xPLl\n0vLl0he/OOnjpYsW6YL77tPcw4d1fOlS/eDmm3V4+XKpyX/7dvjnXQTuu7tw3yhCoUHR3U9Jep+Z\nvVPSn5vZxWWfu5kVODZWmbtvk7RNklauXOmrV6+eqUuZMSMjI+rI+169Wnr3u5M5iQcPSn196hka\n0qqBAa1SC9738PCka9XQUDL3r8iQ2NOTrDfY16dZQ0OaI0k33pjZc5lVCHPm4cPZf8vVq6XPfz75\nnqRV4dFsLffPu0m47+7CfaMITal6dvcfm9nfKBn+fcXMznX3l8Ow8uHwtUOS3lVy2Hmh7VB4Xd5e\nesxLZjZb0jskjYX21WXHjDTyntAGKlURt5KsCu1adk2p1fz5U3ctWbGivuHtdhvOBwDkVmTV85LQ\nkygzmyfpQ5K+J+lRSWkV8qCkR8LrRyWtC5XM5yspWnkqDFMfMbPLwvzDG8qOSc91jaTHwzzGb0i6\nwswWhiKWK0IbMHOy1nXMqtCupZK4mt5eqbdXXmlru3oLfdaunf71AQBaUpFVz+dK+hsz+wdJ31ZS\nXPKXkr4g6UNmtl/SmvBe7v6spF2SnpP0dUm3hqFrSdoo6T4lBS7fl/S10H6/pF4zOyDpdxUqqN19\nXNLnwu9+W9JnQxtQm0Yt2p21s8rwcHZAO3VqajFIPfr7k2VsXn1V33z88eyt7ertGXzssWldHgCg\ndRU29Ozu/yDp5yLtY5IuzzhmSNKUkkh3f1rSxZH2NyVdm3Gu7ZK213bVQIlGLtqd1Wu4aVMSQmO7\npPT3J3MVN22qvKh1as6cJISWDh9nVRnH5kQODVWco5ipXZccAgBUxRZ+QJZGLtqdFabGxuIhMQ14\nAwMTi1r391f+je3bpQceyN4GcHhYl61bl3x2/fVTezel5Pha94BmjiIAdCy28ANihoeTABVTTw9a\nX1/2+cr19EydQ5gW5syeXXmP5qwCntA7emYafLO21nvhheT99dfHF+s2m9zOuogA0NHoUQTKpUPO\nWerpQRsayj/f8PTp7KHtSiGxUk9nrHe0XBqABwayd3Rxz+6xBAB0HIIiUK5SqKq3B61855JKsoLo\n8HDlY0dHswtu8vSClv5uVsV1T0/S63j6dHZRDACgYxAUgXKVQtV0etAGBiZCViVZy81s2lR97+bS\naupS1XpBywNwVs9lpR5NAEDHISiiuRq13EyRskJVf3/jetAqrZF4773Sxo2T24aH81U+S0lv6Pr1\nk/++saHvtHcyNoScVThTraAGANBRCIponkprCbaSWKhqdNFGpTmQ7klYLP271FNpXfr3DUPfby4L\nW6v39CS/09OTfG/z5sm/14y/AQCg5REU0TyNXG6mSOXzCYso2tiyRbrlluzP3ZNewdmzk2vIWzFd\nrvTvOzCgH9x8cxL40iHk9Lk8tDfjbwAAaHkERTRP1ty/Ri3Y3Mhh7dL5hI0q2ti4cSL4zQ4rU1Ub\nym3EnMCSv+8F992XXahTHtqL+BsAANoKQRHNkzX3rxELNseGta+/fupcv5mycaO0devknrytW6UL\nLyz+t0v+vnMPH678XXZZAQCUICiieYqc9xYb1o7N9atXLb2Vse9u2xb/7shIsvVeUcr+vseXLq38\nfXZZAQCUICiieYqc95bVE+Y+/TmQtRThxL57442Vl5s5cWJ615clssPL23MUYyhWAQCUISiiuYqa\n91apJ2y6w6m1FOHEvnvyZPa5Ky2TM12RHV4Or1mThMfe3snf7e2lWAUAMAVBEZ1haCh715LpDqfW\nUoRTayjdsCEZoi5Cpft+443K7wEAEEERnWJgQPrkJ6eGxUYMp9ZShFNLKL3llmSZnGo7tdQr674b\nuUxROyygDgCoG0ERnWPLFumhhxo/B7KWIpzYd2N6e6XHHksCVq3Dz2edVf07t9ySfd+NWqaoXRZQ\nBwDUjaCIzlLEHMhainDS75bPASw1Z4505MhEwKp1rcQzz6y8Hd/OnUlozpLV6zlrVm09g+2ygDoA\noG4ERSCPWADNGnb91rek8fHJx5cGubPPrlzgUs3YmDRvXhJG0+D60ENJ6MwTjrN6PU+dqq1nsOgF\n1LMw3A0ATUNQRGdodniIDbt+/OPSggXJQtruk7/vngS6F16YGiLrMTaWFKA89FDtPaflPaSxoe88\nPYNFLqCeheFuAGgqgiLa30yEh9iw64kT0k9/mn3M6OjE8G4jHDsmbdpUX0Au7SHNKqap1jNY5ALq\nWRjuBoCmIiii/c1EeKh3eLWeOYmVjI1N7dVcvLi24Fhvz2CRC6hnmanhbgDoUgRFtL+ZCA+tutXd\niRNJeKylZ3U6PYNFLaCeZSaGuwGgixEU0f5mIjzkXQZnpuXpWZ2JnsF6zcRwNwB0MYIi2l9R4SEt\nkDGTZs9OntPh3PJw1dtb3A4r05WnZ7XZPYP1aqdQCwAdYPZMXwAwbWlI2Lw5CUV9fUlInE54SAtk\n0rmP6bzCdDg3Ztas4nZZmY5OG5YdGCAYAkCTEBTRGRoZHoaHpcHB7KKTY8ek3/qt5DldBmdsrDG/\nXS8zadGiZCHv0jUaGZYFAExDi46VATMk7UmsVpn8059OXStxpvT3Jz2Zr74qPfAAw7IAgIYhKKL9\nlC6uvXhx7cvBZJ1rxYpkXcLypXZa3dGjE/fdLnMNAQBtgaFntIfh4WQO4uho0lsWG/ItnT8YC0jp\nOQ4e1GVLl0of/ai0Y8dEMBwdLfYeijI2Vvm+AQCoEz2KaH2lO69IlYd80+VgynsKN26ctHvLma+8\nIt17b+v0Hsa20asFu5MAAApAjyJaX2znlUrSnsXSnsJ7743vv9wK3CsvrZNWU/f2Ju+zCmfYnQQA\n0GD0KKL11RqAenqmBstWCYUxw8OVl7B58MHk+l99NXn098e/l+4jXc9cTQAAIgiKaH21rAN4xhnT\n20t5zpyJnrtmWb8+KUjJUj6knLUrzKlTtW3dBwBAFQRFtL5YMDJLnnt7k0e6O0raHlPps5lWaR3G\n8h7V8t1JYvMbmbMIAGgAgiKKU15QUm8PV2zbtocemjwce/q0tGCBdOJE/Bxz5kif/GT13sITJ2Z+\n8exyixYlz6V/z82bkwB9+nT2bjDMWQQATBPFLChG+RZ41ZauqSbPziuVglE6R/HIkdp/e6b95CdJ\n1Xb5Uj7p37OvL760T6dt3QcAaDp6FFGMWKVy0cOhlYLRyZPS1q2Tt7drFydOJNee9feMDc2zdR8A\noAEIiihGVu/e6GhxRRZZRR6tYufOxs+TPHgwPjTP1n0AgAZg6BnFyBoOlYrbRSQ93+Dg9Cqfi5T1\nd5k1KylKqbXHM+1FzTM0DwBAjehRRDEq9e4VOQQ9MJDM5WvFnsUNG6S1a5PCmlJz5iRrJT7wQG1L\n85xxBsPLAIBCERRRjHQ4NEvW0HTeSulK36v22zPl2DFp167sHWIGBpIK7p07sxfVLtXKy/0AADoC\nQRHFGRjIDjyxwpPSPZ3LF44uDYaLF0s33jj1exs3Tnxn06Yi76x+Y2NTh5dPnpx8vQMD0gsvVJ/T\neOIEayUCAApFUESxaqnIzaqU3rRpcoCMha1jx5L9nEu/007Gxqb2ng4MVN96kLUSAQAFIiiiWLVU\n5GaFnrGxqQEyppX3c85jcHDqUHq1IWjWSgQAFIigiOKlQ6mnTyfP5SExHVZu96A3XbG9misVBbFW\nIgCgYCyPg5lVvoNLufnzpXnz2m8oebrSyvAXXkjeb96cBMieniRQ9vcnIZElcQAABSIoYmbF5iWm\n0jAkVQ6TnSodimeNRADADGHoGc2RtZxN1rxEs4lh6nSeYy1rDLaKPNfc0xNvZ/4hAGCGERRRvErL\n3mSFIbPkMXt28tzOy8CUL7AtSWedlSx/4x5fIJz5hwCAFkBQRPGylr3ZvDkJQ7Egdfp08pxuxTc6\n2p7zFMfGkjDY2ztR9b1zp3T06MRwclF7NeddvBwAgAwERRQva3j54MEkDJ19dnOvp9lOnpQWLMiu\n+paqV4bXKqMXd+mePdM7LwCgqxQWFM3sXWb2N2b2nJk9a2abQvsiM9ttZvvD88KSY24zswNmts/M\nrixpv8TMngmf3WOWbFdhZnPN7Muh/UkzW1FyzGD4jf1mNljUfSKHrOHltH18vHnXMlOavTB2Ri/u\nBffd19zrAAC0tSJ7FN+S9HvuvkrSZZJuNbNVkj4jaa+7XyRpb3iv8Nk6Se+RdJWkLWaWzvLfKukT\nki4Kj6tC+02SXnP3CyXdJemOcK5Fkm6X9AFJl0q6vTSQosmq7c7SDUUbzb7HjGA69/Dh5l4HAKCt\nFRYU3f1ld/9OeP0TSc9LWi7pakk7wtd2SPpIeH21pC+5+3F3/6GkA5IuNbNzJZ3j7k+4u0t6sOyY\n9FwPS7o89DZeKWm3u4+7+2uSdmsiXKKR8s6Dmzdv4nVv7+Q5eJUWle4UF17Y3PmCGcH0+NKlxf4u\nAKCjNGUdxTAk/HOSnpS0zN1fDh/9SNKy8Hq5pCdKDnsptJ0Mr8vb02NelCR3f8vMXpfUW9oeOab0\nujZI2iBJS5Ys0cjISD2319aOHj1a930v3bNHK++8Uz3HjycNo6M6ddNN2vf88zq8Zk38O5JOHT2q\nfc8/L/3BH+iC++7T3MOHdfLsszVr3jz1vPGGbLo31YJ8796J+xodla9fr0Nf+YoOfOpThfze0vXr\np/7d587Vc+vX6wj/Oe8a3Hd34b5RhMKDopktkPRVSZ9y9yNheqEkyd3dzGZs3zZ33yZpmyStXLnS\nV69ePVOXMmNGRkZU931/7GNSSRCRpJ7jx7Vq506t+vznk16zL3xhonK59Dv33iu98cbb8+jmHDmS\nVPx2qPI7M0nnPfqozrv22mIW0169Wnr3u5O5igcPSn196hka0pHly+v/593GpvWf8zbGfXcX7htF\nKLTq2czOUBISh939z0LzK2E4WeE5nTR1SNK7Sg4/L7QdCq/L2ycdY2azJb1D0liFc6GRKlUzp1W3\nZSHxbWNjU4stWn2v50Yv+O1e7PqQja6kBgB0nSKrnk3S/ZKed/f/XPLRo5LSKuRBSY+UtK8Llczn\nKylaeSoMUx8xs8vCOW8oOyY91zWSHg/zGL8h6QozWxiKWK4IbWikStXMlbbma0f9/cVUZze7GhoA\ngBoU2aP4i5Kul/SrZvZ34bFW0hckfcjM9ktaE97L3Z+VtEvSc5K+LulWd0+7ozZKuk9Jgcv3JX0t\ntN8vqdfMDkj6XYUKancfl/Q5Sd8Oj8+GNjRSpWrmSgFo/vz2244vXTi70bqh4hsA0LYKm6Po7v+P\npk7NSl2eccyQpCn7lrn705IujrS/KenajHNtl7Q97/WiDulQZsk8OA0NJe2bNyeLPJfr6ZEGB6Vd\nu5p7rdN19Gjjz8k2fQCAFsfOLJierHlwWb2NGzYkexu343Z85fIU3/T2TmzN19s7eSu/RmzTBwBA\ngZqyPA66UGlv4+ho0pN47Jh0772tX7SSV7X7mD9fuvtuwiAAoG3Ro4hE3oWzazEwMNGzmFY/d0pI\nrIQeQwBAh6BHERNL2aRVyqOjyXupvqAzPDwxb3HWrOwlctpZb298+Ly/PxmCBwCgA9CjiPhSNseO\n1bfGXxo6R0eT3sMWDYnT6tfs7U2GlCvtXw0AQAcgKKLywtl5lA5bDw62xfqJde8BUzrvcNu2iUIV\nhpoBAB2IoWcky9rElrLJs8Zf+bB1i/YgNkR//8TyP1LyTDAEAHQwehRReeHsLGkv4vr1+XoQe3om\nlohZsGBal1sYs+Q6Y9KQuHlzYwt+AABoYQRF1D6MWjoPMa9Tp5Ieyrvvln7yE2nnzsZceyNVmlOZ\nFvikcy/T940Mi0VUngMAMA0ERSSyFs6OqXcf59HRpAdy8eLkfX9/PVc6M9J1IEvVW/ATU14EVEQQ\nBQCgRgRF1C5vkUuWsbEkBK1dK51xRmOuqUil60CWm+7fItXIynMAABqEoIjaVSpyyZrjV+7YMWnr\nVunkycZcU1F6eiaG5WPyFPzkMd3KcwAACkBQRO2yil927kz2cS7/rAXlWkdx/vzkfkp3mCn/vFHr\nJmYFzkYFUQAA6kBQRO0GBpL1EtPew56e5H26XMy2bTN7fXmYVR727u2dXNBT9LqJRQdRAADqQFBE\nXKUK3OHhpKctnbd36lQyjLx4cfJZG6wtaO4Ty/VIE6G3vz/pGX311an3UUvBT61YwBsA0IIIipiq\nWgVuVtVzWqQyPDwRwFrZiRPJmo7u0ltvJc8zuVZikUEUAIA6EBQxVVYFbrq0TaX1E9NK3euuK/Ya\nG2V0dKLndPFi6cYbWaIGAICAoIipKlXajo1VP350NBmKbhdpMBwbm1qFzRI1AIAuRlDEVFTaTsYS\nNQCALkVQxFSxCtxuRnAGAHQpgiKmKq3A7XYsUQMA6GIERSTKl8ORksrbnTulOXNm8MIaoLc3vmbi\n7NmVj2OJGgBAlyMoIr4cTlrh/K1vJW0xebfrm0lm0t13Sw88MGnJnhPnnCN98YvZy/j099e2RE2l\ndScBAGhTBMVWV08AqfWYSusi3ntv9n7M7RAU3Sd2jHn11eS9u/7fRx5J2u6+e/o7olRbdxIAgDZF\nUGxl9QSQeo6pVNWb1ZsoJQtWt7pqYbYRO6JkrTvJsjoAgDZHUGxl9QSQeo7p5KredJvBSqa7I0pW\n0EW6mQsAABbgSURBVGZZHQBAmyMotrJ6Akg9x3TycjjNqNzOCtqdHMABAF2BoNjK6gkg9RyTDr+2\nw/7MtWjW0jaxoM2yOgCADkBQbGX1BJAaj1m6Z09S8HL99dKCBdItt0zvmltFM5e2acQ8RwAAWlCV\nheQwo9KgsXlzMnTc15cEvkoBpJZjhoe18s47pePHk/ejo9KOHUm19OnTjb2XZnvhheb+XlpZDQBA\nByEotrp6AkjeYzZvVk8aElOxZXLaTTss2wMAQBtg6LmbdWpVbp5KZwAAUBVBsZt1alVup+5Rze4v\nAIAmIyh2inpCxNCQTs2dO7nNrIira64LL5zpK2g8dn8BAMwAgmInqDdEDAxo36c/PdED19NTeSeW\nVjJnTvZnjz/evOtoFnZ/AQDMAIJiJ5hGiDi8Zs3EkjrtMrevv1/avj3783YJu7Vg9xcAwAwgKHaC\nrLAwOipt3DgxJL14cfIoH56OBc1W1d9f3zZ77Y7dXwAAM4Cg2AkqhYWtWyeGpMfGkkfJ8PTSPXuS\n1+2iNBQvWBD/TlZ7O2P3FwDADCAodoJ692o+dkwX3Hdfe607OGvWRE/ovfdKs8uWAp09O2nvNOz+\nAgCYASy43QnSsLB+fc2Hzn3llQZfTAPMny8NDia7xJQPiZ86lRTqSPXtXNPO2P0FANBk9Ch2ioGB\nzlg/sLc36SnbsiV5jvV2lhbqDAwkcxZPn+7OuYsAABSIoNhJ1q6t+ZCWWTWxv1/auVN69dWJsDcw\nkL3nNNW+AAAUjqDYSR57bKavoH7pkHH5wuGLFsW/T7UvAACFIyh2knbuZdu0Kb5w+E9+Ip1xxuTv\nllb7sq0dAACFISh2kqzet3YwNhZfz/HECemcc+LVvmxrBwBAoah67gazZmXP9WslWes5jo8ncxfL\nVdqRhqIWAACmjR7FdpQ13Do+Hv9+O4TESrLmI7KtHQAAhSIotpus4daNG5Pg2Gkq7T7SStvaMVcS\nANCBOjBZdLis4dZ7700Wo2431dZ+rLT7yNCQNGfO5LY5c5q/rR1zJQEAHYqg2G6yhlXdm3sdjWCW\nLJKdFRb7+6vPNSy/7/R9M3v4Ks2VBACgjVHM0m76+rKLPtpNX18S4I4enfpZpSHn1ObN0smTk9tO\nnkyW2nnjjYnwlvbwScUUuTBXEgDQoQrrUTSz7WZ22My+W9K2yMx2m9n+8Lyw5LPbzOyAme0zsytL\n2i8xs2fCZ/eYmYX2uWb25dD+pJmtKDlmMPzGfjMbLOoeZ8TQUBKiSlnL7K9Sm7VrkwA3Nja5Pd3G\nr1qoywpiY2PN7eFrpbmSAAA0UJFDz1+UdFVZ22ck7XX3iyTtDe9lZqskrZP0nnDMFjNLN/ndKukT\nki4Kj/Sc/397dx9sV1Xecfz7IwlIgkByIUwa4AKN4CDVCEyUopFCRKGO9kWn0EvrC2mU+IJa29FB\na23LVK3TirVBY4jEBEGkVimDYgKkMApKkAQTMQIhKIwSDRHEl0CSp3/sdcjOyT7nnntvzjn77v37\nzJw5+6z9tp57b26eu9Zea10IbIuIWcB/AB9L15oGfBh4CTAH+HA+IR33hoayJCo/r+Db3rZ38lh2\n++0H1167d0IHcNBBnbX8jTQR61YLX1Hy3kmLqJmZWcl1LVGMiNuA5vlaXgcsS9vLgD/JlV8TEdsj\n4iHgAWCOpBnAwRFxZ0QE8IWmcxrXug44K7U2vgpYGRGPR8Q2YCV7J6zlMtLn6YaGsmf7du3K3hct\nypLHCRPan1cmu3bt3ZLY0GlC1ypBGxgoPr5bLXxFyXsnLaJmZmYl1+tnFI+IiJ+m7Z8BR6TtmcCd\nueMeSWXPpO3m8sY5PwGIiB2SngAG8uUF5+xB0gJgAcDhhx/O6tWrRxXUWExftYoTPvEJJmzfnhU8\n/DA7L7yQjffdx5Z58zq+xnFLlnDAzp2UpRP66YMPZtKTT7atT0Dh/t9Nn86dnXwvZs5k+nvek8W+\nZQvbp09n0/z5AHt+TYGdBxzAxgsuYEu67lNPPbVvv98zZ8KVV+5Z1oefp+Hs87jHCcddL467Xuoa\nd89ERNdewDHA+tznXzbt35bePw1ckCu/Ang9cCqwKlf+cuCGtL0eODK370HgMOB9wAdz5R8C3jdc\nXY8//vjoi8HBiGys7p6vwcHOzl+xImLy5OJr9Ou1335ZvS66qP1xAwN7133y5OzcsVqxIvsaStl7\n0zVvvfXWsd9jHHLc9eK468Vx1wuwJrqYwzVevZ4e57HUnUx635LKHwWOyh13ZCp7NG03l+9xjqSJ\nwCHA1jbXKqfRjphtdFdfcEHxc379NHVq1u26aFHrbmAJLruse122zd3z7gY2MzMbsV4nitcDjVHI\nbwS+lis/L41kPpZs0Mp3I+umflLSS9Pzh3/ddE7jWq8HbkkZ9k3A2ZKmpkEsZ6eychrNiNn8BM9l\nlF9KsNWyghG7RyE7oTMzMyulbk6PczVwB3CCpEckXQh8FHilpPuBeekzEbEBuBb4AfAN4O0R0Vhm\nZCGwhGyAy4PA11P5FcCApAeA95JGUEfE48A/A3el1z+lsnIazYjZogmeyySf5LZLeL2CiZmZWal1\nc9Tz+RExIyImRcSREXFFRGyNiLMi4nkRMS+fwEXEpRHx+xFxQkR8PVe+JiJOSvvekVoNiYjfRcQb\nImJWRMyJiE25c5am8lkR8fluxbhPjGbEbJkncs4nua0m085rNb+h1042MzPrO6/MUgZDQyPrci3r\n6iyDg1mSODS0u3u8k5bP5sS3+dxur6xiZmZmhbzW83h06aVZS1vZNJJEGFn3eHP3tNdONjMzK4US\nZhs2rLK2quUTuU67x4uex/TayWZmZqXgRHG82rWrP/edNKn1vnwi1+kqKEXPY3rtZDMzs1Jwolgm\nww3gWLgQJk7MBr30y9y5nS2RVzSau9ngYHHrqNdONjMzKwUnimWRnxsxYu+pYxYuhMsvh50721+n\n226+GWbPHj6Ry4/mhr2T23aJn9dONjMzKwUnimUx3ACOxYt7X6dWVq/uLJFrrI4SAcuXjyzx88oq\nZmZmfefpccqi1UCNhx/OuqKz6SPLYefOkU/pM9LjzczMrO/colgW7QZqlClJBJgwod81MDMzsx5w\nolgWnQz+KIvG5NdmZmZWaU4Uy6J5AMdwJkyAiy7qfr2K7rlo0diu4+X5zMzMxgUnimWSH8DRGC1c\nZHAQli3LErZ2x3VgVyfdyJMnw4oV2T1vvHFsCd5wo7vNzMysNJwoltWsWa335ZOrscwtODjIjilT\nhj3m2RHX+yLB8/J8ZmZm44YTxTJo7oqdNy+br7CdRnI12pHEEmzezKRf/ar9cT/+cXafiy/eNwme\nl+czMzMbN5wo9ltRV+xwSWJDI7kaTfdzGmW9ffr09sc16rR1a/s6jPC+HZebmZlZ3zhR7LeirthO\nNZKr4bqf26yKsmn+fNh//9HdP1+HTnl5PjMzs3HDiWK/jaXL9dxzs/ehoazbupWIlquibJk3D5Yu\nbb1+czujSfC8PJ+Zmdm44USx38bS5Xrjjbu33/rW1scNDrZfDm9oCH7xiyyhbCSVRQYG9k2C5+X5\nzMzMxgUniv02lom2862Rp59e3IU8mla/Vt3Dl13mBM/MzKxGnCj2QrsJphtdsaNZFm/atN3XX7AA\nnn5672MOPHDk13X3sJmZmeFEsfs6mWB6aAh27hz9PdoNiNm6dXTzHbp72MzMrPacKHZbpxNMj6ZF\n8fHHs/fhBsR4QmszMzMbBSeK3dbpBNOjaVFsDITpZECMJ7Q2MzOzEXKi2G2N5wibNSd3I500Oz9I\npZMBMZ7Q2szMzEbIiWI3XXUVFC2RN2nS3iORO0n2BgaKB5fkB59A2wm2zczMzDrlRLGbLrmkeCTy\nwQcXz2XYbvTzwEA212G7uRA3b84GzCxf7hHLZmZmNmZOFLup1XOBjUEozYaGYNmy1nMYdqqTEctp\nyp5XnHnm3lP2mJmZmeFEsXsWLsxa94q0e16wF3MY5qbsUaspe8zMzKz2nCh2w8KFcPnlxfuk3Ws0\nt9LtOQw7nbLHzMzMas2JYjcsXtx6X0TWvdyr1ruiVWE6nbLHzMzMas2JYjcMNydir1rvWq0K0+mU\nPWZmZlZrThS7oZNVVnrReteqixmKB8x4Ch0zMzPLcaLYDQsWDH9ML1rv2o26TgNmwlPomJmZWQtO\nFLth0SK46KLWLYu9ar1rlYweffSzA2b+75ZbRj9gpuj5RzMzM6sMJ4rdsmgR7NiRPRu4YkV/JsAu\nWu1lXyWprZ5/dLJoZmZWGU4Ue6Hb0920u2+35mT0FDtmZmaVN7HfFbAuGxrqTmLqKXbMzMwqzy2K\nNjrtnn80MzOzSnCiaKPTzecfzczMrBScKNbJvhyl3Is1qc3MzKyvnCjWRYtRytNXrRr9Nfs1SMfM\nzMx6woliXbQYpXzckiX9qY+ZmZmVnhPFumgxGvmALVt6XBEzMzMbL5wo1kWL0cjbp0/vcUXMzMxs\nvHCiWBctRilvmj+/P/UxMzOz0nOiWBctRilvmTev3zUzMzOzkvLKLHVStErL6tV9qYqZmZmVn1sU\nzczMzKyQE0UzMzMzK+RE0czMzMwKOVE0MzMzs0JOFM3MzMysUKUTRUmvlrRR0gOS3t/v+piZmZmN\nJ5VNFCVNAP4LOAc4EThf0on9rZWZmZnZ+FHZRBGYAzwQEZsi4mngGuB1fa6TmZmZ2bhR5Qm3ZwI/\nyX1+BHhJ/gBJC4AF6eN2Set7VLcyOQz4Rb8r0QeOu14cd7047nqpa9wn9OImVU4UhxURi4HFAJLW\nRMSpfa5SzznuenHc9eK468Vx14ukNb24T5W7nh8Fjsp9PjKVmZmZmVkHqpwo3gU8T9KxkvYHzgOu\n73OdzMzMzMaNynY9R8QOSe8AbgImAEsjYkObUxb3pmal47jrxXHXi+OuF8ddLz2JWxHRi/uYmZmZ\n2ThT5a5nMzMzMxsDJ4pmZmZmVsiJIuNzqT9JSyVtyc/9KGmapJWS7k/vU3P7PpDi2yjpVbnyUyR9\nP+37lCSl8gMkfSmVf0fSMblz3pjucb+kN/Ym4mfvfZSkWyX9QNIGSRen8krHLuk5kr4raV2K+yN1\niDt3/wmS7pF0Q/pc+bglbU71XduYBqMmcR8q6TpJP5R0n6TTqh63pBPS97nxelLSu6sed7r3e5T9\nTlsv6Wplv+vqEPfFKeYNkt6dysoZd0TU+kU20OVB4Dhgf2AdcGK/69VBvecCJwPrc2UfB96ftt8P\nfCxtn5jiOgA4NsU7Ie37LvBSQMDXgXNS+ULgM2n7POBLaXsasCm9T03bU3sY9wzg5LT9XOBHKb5K\nx57qeFDangR8J9W90nHn4n8v8EXghhr9rG8GDmsqq0Pcy4D5aXt/4NA6xJ2LfwLwM2Cw6nGTLYzx\nEHBg+nwt8KYaxH0SsB6YTDaoeBUwq6xx9/QfQBlfwGnATbnPHwA+0O96dVj3Y9gzUdwIzEjbM4CN\nRTGRjQQ/LR3zw1z5+cBn88ek7Ylks94rf0za91ng/D5+Db4GvLJOsadfLt8jW2mo8nGTzYF6M3Am\nuxPFOsS9mb0TxUrHDRxCljioTnE3xXo28K06xM3uFdSmpTrdkOKvetxvAK7Iff4Q8Pdljdtdz8VL\n/c3sU13G6oiI+Gna/hlwRNpuFePMtN1cvsc5EbEDeAIYaHOtnktN6S8ma12rfOzKul/XAluAlRFR\ni7iBT5L9Et2VK6tD3AGsknS3suVGofpxHwv8HPi8skcNlkiaQvXjzjsPuDptVzruiHgU+ATwY+Cn\nwBMR8U0qHjdZa+LLJQ1ImgycS7ZASCnjdqJYUZH9qRD9rke3SDoI+G/g3RHxZH5fVWOPiJ0RMZus\nhW2OpJOa9lcubkmvAbZExN2tjqli3MnL0vf7HODtkubmd1Y07olkj9RcHhEvBn5N1gX3rIrGDYCy\nxSFeC3y5eV8V407P4L2O7A+E3wOmSLogf0wV446I+4CPAd8EvgGsBXY2HVOauJ0oVmupv8ckzQBI\n71tSeasYH03bzeV7nCNpIlmX0NY21+oZSZPIksSrIuIrqbgWsQNExC+BW4FXU/24TwdeK2kzcA1w\npqQVVD/uRmsLEbEF+B9gDtWP+xHgkdRaDnAdWeJY9bgbzgG+FxGPpc9Vj3se8FBE/DwingG+Avwh\n1Y+biLgiIk6JiLnANrLn7csZdy/648v8IvsLdhPZXzSNwSwv6He9Oqz7Mez5jOK/seeDsB9P2y9g\nzwdhN9H6QdhzU/nb2fNB2GvT9jSyZ4imptdDwLQexizgC8Anm8orHTtwOHBo2j4QuB14TdXjbvoa\nnMHuZxQrHTcwBXhubvvbZH8YVDrudP/bgRPS9j+mmCsfd6rDNcCbc58rHTfZc9YbyJ67FtlApndW\nPe50/+np/Wjgh2SDtkoZd8/+AZT5RfZ8wI/IRhJd0u/6dFjnq8me6XiG7K/wC8meP7gZuJ9sFNW0\n3PGXpPg2kkZFpfJTyZ6XeBD4NLtX63kOWffHA+kH8bjcOW9J5Q+Q+6XWo7hfRtYcfy9Zc/3a9P2r\ndOzAC4F7UtzrgX9I5ZWOu+lrcAa7E8VKx002C8O69NpA+r1U9bjTvWcDa9LP+lfJ/jOrQ9xTyFp8\nDsmV1SHuj5AlSuuB5WTJUB3ivh34Adm/8bPK/P32En5mZmZmVsjPKJqZmZlZISeKZmZmZlbIiaKZ\nmZmZFXKiaGZmZmaFnCiamZmZWSEnimY2rknaKWmtpPWS/lfSoWO41mZJhxWUPzW2WvaOpFMlfarf\n9TCzanCiaGbj3W8jYnZEnAQ8TjbRbC1JmhgRayLiXf2ui5lVgxNFM6uSO8gtcC/p7yTdJeleSR/J\nlX9V0t2SNkha0MmFJV0qaZ2kOyUdkcqOkXRLuv7Nko5O5VdKen3u3KfS+wxJt+VaQF+eys+WdIek\n70n6clrLvPn+qyVdljt3Tir/R0nLJX0LWC7pDEk3pH0HSfq8pO+nOv55p/czMwMnimZWEZImAGcB\n16fPZwPPI1sjeTZwiqS56fC3RMQpZKsavEvSwDCXnwLcGREvAm4D/iaV/yewLCJeCFwFDNfl+5fA\nTRExG3gRsDZ1dX8QmBcRJ5OtSvLeFudPTucuBJbmyk9M55/fdPyHgCci4g9SHW8Z4f3MrOYm9rsC\nZmZjdKCktWQtifcBK1P52el1T/p8EFnieBtZcvinqfyoVL61zT2eBm5I23cDr0zbpwF/lraXAx8f\npq53AUslTQK+GhFrJb2CLNH7liTI1py/o8X5VwNExG2SDs49j3l9RPy24Ph5ZOu8ks7bJuk1I7if\nmdWcE0UzG+9+GxGzJU0GbiJ7RvFTgIB/jYjP5g+WdAZZAnVaRPxG0mqydVHbeSZ2r3e6k+F/d+4g\n9dhI2o8sGWskeHOBPwaulPTvwDZgZUFrYJHmNVcbn3/dwbkNGsH9zKzm3PVsZpUQEb8B3gX8raSJ\nZEnjWxrP30maKWk6cAiwLSWJzwdeOobbfpvdLXZDwO1pezNwStp+LTAp1WEQeCwiPgcsAU4G7gRO\nlzQrHTNF0vEt7vcX6ZiXkXUpPzFM/VaSG9wjaeoI72dmNedE0cwqIyLuAe4Fzo+IbwJfBO6Q9H3g\nOuC5wDeAiZLuAz5KljiN1juBN0u6F/gr4OJU/jngFZLWkXVPN1r8zgDWSbqHLOm7LCJ+DrwJuDpd\n5w7g+S3u97t07meACzuo378AU9Pgl3XAH43wfmZWc9rdm2JmZmWVusjfFxFr+l0XM6sPtyiamZmZ\nWSG3KJqZmZlZIbcompmZmVkhJ4pmZmZmVsiJopmZmZkVcqJoZmZmZoWcKJqZmZlZof8H106uy9JV\nkdsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2beb2d12208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "preds_train_houses = model.predict(all_input_feats_train)\n",
    "print(\"preds_train_houses:\\n\", preds_train_houses)\n",
    "preds_train_houses_dollar = scaler_saleprice.inverse_transform(preds_train_houses)\n",
    "print(\"preds_train_houses_dollar:\\n\", preds_train_houses_dollar)\n",
    "print(\"Shape of preds_train_houses is\", preds_train_houses.shape)\n",
    "print(\"Shape of preds_train_houses_dollar is\", preds_train_houses_dollar.shape)\n",
    "plt.figure( figsize=(10,10) )\n",
    "plt.plot(train_output_matrix, preds_train_houses_dollar, 'ro')\n",
    "plt.xlabel('Real house price', fontsize = 10)\n",
    "plt.ylabel('Predicted house price', fontsize = 10)\n",
    "plt.grid(True)\n",
    "plt.xlim(0,900000)\n",
    "plt.ylim(0,900000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Predicting house prices for the Kaggle competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_house_ids has shape (1459,)\n",
      "preds_test_houses_dollar has shape (1459,)\n",
      "        Id      SalePrice\n",
      "0     1461   91538.976562\n",
      "1     1462   98821.296875\n",
      "2     1463  163718.218750\n",
      "3     1464  163105.546875\n",
      "4     1465  192769.843750\n",
      "5     1466  204428.640625\n",
      "6     1467  174764.031250\n",
      "7     1468  160659.125000\n",
      "8     1469  169847.000000\n",
      "9     1470  124092.062500\n",
      "10    1471  205042.406250\n",
      "11    1472   87996.960938\n",
      "12    1473   99797.578125\n",
      "13    1474  129074.554688\n",
      "14    1475   89230.039062\n",
      "15    1476  428196.156250\n",
      "16    1477  282169.625000\n",
      "17    1478  239056.703125\n",
      "18    1479  344149.750000\n",
      "19    1480  382900.000000\n",
      "20    1481  339308.437500\n",
      "21    1482  183605.109375\n",
      "22    1483  136492.296875\n",
      "23    1484  165348.406250\n",
      "24    1485  174851.750000\n",
      "25    1486  183093.687500\n",
      "26    1487  257906.687500\n",
      "27    1488  241330.375000\n",
      "28    1489  190228.546875\n",
      "29    1490  229323.718750\n",
      "...    ...            ...\n",
      "1429  2890   56540.437500\n",
      "1430  2891  145105.359375\n",
      "1431  2892   84174.960938\n",
      "1432  2893    9809.241211\n",
      "1433  2894   14148.106445\n",
      "1434  2895  360832.531250\n",
      "1435  2896  281582.156250\n",
      "1436  2897  217335.828125\n",
      "1437  2898  116856.578125\n",
      "1438  2899  197839.640625\n",
      "1439  2900  164302.296875\n",
      "1440  2901  146797.234375\n",
      "1441  2902  193866.687500\n",
      "1442  2903  306128.687500\n",
      "1443  2904  357040.281250\n",
      "1444  2905   56994.558594\n",
      "1445  2906  131387.281250\n",
      "1446  2907  117380.398438\n",
      "1447  2908  141889.078125\n",
      "1448  2909   86635.132812\n",
      "1449  2910  108258.976562\n",
      "1450  2911  100460.585938\n",
      "1451  2912  191160.656250\n",
      "1452  2913   54035.562500\n",
      "1453  2914   92335.109375\n",
      "1454  2915   96019.843750\n",
      "1455  2916   73244.140625\n",
      "1456  2917  155172.531250\n",
      "1457  2918  123100.210938\n",
      "1458  2919  196495.171875\n",
      "\n",
      "[1459 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# PREDICT house prices for all the test houses!\n",
    "preds_test_houses = model.predict(all_input_feats_test)\n",
    "preds_test_houses_dollar = scaler_saleprice.inverse_transform(preds_test_houses)\n",
    "\n",
    "\n",
    "# generate a Pandas dataframe\n",
    "# from the NumPy prediction_matrix\n",
    "preds_test_houses_dollar = preds_test_houses_dollar.reshape(-1)\n",
    "print(\"test_house_ids has shape\", test_house_ids.shape)\n",
    "print(\"preds_test_houses_dollar has shape\", preds_test_houses_dollar.shape)\n",
    "predition_dataframe = pd.DataFrame({'Id'       :test_house_ids,\n",
    "                                    'SalePrice':preds_test_houses_dollar}\n",
    "                                  )\n",
    "\n",
    "# convert column \"Id\" to int64 dtype\n",
    "predition_dataframe = predition_dataframe.astype({\"Id\": int})\n",
    "print(predition_dataframe)\n",
    "\n",
    "# now save the Pandas dataframe to a .csv file\n",
    "PREDICTION_FILENAME = \"my_predicted_house_prices.csv\"\n",
    "predition_dataframe.to_csv(PREDICTION_FILENAME, sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 8. Reference algorithm: Nearest Neighbour Regression\n",
    "\n",
    "A valid question is: How good is the MLP approach?\n",
    "\n",
    "For this, we will follow a straightforward approach here: a new 311D feature vector v will be compared with all the 1460 existing 311D vectors w of the training dataset, we determine the \"most similar\" one wbest and take the house price of wbest as the predicted house price for v.\n",
    "\n",
    "This is a simple approach a real estate agent could follow:\n",
    "\n",
    "\"You want to know for what you can sell your hourse approximately? Ok, let's look in my database of houses I already sold. Your house is most similar to this one. And this house was sold for $X.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1459 for which I will predict sale prices.\n",
      "There are 1460 train data houses.\n",
      "house # 1461 --> $ 129000\n",
      "house # 1462 --> $ 158000\n",
      "house # 1463 --> $ 180000\n",
      "house # 1464 --> $ 178000\n",
      "house # 1465 --> $ 192000\n",
      "house # 1466 --> $ 189000\n",
      "house # 1467 --> $ 206000\n",
      "house # 1468 --> $ 189000\n",
      "house # 1469 --> $ 197500\n",
      "house # 1470 --> $ 97000\n",
      "house # 1471 --> $ 224000\n",
      "house # 1472 --> $ 88000\n",
      "house # 1473 --> $ 88000\n",
      "house # 1474 --> $ 146000\n",
      "house # 1475 --> $ 99500\n",
      "house # 1476 --> $ 252000\n",
      "house # 1477 --> $ 198900\n",
      "house # 1478 --> $ 256300\n",
      "house # 1479 --> $ 378500\n",
      "house # 1480 --> $ 611657\n",
      "house # 1481 --> $ 236000\n",
      "house # 1482 --> $ 192500\n",
      "house # 1483 --> $ 186000\n",
      "house # 1484 --> $ 192000\n",
      "house # 1485 --> $ 188500\n",
      "house # 1486 --> $ 189000\n",
      "house # 1487 --> $ 582933\n",
      "house # 1488 --> $ 287090\n",
      "house # 1489 --> $ 185500\n",
      "house # 1490 --> $ 220000\n",
      "house # 1491 --> $ 215200\n",
      "house # 1492 --> $ 83000\n",
      "house # 1493 --> $ 139000\n",
      "house # 1494 --> $ 236000\n",
      "house # 1495 --> $ 236000\n",
      "house # 1496 --> $ 236500\n",
      "house # 1497 --> $ 151000\n",
      "house # 1498 --> $ 151000\n",
      "house # 1499 --> $ 155000\n",
      "house # 1500 --> $ 155000\n",
      "house # 1501 --> $ 172500\n",
      "house # 1502 --> $ 151000\n",
      "house # 1503 --> $ 395192\n",
      "house # 1504 --> $ 236000\n",
      "house # 1505 --> $ 203000\n",
      "house # 1506 --> $ 185750\n",
      "house # 1507 --> $ 190000\n",
      "house # 1508 --> $ 167900\n",
      "house # 1509 --> $ 154000\n",
      "house # 1510 --> $ 139000\n",
      "house # 1511 --> $ 142000\n",
      "house # 1512 --> $ 142000\n",
      "house # 1513 --> $ 140000\n",
      "house # 1514 --> $ 179000\n",
      "house # 1515 --> $ 205000\n",
      "house # 1516 --> $ 127000\n",
      "house # 1517 --> $ 133000\n",
      "house # 1518 --> $ 124000\n",
      "house # 1519 --> $ 206000\n",
      "house # 1520 --> $ 142000\n",
      "house # 1521 --> $ 139000\n",
      "house # 1522 --> $ 159000\n",
      "house # 1523 --> $ 122000\n",
      "house # 1524 --> $ 117500\n",
      "house # 1525 --> $ 83000\n",
      "house # 1526 --> $ 80000\n",
      "house # 1527 --> $ 134800\n",
      "house # 1528 --> $ 155000\n",
      "house # 1529 --> $ 154000\n",
      "house # 1530 --> $ 162000\n",
      "house # 1531 --> $ 167500\n",
      "house # 1532 --> $ 98000\n",
      "house # 1533 --> $ 117500\n",
      "house # 1534 --> $ 119000\n",
      "house # 1535 --> $ 147000\n",
      "house # 1536 --> $ 68400\n",
      "house # 1537 --> $ 82000\n",
      "house # 1538 --> $ 147000\n",
      "house # 1539 --> $ 128000\n",
      "house # 1540 --> $ 55000\n",
      "house # 1541 --> $ 110000\n",
      "house # 1542 --> $ 161750\n",
      "house # 1543 --> $ 155000\n",
      "house # 1544 --> $ 80000\n",
      "house # 1545 --> $ 128000\n",
      "house # 1546 --> $ 165500\n",
      "house # 1547 --> $ 113000\n",
      "house # 1548 --> $ 110000\n",
      "house # 1549 --> $ 134900\n",
      "house # 1550 --> $ 166000\n",
      "house # 1551 --> $ 100000\n",
      "house # 1552 --> $ 129500\n",
      "house # 1553 --> $ 96500\n",
      "house # 1554 --> $ 143000\n",
      "house # 1555 --> $ 132000\n",
      "house # 1556 --> $ 128000\n",
      "house # 1557 --> $ 75500\n",
      "house # 1558 --> $ 83000\n",
      "house # 1559 --> $ 76500\n",
      "house # 1560 --> $ 95000\n",
      "house # 1561 --> $ 136500\n",
      "house # 1562 --> $ 144900\n",
      "house # 1563 --> $ 145000\n",
      "house # 1564 --> $ 180500\n",
      "house # 1565 --> $ 172500\n",
      "house # 1566 --> $ 260000\n",
      "house # 1567 --> $ 141000\n",
      "house # 1568 --> $ 236000\n",
      "house # 1569 --> $ 134800\n",
      "house # 1570 --> $ 132000\n",
      "house # 1571 --> $ 133000\n",
      "house # 1572 --> $ 153000\n",
      "house # 1573 --> $ 205000\n",
      "house # 1574 --> $ 60000\n",
      "house # 1575 --> $ 159000\n",
      "house # 1576 --> $ 290000\n",
      "house # 1577 --> $ 186000\n",
      "house # 1578 --> $ 105900\n",
      "house # 1579 --> $ 147000\n",
      "house # 1580 --> $ 222500\n",
      "house # 1581 --> $ 154000\n",
      "house # 1582 --> $ 139000\n",
      "house # 1583 --> $ 395192\n",
      "house # 1584 --> $ 189000\n",
      "house # 1585 --> $ 143000\n",
      "house # 1586 --> $ 84900\n",
      "house # 1587 --> $ 122000\n",
      "house # 1588 --> $ 145000\n",
      "house # 1589 --> $ 55993\n",
      "house # 1590 --> $ 155000\n",
      "house # 1591 --> $ 115000\n",
      "house # 1592 --> $ 144500\n",
      "house # 1593 --> $ 149000\n",
      "house # 1594 --> $ 100000\n",
      "house # 1595 --> $ 76500\n",
      "house # 1596 --> $ 135000\n",
      "house # 1597 --> $ 154300\n",
      "house # 1598 --> $ 230000\n",
      "house # 1599 --> $ 147000\n",
      "house # 1600 --> $ 192000\n",
      "house # 1601 --> $ 60000\n",
      "house # 1602 --> $ 125500\n",
      "house # 1603 --> $ 83000\n",
      "house # 1604 --> $ 275000\n",
      "house # 1605 --> $ 236000\n",
      "house # 1606 --> $ 215000\n",
      "house # 1607 --> $ 124000\n",
      "house # 1608 --> $ 306000\n",
      "house # 1609 --> $ 186000\n",
      "house # 1610 --> $ 200500\n",
      "house # 1611 --> $ 107000\n",
      "house # 1612 --> $ 186000\n",
      "house # 1613 --> $ 189000\n",
      "house # 1614 --> $ 180500\n",
      "house # 1615 --> $ 97000\n",
      "house # 1616 --> $ 97000\n",
      "house # 1617 --> $ 83500\n",
      "house # 1618 --> $ 130000\n",
      "house # 1619 --> $ 128200\n",
      "house # 1620 --> $ 202900\n",
      "house # 1621 --> $ 200500\n",
      "house # 1622 --> $ 98000\n",
      "house # 1623 --> $ 220000\n",
      "house # 1624 --> $ 215000\n",
      "house # 1625 --> $ 127500\n",
      "house # 1626 --> $ 157000\n",
      "house # 1627 --> $ 188500\n",
      "house # 1628 --> $ 225000\n",
      "house # 1629 --> $ 168500\n",
      "house # 1630 --> $ 278000\n",
      "house # 1631 --> $ 181000\n",
      "house # 1632 --> $ 181000\n",
      "house # 1633 --> $ 180000\n",
      "house # 1634 --> $ 223500\n",
      "house # 1635 --> $ 244000\n",
      "house # 1636 --> $ 165000\n",
      "house # 1637 --> $ 171000\n",
      "house # 1638 --> $ 192000\n",
      "house # 1639 --> $ 165000\n",
      "house # 1640 --> $ 225000\n",
      "house # 1641 --> $ 157000\n",
      "house # 1642 --> $ 188500\n",
      "house # 1643 --> $ 253000\n",
      "house # 1644 --> $ 194500\n",
      "house # 1645 --> $ 275000\n",
      "house # 1646 --> $ 157000\n",
      "house # 1647 --> $ 165000\n",
      "house # 1648 --> $ 167000\n",
      "house # 1649 --> $ 143500\n",
      "house # 1650 --> $ 123000\n",
      "house # 1651 --> $ 130000\n",
      "house # 1652 --> $ 85400\n",
      "house # 1653 --> $ 85400\n",
      "house # 1654 --> $ 148500\n",
      "house # 1655 --> $ 147000\n",
      "house # 1656 --> $ 148500\n",
      "house # 1657 --> $ 147000\n",
      "house # 1658 --> $ 148500\n",
      "house # 1659 --> $ 140000\n",
      "house # 1660 --> $ 153000\n",
      "house # 1661 --> $ 305000\n",
      "house # 1662 --> $ 335000\n",
      "house # 1663 --> $ 315000\n",
      "house # 1664 --> $ 395192\n",
      "house # 1665 --> $ 255500\n",
      "house # 1666 --> $ 306000\n",
      "house # 1667 --> $ 335000\n",
      "house # 1668 --> $ 337500\n",
      "house # 1669 --> $ 320000\n",
      "house # 1670 --> $ 320000\n",
      "house # 1671 --> $ 203000\n",
      "house # 1672 --> $ 501837\n",
      "house # 1673 --> $ 325000\n",
      "house # 1674 --> $ 255500\n",
      "house # 1675 --> $ 230000\n",
      "house # 1676 --> $ 207500\n",
      "house # 1677 --> $ 207500\n",
      "house # 1678 --> $ 395192\n",
      "house # 1679 --> $ 402861\n",
      "house # 1680 --> $ 319900\n",
      "house # 1681 --> $ 192500\n",
      "house # 1682 --> $ 297000\n",
      "house # 1683 --> $ 160200\n",
      "house # 1684 --> $ 179000\n",
      "house # 1685 --> $ 179000\n",
      "house # 1686 --> $ 160200\n",
      "house # 1687 --> $ 188500\n",
      "house # 1688 --> $ 179000\n",
      "house # 1689 --> $ 179000\n",
      "house # 1690 --> $ 244000\n",
      "house # 1691 --> $ 184100\n",
      "house # 1692 --> $ 244000\n",
      "house # 1693 --> $ 167500\n",
      "house # 1694 --> $ 188500\n",
      "house # 1695 --> $ 174000\n",
      "house # 1696 --> $ 350000\n",
      "house # 1697 --> $ 178000\n",
      "house # 1698 --> $ 305000\n",
      "house # 1699 --> $ 213250\n",
      "house # 1700 --> $ 213000\n",
      "house # 1701 --> $ 261500\n",
      "house # 1702 --> $ 201000\n",
      "house # 1703 --> $ 372500\n",
      "house # 1704 --> $ 306000\n",
      "house # 1705 --> $ 190000\n",
      "house # 1706 --> $ 377500\n",
      "house # 1707 --> $ 233170\n",
      "house # 1708 --> $ 185500\n",
      "house # 1709 --> $ 255500\n",
      "house # 1710 --> $ 264132\n",
      "house # 1711 --> $ 335000\n",
      "house # 1712 --> $ 213000\n",
      "house # 1713 --> $ 280000\n",
      "house # 1714 --> $ 229456\n",
      "house # 1715 --> $ 173500\n",
      "house # 1716 --> $ 155000\n",
      "house # 1717 --> $ 159000\n",
      "house # 1718 --> $ 93500\n",
      "house # 1719 --> $ 197900\n",
      "house # 1720 --> $ 194500\n",
      "house # 1721 --> $ 145000\n",
      "house # 1722 --> $ 93500\n",
      "house # 1723 --> $ 184900\n",
      "house # 1724 --> $ 245350\n",
      "house # 1725 --> $ 214900\n",
      "house # 1726 --> $ 185000\n",
      "house # 1727 --> $ 153900\n",
      "house # 1728 --> $ 227000\n",
      "house # 1729 --> $ 200500\n",
      "house # 1730 --> $ 235000\n",
      "house # 1731 --> $ 123000\n",
      "house # 1732 --> $ 139000\n",
      "house # 1733 --> $ 100000\n",
      "house # 1734 --> $ 100000\n",
      "house # 1735 --> $ 136500\n",
      "house # 1736 --> $ 117500\n",
      "house # 1737 --> $ 196000\n",
      "house # 1738 --> $ 175000\n",
      "house # 1739 --> $ 250000\n",
      "house # 1740 --> $ 208900\n",
      "house # 1741 --> $ 274000\n",
      "house # 1742 --> $ 172500\n",
      "house # 1743 --> $ 172500\n",
      "house # 1744 --> $ 222000\n",
      "house # 1745 --> $ 145000\n",
      "house # 1746 --> $ 174000\n",
      "house # 1747 --> $ 206900\n",
      "house # 1748 --> $ 178000\n",
      "house # 1749 --> $ 167900\n",
      "house # 1750 --> $ 130000\n",
      "house # 1751 --> $ 201800\n",
      "house # 1752 --> $ 117500\n",
      "house # 1753 --> $ 167000\n",
      "house # 1754 --> $ 193500\n",
      "house # 1755 --> $ 167900\n",
      "house # 1756 --> $ 180500\n",
      "house # 1757 --> $ 127500\n",
      "house # 1758 --> $ 142000\n",
      "house # 1759 --> $ 139600\n",
      "house # 1760 --> $ 197500\n",
      "house # 1761 --> $ 166000\n",
      "house # 1762 --> $ 155000\n",
      "house # 1763 --> $ 125000\n",
      "house # 1764 --> $ 128000\n",
      "house # 1765 --> $ 139000\n",
      "house # 1766 --> $ 167500\n",
      "house # 1767 --> $ 257500\n",
      "house # 1768 --> $ 124500\n",
      "house # 1769 --> $ 143000\n",
      "house # 1770 --> $ 136500\n",
      "house # 1771 --> $ 117500\n",
      "house # 1772 --> $ 117500\n",
      "house # 1773 --> $ 62383\n",
      "house # 1774 --> $ 135000\n",
      "house # 1775 --> $ 129500\n",
      "house # 1776 --> $ 112000\n",
      "house # 1777 --> $ 113000\n",
      "house # 1778 --> $ 109000\n",
      "house # 1779 --> $ 66500\n",
      "house # 1780 --> $ 227000\n",
      "house # 1781 --> $ 128900\n",
      "house # 1782 --> $ 109500\n",
      "house # 1783 --> $ 128000\n",
      "house # 1784 --> $ 133000\n",
      "house # 1785 --> $ 100000\n",
      "house # 1786 --> $ 115000\n",
      "house # 1787 --> $ 128000\n",
      "house # 1788 --> $ 98000\n",
      "house # 1789 --> $ 105500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "house # 1790 --> $ 105000\n",
      "house # 1791 --> $ 200000\n",
      "house # 1792 --> $ 167000\n",
      "house # 1793 --> $ 142000\n",
      "house # 1794 --> $ 163500\n",
      "house # 1795 --> $ 139000\n",
      "house # 1796 --> $ 125000\n",
      "house # 1797 --> $ 100000\n",
      "house # 1798 --> $ 139000\n",
      "house # 1799 --> $ 113000\n",
      "house # 1800 --> $ 132500\n",
      "house # 1801 --> $ 133000\n",
      "house # 1802 --> $ 130000\n",
      "house # 1803 --> $ 154000\n",
      "house # 1804 --> $ 137000\n",
      "house # 1805 --> $ 139000\n",
      "house # 1806 --> $ 133000\n",
      "house # 1807 --> $ 124500\n",
      "house # 1808 --> $ 140000\n",
      "house # 1809 --> $ 94000\n",
      "house # 1810 --> $ 174500\n",
      "house # 1811 --> $ 55000\n",
      "house # 1812 --> $ 79000\n",
      "house # 1813 --> $ 110000\n",
      "house # 1814 --> $ 100000\n",
      "house # 1815 --> $ 82000\n",
      "house # 1816 --> $ 102776\n",
      "house # 1817 --> $ 68400\n",
      "house # 1818 --> $ 184000\n",
      "house # 1819 --> $ 117000\n",
      "house # 1820 --> $ 37900\n",
      "house # 1821 --> $ 133000\n",
      "house # 1822 --> $ 110000\n",
      "house # 1823 --> $ 67000\n",
      "house # 1824 --> $ 121000\n",
      "house # 1825 --> $ 169000\n",
      "house # 1826 --> $ 78000\n",
      "house # 1827 --> $ 119000\n",
      "house # 1828 --> $ 132500\n",
      "house # 1829 --> $ 81000\n",
      "house # 1830 --> $ 140000\n",
      "house # 1831 --> $ 128000\n",
      "house # 1832 --> $ 118000\n",
      "house # 1833 --> $ 139000\n",
      "house # 1834 --> $ 119000\n",
      "house # 1835 --> $ 135900\n",
      "house # 1836 --> $ 78000\n",
      "house # 1837 --> $ 109500\n",
      "house # 1838 --> $ 125000\n",
      "house # 1839 --> $ 158500\n",
      "house # 1840 --> $ 179000\n",
      "house # 1841 --> $ 173000\n",
      "house # 1842 --> $ 98000\n",
      "house # 1843 --> $ 157000\n",
      "house # 1844 --> $ 148000\n",
      "house # 1845 --> $ 139500\n",
      "house # 1846 --> $ 180000\n",
      "house # 1847 --> $ 158000\n",
      "house # 1848 --> $ 80000\n",
      "house # 1849 --> $ 98300\n",
      "house # 1850 --> $ 129500\n",
      "house # 1851 --> $ 174500\n",
      "house # 1852 --> $ 140000\n",
      "house # 1853 --> $ 162000\n",
      "house # 1854 --> $ 137000\n",
      "house # 1855 --> $ 154000\n",
      "house # 1856 --> $ 213000\n",
      "house # 1857 --> $ 157500\n",
      "house # 1858 --> $ 142953\n",
      "house # 1859 --> $ 118858\n",
      "house # 1860 --> $ 153337\n",
      "house # 1861 --> $ 118858\n",
      "house # 1862 --> $ 305000\n",
      "house # 1863 --> $ 305000\n",
      "house # 1864 --> $ 305000\n",
      "house # 1865 --> $ 402861\n",
      "house # 1866 --> $ 297000\n",
      "house # 1867 --> $ 232000\n",
      "house # 1868 --> $ 280000\n",
      "house # 1869 --> $ 260000\n",
      "house # 1870 --> $ 194500\n",
      "house # 1871 --> $ 287090\n",
      "house # 1872 --> $ 179000\n",
      "house # 1873 --> $ 188000\n",
      "house # 1874 --> $ 152000\n",
      "house # 1875 --> $ 185000\n",
      "house # 1876 --> $ 215000\n",
      "house # 1877 --> $ 194500\n",
      "house # 1878 --> $ 240000\n",
      "house # 1879 --> $ 100000\n",
      "house # 1880 --> $ 140000\n",
      "house # 1881 --> $ 297000\n",
      "house # 1882 --> $ 236000\n",
      "house # 1883 --> $ 173000\n",
      "house # 1884 --> $ 190000\n",
      "house # 1885 --> $ 260000\n",
      "house # 1886 --> $ 203000\n",
      "house # 1887 --> $ 203000\n",
      "house # 1888 --> $ 274900\n",
      "house # 1889 --> $ 179000\n",
      "house # 1890 --> $ 139000\n",
      "house # 1891 --> $ 148000\n",
      "house # 1892 --> $ 139000\n",
      "house # 1893 --> $ 124500\n",
      "house # 1894 --> $ 118000\n",
      "house # 1895 --> $ 132000\n",
      "house # 1896 --> $ 177500\n",
      "house # 1897 --> $ 160000\n",
      "house # 1898 --> $ 154900\n",
      "house # 1899 --> $ 147000\n",
      "house # 1900 --> $ 161000\n",
      "house # 1901 --> $ 140000\n",
      "house # 1902 --> $ 82000\n",
      "house # 1903 --> $ 176000\n",
      "house # 1904 --> $ 123000\n",
      "house # 1905 --> $ 161500\n",
      "house # 1906 --> $ 161000\n",
      "house # 1907 --> $ 163000\n",
      "house # 1908 --> $ 124000\n",
      "house # 1909 --> $ 124000\n",
      "house # 1910 --> $ 124000\n",
      "house # 1911 --> $ 212000\n",
      "house # 1912 --> $ 335000\n",
      "house # 1913 --> $ 191000\n",
      "house # 1914 --> $ 68500\n",
      "house # 1915 --> $ 392500\n",
      "house # 1916 --> $ 60000\n",
      "house # 1917 --> $ 239500\n",
      "house # 1918 --> $ 131400\n",
      "house # 1919 --> $ 245500\n",
      "house # 1920 --> $ 187100\n",
      "house # 1921 --> $ 337500\n",
      "house # 1922 --> $ 372402\n",
      "house # 1923 --> $ 185000\n",
      "house # 1924 --> $ 212000\n",
      "house # 1925 --> $ 190000\n",
      "house # 1926 --> $ 337500\n",
      "house # 1927 --> $ 139000\n",
      "house # 1928 --> $ 157000\n",
      "house # 1929 --> $ 117500\n",
      "house # 1930 --> $ 133900\n",
      "house # 1931 --> $ 137000\n",
      "house # 1932 --> $ 159500\n",
      "house # 1933 --> $ 155000\n",
      "house # 1934 --> $ 174000\n",
      "house # 1935 --> $ 194500\n",
      "house # 1936 --> $ 187000\n",
      "house # 1937 --> $ 194500\n",
      "house # 1938 --> $ 214000\n",
      "house # 1939 --> $ 286000\n",
      "house # 1940 --> $ 196000\n",
      "house # 1941 --> $ 181000\n",
      "house # 1942 --> $ 224900\n",
      "house # 1943 --> $ 328900\n",
      "house # 1944 --> $ 325000\n",
      "house # 1945 --> $ 297000\n",
      "house # 1946 --> $ 110000\n",
      "house # 1947 --> $ 207000\n",
      "house # 1948 --> $ 126500\n",
      "house # 1949 --> $ 203000\n",
      "house # 1950 --> $ 173000\n",
      "house # 1951 --> $ 301500\n",
      "house # 1952 --> $ 171500\n",
      "house # 1953 --> $ 184000\n",
      "house # 1954 --> $ 192000\n",
      "house # 1955 --> $ 154000\n",
      "house # 1956 --> $ 228000\n",
      "house # 1957 --> $ 174000\n",
      "house # 1958 --> $ 341000\n",
      "house # 1959 --> $ 185000\n",
      "house # 1960 --> $ 109008\n",
      "house # 1961 --> $ 112500\n",
      "house # 1962 --> $ 118000\n",
      "house # 1963 --> $ 113000\n",
      "house # 1964 --> $ 106000\n",
      "house # 1965 --> $ 106000\n",
      "house # 1966 --> $ 143750\n",
      "house # 1967 --> $ 372402\n",
      "house # 1968 --> $ 350000\n",
      "house # 1969 --> $ 315000\n",
      "house # 1970 --> $ 426000\n",
      "house # 1971 --> $ 337000\n",
      "house # 1972 --> $ 426000\n",
      "house # 1973 --> $ 325000\n",
      "house # 1974 --> $ 284000\n",
      "house # 1975 --> $ 611657\n",
      "house # 1976 --> $ 293077\n",
      "house # 1977 --> $ 315000\n",
      "house # 1978 --> $ 325000\n",
      "house # 1979 --> $ 318061\n",
      "house # 1980 --> $ 191000\n",
      "house # 1981 --> $ 277500\n",
      "house # 1982 --> $ 207000\n",
      "house # 1983 --> $ 191000\n",
      "house # 1984 --> $ 179400\n",
      "house # 1985 --> $ 207000\n",
      "house # 1986 --> $ 207500\n",
      "house # 1987 --> $ 159895\n",
      "house # 1988 --> $ 181000\n",
      "house # 1989 --> $ 165400\n",
      "house # 1990 --> $ 236000\n",
      "house # 1991 --> $ 254000\n",
      "house # 1992 --> $ 250000\n",
      "house # 1993 --> $ 168000\n",
      "house # 1994 --> $ 236000\n",
      "house # 1995 --> $ 181000\n",
      "house # 1996 --> $ 250000\n",
      "house # 1997 --> $ 228500\n",
      "house # 1998 --> $ 440000\n",
      "house # 1999 --> $ 305000\n",
      "house # 2000 --> $ 325000\n",
      "house # 2001 --> $ 284000\n",
      "house # 2002 --> $ 293077\n",
      "house # 2003 --> $ 232000\n",
      "house # 2004 --> $ 339750\n",
      "house # 2005 --> $ 200141\n",
      "house # 2006 --> $ 191000\n",
      "house # 2007 --> $ 232000\n",
      "house # 2008 --> $ 210000\n",
      "house # 2009 --> $ 193000\n",
      "house # 2010 --> $ 176000\n",
      "house # 2011 --> $ 141000\n",
      "house # 2012 --> $ 179900\n",
      "house # 2013 --> $ 187000\n",
      "house # 2014 --> $ 201000\n",
      "house # 2015 --> $ 185900\n",
      "house # 2016 --> $ 224900\n",
      "house # 2017 --> $ 201000\n",
      "house # 2018 --> $ 134000\n",
      "house # 2019 --> $ 132000\n",
      "house # 2020 --> $ 80000\n",
      "house # 2021 --> $ 111000\n",
      "house # 2022 --> $ 171500\n",
      "house # 2023 --> $ 117000\n",
      "house # 2024 --> $ 240000\n",
      "house # 2025 --> $ 250000\n",
      "house # 2026 --> $ 196000\n",
      "house # 2027 --> $ 160000\n",
      "house # 2028 --> $ 146000\n",
      "house # 2029 --> $ 160000\n",
      "house # 2030 --> $ 207000\n",
      "house # 2031 --> $ 274000\n",
      "house # 2032 --> $ 286000\n",
      "house # 2033 --> $ 207000\n",
      "house # 2034 --> $ 160000\n",
      "house # 2035 --> $ 223500\n",
      "house # 2036 --> $ 154000\n",
      "house # 2037 --> $ 154000\n",
      "house # 2038 --> $ 151000\n",
      "house # 2039 --> $ 194000\n",
      "house # 2040 --> $ 340000\n",
      "house # 2041 --> $ 260000\n",
      "house # 2042 --> $ 183500\n",
      "house # 2043 --> $ 180000\n",
      "house # 2044 --> $ 127500\n",
      "house # 2045 --> $ 185750\n",
      "house # 2046 --> $ 143000\n",
      "house # 2047 --> $ 154000\n",
      "house # 2048 --> $ 156000\n",
      "house # 2049 --> $ 142600\n",
      "house # 2050 --> $ 154000\n",
      "house # 2051 --> $ 110000\n",
      "house # 2052 --> $ 135000\n",
      "house # 2053 --> $ 119500\n",
      "house # 2054 --> $ 128000\n",
      "house # 2055 --> $ 132000\n",
      "house # 2056 --> $ 147000\n",
      "house # 2057 --> $ 112500\n",
      "house # 2058 --> $ 240000\n",
      "house # 2059 --> $ 139000\n",
      "house # 2060 --> $ 218000\n",
      "house # 2061 --> $ 140000\n",
      "house # 2062 --> $ 129500\n",
      "house # 2063 --> $ 112500\n",
      "house # 2064 --> $ 126500\n",
      "house # 2065 --> $ 98300\n",
      "house # 2066 --> $ 165000\n",
      "house # 2067 --> $ 137450\n",
      "house # 2068 --> $ 143000\n",
      "house # 2069 --> $ 72500\n",
      "house # 2070 --> $ 120000\n",
      "house # 2071 --> $ 107900\n",
      "house # 2072 --> $ 81000\n",
      "house # 2073 --> $ 170000\n",
      "house # 2074 --> $ 189950\n",
      "house # 2075 --> $ 130000\n",
      "house # 2076 --> $ 120500\n",
      "house # 2077 --> $ 124000\n",
      "house # 2078 --> $ 117500\n",
      "house # 2079 --> $ 117000\n",
      "house # 2080 --> $ 112500\n",
      "house # 2081 --> $ 119000\n",
      "house # 2082 --> $ 141000\n",
      "house # 2083 --> $ 154000\n",
      "house # 2084 --> $ 112500\n",
      "house # 2085 --> $ 110000\n",
      "house # 2086 --> $ 155000\n",
      "house # 2087 --> $ 110000\n",
      "house # 2088 --> $ 101000\n",
      "house # 2089 --> $ 114500\n",
      "house # 2090 --> $ 137500\n",
      "house # 2091 --> $ 106250\n",
      "house # 2092 --> $ 162900\n",
      "house # 2093 --> $ 40000\n",
      "house # 2094 --> $ 118000\n",
      "house # 2095 --> $ 118000\n",
      "house # 2096 --> $ 96500\n",
      "house # 2097 --> $ 79900\n",
      "house # 2098 --> $ 98000\n",
      "house # 2099 --> $ 68500\n",
      "house # 2100 --> $ 124000\n",
      "house # 2101 --> $ 163000\n",
      "house # 2102 --> $ 177000\n",
      "house # 2103 --> $ 101000\n",
      "house # 2104 --> $ 122500\n",
      "house # 2105 --> $ 127000\n",
      "house # 2106 --> $ 76000\n",
      "house # 2107 --> $ 168000\n",
      "house # 2108 --> $ 112500\n",
      "house # 2109 --> $ 107900\n",
      "house # 2110 --> $ 108000\n",
      "house # 2111 --> $ 66500\n",
      "house # 2112 --> $ 135000\n",
      "house # 2113 --> $ 130500\n",
      "house # 2114 --> $ 112000\n",
      "house # 2115 --> $ 188700\n",
      "house # 2116 --> $ 122900\n",
      "house # 2117 --> $ 128000\n",
      "house # 2118 --> $ 143000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "house # 2119 --> $ 119000\n",
      "house # 2120 --> $ 117000\n",
      "house # 2121 --> $ 110000\n",
      "house # 2122 --> $ 120000\n",
      "house # 2123 --> $ 130500\n",
      "house # 2124 --> $ 143000\n",
      "house # 2125 --> $ 131500\n",
      "house # 2126 --> $ 157500\n",
      "house # 2127 --> $ 115000\n",
      "house # 2128 --> $ 184000\n",
      "house # 2129 --> $ 105500\n",
      "house # 2130 --> $ 110000\n",
      "house # 2131 --> $ 114500\n",
      "house # 2132 --> $ 177000\n",
      "house # 2133 --> $ 132000\n",
      "house # 2134 --> $ 135000\n",
      "house # 2135 --> $ 108000\n",
      "house # 2136 --> $ 55000\n",
      "house # 2137 --> $ 107900\n",
      "house # 2138 --> $ 129500\n",
      "house # 2139 --> $ 125000\n",
      "house # 2140 --> $ 130000\n",
      "house # 2141 --> $ 181500\n",
      "house # 2142 --> $ 108000\n",
      "house # 2143 --> $ 112500\n",
      "house # 2144 --> $ 145000\n",
      "house # 2145 --> $ 134500\n",
      "house # 2146 --> $ 140000\n",
      "house # 2147 --> $ 207500\n",
      "house # 2148 --> $ 156000\n",
      "house # 2149 --> $ 129500\n",
      "house # 2150 --> $ 208900\n",
      "house # 2151 --> $ 119000\n",
      "house # 2152 --> $ 116500\n",
      "house # 2153 --> $ 163000\n",
      "house # 2154 --> $ 118500\n",
      "house # 2155 --> $ 140000\n",
      "house # 2156 --> $ 211000\n",
      "house # 2157 --> $ 220000\n",
      "house # 2158 --> $ 194500\n",
      "house # 2159 --> $ 253000\n",
      "house # 2160 --> $ 156932\n",
      "house # 2161 --> $ 232000\n",
      "house # 2162 --> $ 385000\n",
      "house # 2163 --> $ 402861\n",
      "house # 2164 --> $ 328900\n",
      "house # 2165 --> $ 235000\n",
      "house # 2166 --> $ 130000\n",
      "house # 2167 --> $ 208900\n",
      "house # 2168 --> $ 193000\n",
      "house # 2169 --> $ 187000\n",
      "house # 2170 --> $ 193000\n",
      "house # 2171 --> $ 140000\n",
      "house # 2172 --> $ 154000\n",
      "house # 2173 --> $ 150500\n",
      "house # 2174 --> $ 255900\n",
      "house # 2175 --> $ 339750\n",
      "house # 2176 --> $ 293077\n",
      "house # 2177 --> $ 255900\n",
      "house # 2178 --> $ 222500\n",
      "house # 2179 --> $ 136500\n",
      "house # 2180 --> $ 206300\n",
      "house # 2181 --> $ 227680\n",
      "house # 2182 --> $ 235128\n",
      "house # 2183 --> $ 250580\n",
      "house # 2184 --> $ 136000\n",
      "house # 2185 --> $ 175000\n",
      "house # 2186 --> $ 165500\n",
      "house # 2187 --> $ 125000\n",
      "house # 2188 --> $ 149000\n",
      "house # 2189 --> $ 240000\n",
      "house # 2190 --> $ 82000\n",
      "house # 2191 --> $ 82000\n",
      "house # 2192 --> $ 81000\n",
      "house # 2193 --> $ 127000\n",
      "house # 2194 --> $ 127000\n",
      "house # 2195 --> $ 110000\n",
      "house # 2196 --> $ 118500\n",
      "house # 2197 --> $ 137500\n",
      "house # 2198 --> $ 120000\n",
      "house # 2199 --> $ 206900\n",
      "house # 2200 --> $ 105000\n",
      "house # 2201 --> $ 132000\n",
      "house # 2202 --> $ 161000\n",
      "house # 2203 --> $ 98000\n",
      "house # 2204 --> $ 104900\n",
      "house # 2205 --> $ 119000\n",
      "house # 2206 --> $ 180000\n",
      "house # 2207 --> $ 129500\n",
      "house # 2208 --> $ 240000\n",
      "house # 2209 --> $ 237500\n",
      "house # 2210 --> $ 173000\n",
      "house # 2211 --> $ 120500\n",
      "house # 2212 --> $ 108000\n",
      "house # 2213 --> $ 79000\n",
      "house # 2214 --> $ 171000\n",
      "house # 2215 --> $ 130500\n",
      "house # 2216 --> $ 143000\n",
      "house # 2217 --> $ 60000\n",
      "house # 2218 --> $ 128000\n",
      "house # 2219 --> $ 130500\n",
      "house # 2220 --> $ 66500\n",
      "house # 2221 --> $ 392500\n",
      "house # 2222 --> $ 275000\n",
      "house # 2223 --> $ 305900\n",
      "house # 2224 --> $ 194500\n",
      "house # 2225 --> $ 185000\n",
      "house # 2226 --> $ 193500\n",
      "house # 2227 --> $ 207000\n",
      "house # 2228 --> $ 210000\n",
      "house # 2229 --> $ 237000\n",
      "house # 2230 --> $ 167500\n",
      "house # 2231 --> $ 185900\n",
      "house # 2232 --> $ 176000\n",
      "house # 2233 --> $ 82500\n",
      "house # 2234 --> $ 208900\n",
      "house # 2235 --> $ 214000\n",
      "house # 2236 --> $ 280000\n",
      "house # 2237 --> $ 350000\n",
      "house # 2238 --> $ 206300\n",
      "house # 2239 --> $ 141000\n",
      "house # 2240 --> $ 162000\n",
      "house # 2241 --> $ 132500\n",
      "house # 2242 --> $ 129900\n",
      "house # 2243 --> $ 127000\n",
      "house # 2244 --> $ 86000\n",
      "house # 2245 --> $ 91000\n",
      "house # 2246 --> $ 129500\n",
      "house # 2247 --> $ 97000\n",
      "house # 2248 --> $ 142000\n",
      "house # 2249 --> $ 130000\n",
      "house # 2250 --> $ 120500\n",
      "house # 2251 --> $ 117500\n",
      "house # 2252 --> $ 182900\n",
      "house # 2253 --> $ 156932\n",
      "house # 2254 --> $ 160000\n",
      "house # 2255 --> $ 193000\n",
      "house # 2256 --> $ 185900\n",
      "house # 2257 --> $ 215000\n",
      "house # 2258 --> $ 176485\n",
      "house # 2259 --> $ 169000\n",
      "house # 2260 --> $ 140000\n",
      "house # 2261 --> $ 179200\n",
      "house # 2262 --> $ 189000\n",
      "house # 2263 --> $ 354000\n",
      "house # 2264 --> $ 415298\n",
      "house # 2265 --> $ 122500\n",
      "house # 2266 --> $ 328900\n",
      "house # 2267 --> $ 466500\n",
      "house # 2268 --> $ 319000\n",
      "house # 2269 --> $ 162500\n",
      "house # 2270 --> $ 187500\n",
      "house # 2271 --> $ 184000\n",
      "house # 2272 --> $ 262280\n",
      "house # 2273 --> $ 214000\n",
      "house # 2274 --> $ 307000\n",
      "house # 2275 --> $ 159500\n",
      "house # 2276 --> $ 171500\n",
      "house # 2277 --> $ 240000\n",
      "house # 2278 --> $ 150750\n",
      "house # 2279 --> $ 117000\n",
      "house # 2280 --> $ 125500\n",
      "house # 2281 --> $ 194700\n",
      "house # 2282 --> $ 189000\n",
      "house # 2283 --> $ 113000\n",
      "house # 2284 --> $ 106000\n",
      "house # 2285 --> $ 153900\n",
      "house # 2286 --> $ 128500\n",
      "house # 2287 --> $ 377426\n",
      "house # 2288 --> $ 250000\n",
      "house # 2289 --> $ 465000\n",
      "house # 2290 --> $ 325624\n",
      "house # 2291 --> $ 281213\n",
      "house # 2292 --> $ 350000\n",
      "house # 2293 --> $ 318000\n",
      "house # 2294 --> $ 285000\n",
      "house # 2295 --> $ 350000\n",
      "house # 2296 --> $ 250000\n",
      "house # 2297 --> $ 284000\n",
      "house # 2298 --> $ 415298\n",
      "house # 2299 --> $ 317000\n",
      "house # 2300 --> $ 339750\n",
      "house # 2301 --> $ 284000\n",
      "house # 2302 --> $ 305900\n",
      "house # 2303 --> $ 200000\n",
      "house # 2304 --> $ 213500\n",
      "house # 2305 --> $ 199900\n",
      "house # 2306 --> $ 197000\n",
      "house # 2307 --> $ 185850\n",
      "house # 2308 --> $ 209500\n",
      "house # 2309 --> $ 277500\n",
      "house # 2310 --> $ 202500\n",
      "house # 2311 --> $ 185850\n",
      "house # 2312 --> $ 227875\n",
      "house # 2313 --> $ 183500\n",
      "house # 2314 --> $ 227875\n",
      "house # 2315 --> $ 156932\n",
      "house # 2316 --> $ 183500\n",
      "house # 2317 --> $ 226000\n",
      "house # 2318 --> $ 173000\n",
      "house # 2319 --> $ 180000\n",
      "house # 2320 --> $ 176485\n",
      "house # 2321 --> $ 246578\n",
      "house # 2322 --> $ 192000\n",
      "house # 2323 --> $ 190000\n",
      "house # 2324 --> $ 185000\n",
      "house # 2325 --> $ 176432\n",
      "house # 2326 --> $ 225000\n",
      "house # 2327 --> $ 219500\n",
      "house # 2328 --> $ 215000\n",
      "house # 2329 --> $ 185000\n",
      "house # 2330 --> $ 183500\n",
      "house # 2331 --> $ 305000\n",
      "house # 2332 --> $ 315000\n",
      "house # 2333 --> $ 220000\n",
      "house # 2334 --> $ 285000\n",
      "house # 2335 --> $ 240000\n",
      "house # 2336 --> $ 290000\n",
      "house # 2337 --> $ 176485\n",
      "house # 2338 --> $ 285000\n",
      "house # 2339 --> $ 176432\n",
      "house # 2340 --> $ 374000\n",
      "house # 2341 --> $ 230500\n",
      "house # 2342 --> $ 214000\n",
      "house # 2343 --> $ 208900\n",
      "house # 2344 --> $ 180000\n",
      "house # 2345 --> $ 213500\n",
      "house # 2346 --> $ 179000\n",
      "house # 2347 --> $ 250580\n",
      "house # 2348 --> $ 180000\n",
      "house # 2349 --> $ 227875\n",
      "house # 2350 --> $ 250000\n",
      "house # 2351 --> $ 232600\n",
      "house # 2352 --> $ 250000\n",
      "house # 2353 --> $ 285000\n",
      "house # 2354 --> $ 84500\n",
      "house # 2355 --> $ 147000\n",
      "house # 2356 --> $ 155000\n",
      "house # 2357 --> $ 176485\n",
      "house # 2358 --> $ 187000\n",
      "house # 2359 --> $ 115000\n",
      "house # 2360 --> $ 112500\n",
      "house # 2361 --> $ 156000\n",
      "house # 2362 --> $ 277000\n",
      "house # 2363 --> $ 128500\n",
      "house # 2364 --> $ 203000\n",
      "house # 2365 --> $ 277500\n",
      "house # 2366 --> $ 167240\n",
      "house # 2367 --> $ 167240\n",
      "house # 2368 --> $ 277500\n",
      "house # 2369 --> $ 196000\n",
      "house # 2370 --> $ 177000\n",
      "house # 2371 --> $ 179400\n",
      "house # 2372 --> $ 179000\n",
      "house # 2373 --> $ 192000\n",
      "house # 2374 --> $ 305000\n",
      "house # 2375 --> $ 270000\n",
      "house # 2376 --> $ 350000\n",
      "house # 2377 --> $ 207500\n",
      "house # 2378 --> $ 153500\n",
      "house # 2379 --> $ 195000\n",
      "house # 2380 --> $ 129500\n",
      "house # 2381 --> $ 153500\n",
      "house # 2382 --> $ 175000\n",
      "house # 2383 --> $ 176000\n",
      "house # 2384 --> $ 269790\n",
      "house # 2385 --> $ 155000\n",
      "house # 2386 --> $ 129500\n",
      "house # 2387 --> $ 132500\n",
      "house # 2388 --> $ 101800\n",
      "house # 2389 --> $ 130000\n",
      "house # 2390 --> $ 128500\n",
      "house # 2391 --> $ 129000\n",
      "house # 2392 --> $ 111000\n",
      "house # 2393 --> $ 148000\n",
      "house # 2394 --> $ 132000\n",
      "house # 2395 --> $ 187000\n",
      "house # 2396 --> $ 139000\n",
      "house # 2397 --> $ 165000\n",
      "house # 2398 --> $ 116000\n",
      "house # 2399 --> $ 39300\n",
      "house # 2400 --> $ 80500\n",
      "house # 2401 --> $ 108000\n",
      "house # 2402 --> $ 135000\n",
      "house # 2403 --> $ 143000\n",
      "house # 2404 --> $ 148000\n",
      "house # 2405 --> $ 167000\n",
      "house # 2406 --> $ 139000\n",
      "house # 2407 --> $ 119000\n",
      "house # 2408 --> $ 132500\n",
      "house # 2409 --> $ 119750\n",
      "house # 2410 --> $ 160000\n",
      "house # 2411 --> $ 118500\n",
      "house # 2412 --> $ 124500\n",
      "house # 2413 --> $ 114500\n",
      "house # 2414 --> $ 145500\n",
      "house # 2415 --> $ 142600\n",
      "house # 2416 --> $ 110000\n",
      "house # 2417 --> $ 123500\n",
      "house # 2418 --> $ 147000\n",
      "house # 2419 --> $ 94750\n",
      "house # 2420 --> $ 105000\n",
      "house # 2421 --> $ 141000\n",
      "house # 2422 --> $ 91000\n",
      "house # 2423 --> $ 115000\n",
      "house # 2424 --> $ 179500\n",
      "house # 2425 --> $ 189000\n",
      "house # 2426 --> $ 109900\n",
      "house # 2427 --> $ 127000\n",
      "house # 2428 --> $ 175000\n",
      "house # 2429 --> $ 111000\n",
      "house # 2430 --> $ 129900\n",
      "house # 2431 --> $ 119200\n",
      "house # 2432 --> $ 134500\n",
      "house # 2433 --> $ 131000\n",
      "house # 2434 --> $ 126000\n",
      "house # 2435 --> $ 119000\n",
      "house # 2436 --> $ 106500\n",
      "house # 2437 --> $ 115000\n",
      "house # 2438 --> $ 123000\n",
      "house # 2439 --> $ 115000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "house # 2440 --> $ 180500\n",
      "house # 2441 --> $ 105000\n",
      "house # 2442 --> $ 139900\n",
      "house # 2443 --> $ 119750\n",
      "house # 2444 --> $ 122900\n",
      "house # 2445 --> $ 79000\n",
      "house # 2446 --> $ 210000\n",
      "house # 2447 --> $ 159500\n",
      "house # 2448 --> $ 131000\n",
      "house # 2449 --> $ 180500\n",
      "house # 2450 --> $ 139000\n",
      "house # 2451 --> $ 188700\n",
      "house # 2452 --> $ 157500\n",
      "house # 2453 --> $ 106500\n",
      "house # 2454 --> $ 90350\n",
      "house # 2455 --> $ 165500\n",
      "house # 2456 --> $ 105000\n",
      "house # 2457 --> $ 101000\n",
      "house # 2458 --> $ 150000\n",
      "house # 2459 --> $ 96500\n",
      "house # 2460 --> $ 140000\n",
      "house # 2461 --> $ 109000\n",
      "house # 2462 --> $ 125000\n",
      "house # 2463 --> $ 109000\n",
      "house # 2464 --> $ 137500\n",
      "house # 2465 --> $ 87000\n",
      "house # 2466 --> $ 116000\n",
      "house # 2467 --> $ 188700\n",
      "house # 2468 --> $ 108500\n",
      "house # 2469 --> $ 67000\n",
      "house # 2470 --> $ 169500\n",
      "house # 2471 --> $ 146500\n",
      "house # 2472 --> $ 129500\n",
      "house # 2473 --> $ 140000\n",
      "house # 2474 --> $ 107000\n",
      "house # 2475 --> $ 146500\n",
      "house # 2476 --> $ 125000\n",
      "house # 2477 --> $ 140000\n",
      "house # 2478 --> $ 140000\n",
      "house # 2479 --> $ 129900\n",
      "house # 2480 --> $ 148000\n",
      "house # 2481 --> $ 138800\n",
      "house # 2482 --> $ 145000\n",
      "house # 2483 --> $ 111000\n",
      "house # 2484 --> $ 112500\n",
      "house # 2485 --> $ 130250\n",
      "house # 2486 --> $ 139000\n",
      "house # 2487 --> $ 240000\n",
      "house # 2488 --> $ 165000\n",
      "house # 2489 --> $ 116000\n",
      "house # 2490 --> $ 241500\n",
      "house # 2491 --> $ 109500\n",
      "house # 2492 --> $ 155000\n",
      "house # 2493 --> $ 122900\n",
      "house # 2494 --> $ 130000\n",
      "house # 2495 --> $ 114500\n",
      "house # 2496 --> $ 257500\n",
      "house # 2497 --> $ 152000\n",
      "house # 2498 --> $ 114500\n",
      "house # 2499 --> $ 139000\n",
      "house # 2500 --> $ 135500\n",
      "house # 2501 --> $ 94750\n",
      "house # 2502 --> $ 139000\n",
      "house # 2503 --> $ 79000\n",
      "house # 2504 --> $ 235000\n",
      "house # 2505 --> $ 269790\n",
      "house # 2506 --> $ 279500\n",
      "house # 2507 --> $ 415298\n",
      "house # 2508 --> $ 279500\n",
      "house # 2509 --> $ 212900\n",
      "house # 2510 --> $ 192000\n",
      "house # 2511 --> $ 176485\n",
      "house # 2512 --> $ 180000\n",
      "house # 2513 --> $ 277000\n",
      "house # 2514 --> $ 235000\n",
      "house # 2515 --> $ 156000\n",
      "house # 2516 --> $ 211000\n",
      "house # 2517 --> $ 124500\n",
      "house # 2518 --> $ 176000\n",
      "house # 2519 --> $ 183500\n",
      "house # 2520 --> $ 173000\n",
      "house # 2521 --> $ 213500\n",
      "house # 2522 --> $ 269790\n",
      "house # 2523 --> $ 120500\n",
      "house # 2524 --> $ 143000\n",
      "house # 2525 --> $ 129900\n",
      "house # 2526 --> $ 122000\n",
      "house # 2527 --> $ 112500\n",
      "house # 2528 --> $ 119500\n",
      "house # 2529 --> $ 150750\n",
      "house # 2530 --> $ 129000\n",
      "house # 2531 --> $ 203000\n",
      "house # 2532 --> $ 269790\n",
      "house # 2533 --> $ 180000\n",
      "house # 2534 --> $ 175500\n",
      "house # 2535 --> $ 220000\n",
      "house # 2536 --> $ 192000\n",
      "house # 2537 --> $ 158000\n",
      "house # 2538 --> $ 183000\n",
      "house # 2539 --> $ 195400\n",
      "house # 2540 --> $ 164990\n",
      "house # 2541 --> $ 183500\n",
      "house # 2542 --> $ 173900\n",
      "house # 2543 --> $ 140000\n",
      "house # 2544 --> $ 125000\n",
      "house # 2545 --> $ 180500\n",
      "house # 2546 --> $ 132000\n",
      "house # 2547 --> $ 129900\n",
      "house # 2548 --> $ 250000\n",
      "house # 2549 --> $ 204750\n",
      "house # 2550 --> $ 184750\n",
      "house # 2551 --> $ 148000\n",
      "house # 2552 --> $ 125000\n",
      "house # 2553 --> $ 82000\n",
      "house # 2554 --> $ 118000\n",
      "house # 2555 --> $ 130000\n",
      "house # 2556 --> $ 137000\n",
      "house # 2557 --> $ 128500\n",
      "house # 2558 --> $ 128000\n",
      "house # 2559 --> $ 96500\n",
      "house # 2560 --> $ 139000\n",
      "house # 2561 --> $ 125000\n",
      "house # 2562 --> $ 105000\n",
      "house # 2563 --> $ 125000\n",
      "house # 2564 --> $ 190000\n",
      "house # 2565 --> $ 144000\n",
      "house # 2566 --> $ 139000\n",
      "house # 2567 --> $ 125000\n",
      "house # 2568 --> $ 164500\n",
      "house # 2569 --> $ 148000\n",
      "house # 2570 --> $ 207500\n",
      "house # 2571 --> $ 191000\n",
      "house # 2572 --> $ 136500\n",
      "house # 2573 --> $ 182900\n",
      "house # 2574 --> $ 180000\n",
      "house # 2575 --> $ 109500\n",
      "house # 2576 --> $ 115000\n",
      "house # 2577 --> $ 115000\n",
      "house # 2578 --> $ 109000\n",
      "house # 2579 --> $ 111000\n",
      "house # 2580 --> $ 160000\n",
      "house # 2581 --> $ 179900\n",
      "house # 2582 --> $ 108000\n",
      "house # 2583 --> $ 235000\n",
      "house # 2584 --> $ 171000\n",
      "house # 2585 --> $ 265900\n",
      "house # 2586 --> $ 157000\n",
      "house # 2587 --> $ 180000\n",
      "house # 2588 --> $ 145000\n",
      "house # 2589 --> $ 129500\n",
      "house # 2590 --> $ 189000\n",
      "house # 2591 --> $ 187000\n",
      "house # 2592 --> $ 225000\n",
      "house # 2593 --> $ 265900\n",
      "house # 2594 --> $ 181500\n",
      "house # 2595 --> $ 179900\n",
      "house # 2596 --> $ 312500\n",
      "house # 2597 --> $ 192000\n",
      "house # 2598 --> $ 268000\n",
      "house # 2599 --> $ 248900\n",
      "house # 2600 --> $ 81000\n",
      "house # 2601 --> $ 270000\n",
      "house # 2602 --> $ 113000\n",
      "house # 2603 --> $ 91000\n",
      "house # 2604 --> $ 91500\n",
      "house # 2605 --> $ 75000\n",
      "house # 2606 --> $ 140000\n",
      "house # 2607 --> $ 134000\n",
      "house # 2608 --> $ 148000\n",
      "house # 2609 --> $ 159000\n",
      "house # 2610 --> $ 85500\n",
      "house # 2611 --> $ 98600\n",
      "house # 2612 --> $ 157000\n",
      "house # 2613 --> $ 124500\n",
      "house # 2614 --> $ 140000\n",
      "house # 2615 --> $ 138800\n",
      "house # 2616 --> $ 141000\n",
      "house # 2617 --> $ 175500\n",
      "house # 2618 --> $ 175500\n",
      "house # 2619 --> $ 174000\n",
      "house # 2620 --> $ 185000\n",
      "house # 2621 --> $ 172400\n",
      "house # 2622 --> $ 192500\n",
      "house # 2623 --> $ 192500\n",
      "house # 2624 --> $ 275500\n",
      "house # 2625 --> $ 230500\n",
      "house # 2626 --> $ 241500\n",
      "house # 2627 --> $ 202500\n",
      "house # 2628 --> $ 354000\n",
      "house # 2629 --> $ 438780\n",
      "house # 2630 --> $ 361919\n",
      "house # 2631 --> $ 437154\n",
      "house # 2632 --> $ 281213\n",
      "house # 2633 --> $ 277500\n",
      "house # 2634 --> $ 465000\n",
      "house # 2635 --> $ 129900\n",
      "house # 2636 --> $ 215000\n",
      "house # 2637 --> $ 151000\n",
      "house # 2638 --> $ 274300\n",
      "house # 2639 --> $ 170000\n",
      "house # 2640 --> $ 160000\n",
      "house # 2641 --> $ 112500\n",
      "house # 2642 --> $ 251000\n",
      "house # 2643 --> $ 83000\n",
      "house # 2644 --> $ 94500\n",
      "house # 2645 --> $ 83000\n",
      "house # 2646 --> $ 83000\n",
      "house # 2647 --> $ 83000\n",
      "house # 2648 --> $ 148000\n",
      "house # 2649 --> $ 133700\n",
      "house # 2650 --> $ 164000\n",
      "house # 2651 --> $ 155000\n",
      "house # 2652 --> $ 348000\n",
      "house # 2653 --> $ 250000\n",
      "house # 2654 --> $ 221500\n",
      "house # 2655 --> $ 374000\n",
      "house # 2656 --> $ 361919\n",
      "house # 2657 --> $ 275000\n",
      "house # 2658 --> $ 233230\n",
      "house # 2659 --> $ 219210\n",
      "house # 2660 --> $ 275000\n",
      "house # 2661 --> $ 312500\n",
      "house # 2662 --> $ 374000\n",
      "house # 2663 --> $ 220000\n",
      "house # 2664 --> $ 185850\n",
      "house # 2665 --> $ 374000\n",
      "house # 2666 --> $ 185850\n",
      "house # 2667 --> $ 192500\n",
      "house # 2668 --> $ 196500\n",
      "house # 2669 --> $ 181134\n",
      "house # 2670 --> $ 285000\n",
      "house # 2671 --> $ 196500\n",
      "house # 2672 --> $ 246578\n",
      "house # 2673 --> $ 185850\n",
      "house # 2674 --> $ 246578\n",
      "house # 2675 --> $ 167240\n",
      "house # 2676 --> $ 180000\n",
      "house # 2677 --> $ 165600\n",
      "house # 2678 --> $ 342643\n",
      "house # 2679 --> $ 348000\n",
      "house # 2680 --> $ 348000\n",
      "house # 2681 --> $ 437154\n",
      "house # 2682 --> $ 290000\n",
      "house # 2683 --> $ 301000\n",
      "house # 2684 --> $ 348000\n",
      "house # 2685 --> $ 301000\n",
      "house # 2686 --> $ 192500\n",
      "house # 2687 --> $ 465000\n",
      "house # 2688 --> $ 181134\n",
      "house # 2689 --> $ 176485\n",
      "house # 2690 --> $ 465000\n",
      "house # 2691 --> $ 186500\n",
      "house # 2692 --> $ 84500\n",
      "house # 2693 --> $ 179600\n",
      "house # 2694 --> $ 84500\n",
      "house # 2695 --> $ 213500\n",
      "house # 2696 --> $ 186500\n",
      "house # 2697 --> $ 196500\n",
      "house # 2698 --> $ 200000\n",
      "house # 2699 --> $ 164000\n",
      "house # 2700 --> $ 167240\n",
      "house # 2701 --> $ 175900\n",
      "house # 2702 --> $ 129900\n",
      "house # 2703 --> $ 150900\n",
      "house # 2704 --> $ 135000\n",
      "house # 2705 --> $ 125500\n",
      "house # 2706 --> $ 115000\n",
      "house # 2707 --> $ 120000\n",
      "house # 2708 --> $ 145250\n",
      "house # 2709 --> $ 88000\n",
      "house # 2710 --> $ 137000\n",
      "house # 2711 --> $ 171500\n",
      "house # 2712 --> $ 290000\n",
      "house # 2713 --> $ 197000\n",
      "house # 2714 --> $ 147400\n",
      "house # 2715 --> $ 168500\n",
      "house # 2716 --> $ 147400\n",
      "house # 2717 --> $ 215000\n",
      "house # 2718 --> $ 232600\n",
      "house # 2719 --> $ 129000\n",
      "house # 2720 --> $ 167000\n",
      "house # 2721 --> $ 129900\n",
      "house # 2722 --> $ 167900\n",
      "house # 2723 --> $ 176000\n",
      "house # 2724 --> $ 134500\n",
      "house # 2725 --> $ 118500\n",
      "house # 2726 --> $ 132500\n",
      "house # 2727 --> $ 127000\n",
      "house # 2728 --> $ 165000\n",
      "house # 2729 --> $ 155000\n",
      "house # 2730 --> $ 129900\n",
      "house # 2731 --> $ 129900\n",
      "house # 2732 --> $ 119750\n",
      "house # 2733 --> $ 170000\n",
      "house # 2734 --> $ 113000\n",
      "house # 2735 --> $ 118500\n",
      "house # 2736 --> $ 155000\n",
      "house # 2737 --> $ 128500\n",
      "house # 2738 --> $ 136905\n",
      "house # 2739 --> $ 135960\n",
      "house # 2740 --> $ 120000\n",
      "house # 2741 --> $ 128500\n",
      "house # 2742 --> $ 175500\n",
      "house # 2743 --> $ 143000\n",
      "house # 2744 --> $ 135000\n",
      "house # 2745 --> $ 134500\n",
      "house # 2746 --> $ 132500\n",
      "house # 2747 --> $ 128500\n",
      "house # 2748 --> $ 109500\n",
      "house # 2749 --> $ 135000\n",
      "house # 2750 --> $ 124000\n",
      "house # 2751 --> $ 139000\n",
      "house # 2752 --> $ 149900\n",
      "house # 2753 --> $ 150750\n",
      "house # 2754 --> $ 151400\n",
      "house # 2755 --> $ 126500\n",
      "house # 2756 --> $ 117000\n",
      "house # 2757 --> $ 79900\n",
      "house # 2758 --> $ 101000\n",
      "house # 2759 --> $ 112000\n",
      "house # 2760 --> $ 210000\n",
      "house # 2761 --> $ 145000\n",
      "house # 2762 --> $ 145250\n",
      "house # 2763 --> $ 257500\n",
      "house # 2764 --> $ 167000\n",
      "house # 2765 --> $ 170000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "house # 2766 --> $ 108000\n",
      "house # 2767 --> $ 90000\n",
      "house # 2768 --> $ 141000\n",
      "house # 2769 --> $ 118000\n",
      "house # 2770 --> $ 129900\n",
      "house # 2771 --> $ 111000\n",
      "house # 2772 --> $ 92000\n",
      "house # 2773 --> $ 132000\n",
      "house # 2774 --> $ 104900\n",
      "house # 2775 --> $ 149000\n",
      "house # 2776 --> $ 135000\n",
      "house # 2777 --> $ 127000\n",
      "house # 2778 --> $ 139400\n",
      "house # 2779 --> $ 137000\n",
      "house # 2780 --> $ 96500\n",
      "house # 2781 --> $ 96500\n",
      "house # 2782 --> $ 114500\n",
      "house # 2783 --> $ 79000\n",
      "house # 2784 --> $ 140000\n",
      "house # 2785 --> $ 157000\n",
      "house # 2786 --> $ 89000\n",
      "house # 2787 --> $ 126000\n",
      "house # 2788 --> $ 106500\n",
      "house # 2789 --> $ 114504\n",
      "house # 2790 --> $ 85500\n",
      "house # 2791 --> $ 132000\n",
      "house # 2792 --> $ 67000\n",
      "house # 2793 --> $ 125000\n",
      "house # 2794 --> $ 107000\n",
      "house # 2795 --> $ 101000\n",
      "house # 2796 --> $ 96500\n",
      "house # 2797 --> $ 225000\n",
      "house # 2798 --> $ 136900\n",
      "house # 2799 --> $ 125000\n",
      "house # 2800 --> $ 85500\n",
      "house # 2801 --> $ 125000\n",
      "house # 2802 --> $ 105000\n",
      "house # 2803 --> $ 143000\n",
      "house # 2804 --> $ 144000\n",
      "house # 2805 --> $ 89000\n",
      "house # 2806 --> $ 86000\n",
      "house # 2807 --> $ 194000\n",
      "house # 2808 --> $ 181900\n",
      "house # 2809 --> $ 145000\n",
      "house # 2810 --> $ 129900\n",
      "house # 2811 --> $ 165000\n",
      "house # 2812 --> $ 156000\n",
      "house # 2813 --> $ 136905\n",
      "house # 2814 --> $ 143000\n",
      "house # 2815 --> $ 118400\n",
      "house # 2816 --> $ 164500\n",
      "house # 2817 --> $ 164500\n",
      "house # 2818 --> $ 135000\n",
      "house # 2819 --> $ 165000\n",
      "house # 2820 --> $ 128500\n",
      "house # 2821 --> $ 105000\n",
      "house # 2822 --> $ 159000\n",
      "house # 2823 --> $ 302000\n",
      "house # 2824 --> $ 207500\n",
      "house # 2825 --> $ 257500\n",
      "house # 2826 --> $ 154500\n",
      "house # 2827 --> $ 144000\n",
      "house # 2828 --> $ 211000\n",
      "house # 2829 --> $ 207500\n",
      "house # 2830 --> $ 239799\n",
      "house # 2831 --> $ 185000\n",
      "house # 2832 --> $ 217000\n",
      "house # 2833 --> $ 465000\n",
      "house # 2834 --> $ 213500\n",
      "house # 2835 --> $ 176000\n",
      "house # 2836 --> $ 197000\n",
      "house # 2837 --> $ 160000\n",
      "house # 2838 --> $ 142500\n",
      "house # 2839 --> $ 187500\n",
      "house # 2840 --> $ 194000\n",
      "house # 2841 --> $ 175900\n",
      "house # 2842 --> $ 179665\n",
      "house # 2843 --> $ 138500\n",
      "house # 2844 --> $ 170000\n",
      "house # 2845 --> $ 128500\n",
      "house # 2846 --> $ 187500\n",
      "house # 2847 --> $ 179665\n",
      "house # 2848 --> $ 158000\n",
      "house # 2849 --> $ 196500\n",
      "house # 2850 --> $ 348000\n",
      "house # 2851 --> $ 199900\n",
      "house # 2852 --> $ 201000\n",
      "house # 2853 --> $ 274300\n",
      "house # 2854 --> $ 155900\n",
      "house # 2855 --> $ 260000\n",
      "house # 2856 --> $ 196500\n",
      "house # 2857 --> $ 186500\n",
      "house # 2858 --> $ 235000\n",
      "house # 2859 --> $ 139000\n",
      "house # 2860 --> $ 127500\n",
      "house # 2861 --> $ 107000\n",
      "house # 2862 --> $ 176500\n",
      "house # 2863 --> $ 123600\n",
      "house # 2864 --> $ 192500\n",
      "house # 2865 --> $ 145000\n",
      "house # 2866 --> $ 130000\n",
      "house # 2867 --> $ 106500\n",
      "house # 2868 --> $ 79000\n",
      "house # 2869 --> $ 138887\n",
      "house # 2870 --> $ 160000\n",
      "house # 2871 --> $ 102000\n",
      "house # 2872 --> $ 86000\n",
      "house # 2873 --> $ 100000\n",
      "house # 2874 --> $ 112000\n",
      "house # 2875 --> $ 140000\n",
      "house # 2876 --> $ 151000\n",
      "house # 2877 --> $ 112000\n",
      "house # 2878 --> $ 200100\n",
      "house # 2879 --> $ 119500\n",
      "house # 2880 --> $ 140200\n",
      "house # 2881 --> $ 144000\n",
      "house # 2882 --> $ 126000\n",
      "house # 2883 --> $ 143000\n",
      "house # 2884 --> $ 159000\n",
      "house # 2885 --> $ 141000\n",
      "house # 2886 --> $ 164000\n",
      "house # 2887 --> $ 120000\n",
      "house # 2888 --> $ 131000\n",
      "house # 2889 --> $ 80500\n",
      "house # 2890 --> $ 106500\n",
      "house # 2891 --> $ 119500\n",
      "house # 2892 --> $ 80500\n",
      "house # 2893 --> $ 118000\n",
      "house # 2894 --> $ 80500\n",
      "house # 2895 --> $ 153500\n",
      "house # 2896 --> $ 235000\n",
      "house # 2897 --> $ 241500\n",
      "house # 2898 --> $ 136905\n",
      "house # 2899 --> $ 225000\n",
      "house # 2900 --> $ 132500\n",
      "house # 2901 --> $ 167000\n",
      "house # 2902 --> $ 235000\n",
      "house # 2903 --> $ 315000\n",
      "house # 2904 --> $ 270000\n",
      "house # 2905 --> $ 108000\n",
      "house # 2906 --> $ 206300\n",
      "house # 2907 --> $ 94500\n",
      "house # 2908 --> $ 139000\n",
      "house # 2909 --> $ 136905\n",
      "house # 2910 --> $ 92000\n",
      "house # 2911 --> $ 83000\n",
      "house # 2912 --> $ 170000\n",
      "house # 2913 --> $ 106000\n",
      "house # 2914 --> $ 91500\n",
      "house # 2915 --> $ 91500\n",
      "house # 2916 --> $ 75000\n",
      "house # 2917 --> $ 151000\n",
      "house # 2918 --> $ 93500\n",
      "house # 2919 --> $ 215000\n",
      "        Id  SalePrice\n",
      "0     1461   129000.0\n",
      "1     1462   158000.0\n",
      "2     1463   180000.0\n",
      "3     1464   178000.0\n",
      "4     1465   192000.0\n",
      "5     1466   189000.0\n",
      "6     1467   206000.0\n",
      "7     1468   189000.0\n",
      "8     1469   197500.0\n",
      "9     1470    97000.0\n",
      "10    1471   224000.0\n",
      "11    1472    88000.0\n",
      "12    1473    88000.0\n",
      "13    1474   146000.0\n",
      "14    1475    99500.0\n",
      "15    1476   252000.0\n",
      "16    1477   198900.0\n",
      "17    1478   256300.0\n",
      "18    1479   378500.0\n",
      "19    1480   611657.0\n",
      "20    1481   236000.0\n",
      "21    1482   192500.0\n",
      "22    1483   186000.0\n",
      "23    1484   192000.0\n",
      "24    1485   188500.0\n",
      "25    1486   189000.0\n",
      "26    1487   582933.0\n",
      "27    1488   287090.0\n",
      "28    1489   185500.0\n",
      "29    1490   220000.0\n",
      "...    ...        ...\n",
      "1429  2890   106500.0\n",
      "1430  2891   119500.0\n",
      "1431  2892    80500.0\n",
      "1432  2893   118000.0\n",
      "1433  2894    80500.0\n",
      "1434  2895   153500.0\n",
      "1435  2896   235000.0\n",
      "1436  2897   241500.0\n",
      "1437  2898   136905.0\n",
      "1438  2899   225000.0\n",
      "1439  2900   132500.0\n",
      "1440  2901   167000.0\n",
      "1441  2902   235000.0\n",
      "1442  2903   315000.0\n",
      "1443  2904   270000.0\n",
      "1444  2905   108000.0\n",
      "1445  2906   206300.0\n",
      "1446  2907    94500.0\n",
      "1447  2908   139000.0\n",
      "1448  2909   136905.0\n",
      "1449  2910    92000.0\n",
      "1450  2911    83000.0\n",
      "1451  2912   170000.0\n",
      "1452  2913   106000.0\n",
      "1453  2914    91500.0\n",
      "1454  2915    91500.0\n",
      "1455  2916    75000.0\n",
      "1456  2917   151000.0\n",
      "1457  2918    93500.0\n",
      "1458  2919   215000.0\n",
      "\n",
      "[1459 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 1.\n",
    "# Show nr of test and train houses\n",
    "nr_test_houses = all_input_feats_test.shape[0]\n",
    "print(\"There are\", nr_test_houses,\n",
    "      \"for which I will predict sale prices.\")\n",
    "nr_train_houses = all_input_feats_train.shape[0]\n",
    "print(\"There are\", nr_train_houses,\n",
    "      \"train data houses.\")\n",
    "\n",
    "# 2.\n",
    "# Define a function to measure the\n",
    "# distance between two feature vectors\n",
    "def get_feature_vec_distance(v,w):\n",
    "    \n",
    "    return np.linalg.norm(v-w)\n",
    "    \n",
    "\n",
    "# 3.\n",
    "# Loop over all 1459 test houses\n",
    "preds_test_houses_dollar = np.zeros((nr_test_houses,1))\n",
    "for test_house_nr in range(0,nr_test_houses):\n",
    "    \n",
    "    # 3.1 get the feature vector of the test house\n",
    "    \n",
    "    # Use numerical + categorial feature vector\n",
    "    #v = all_input_feats_test[test_house_nr,:]\n",
    "    \n",
    "    # Just use numerical feature vector\n",
    "    v = normalized_test_input_matrix_feats[test_house_nr,:]\n",
    "    \n",
    "    \n",
    "    # 3.2 compare v with all 1460 train houses\n",
    "    min_dist = -1.0\n",
    "    predicted_sale_price = -1.0\n",
    "    for train_house_nr in range(0,nr_train_houses):\n",
    "        \n",
    "        # get the feature vector of the train house\n",
    "        \n",
    "        # Use numerical + categorial feature vector\n",
    "        #w = all_input_feats_train[train_house_nr,:]\n",
    "        \n",
    "        # Just use numerical feature vector\n",
    "        w = normalized_train_input_matrix_feats[train_house_nr,:]\n",
    "        \n",
    "        # compare vector v and w\n",
    "        distance = get_feature_vec_distance(v,w)\n",
    "        \n",
    "        # found a vector w that is more similar to v?\n",
    "        if (train_house_nr==0 or distance<min_dist):\n",
    "            min_dist = distance\n",
    "            predicted_sale_price =\\\n",
    "                train_output_matrix[train_house_nr][0]\n",
    "                \n",
    "    # 3.3 show predicted sale price for current test house\n",
    "    print(\"house #\", test_house_ids[test_house_nr],\n",
    "          \"--> $\", predicted_sale_price)\n",
    "    \n",
    "    # 3.4 store the predicted house price\n",
    "    preds_test_houses_dollar[test_house_nr][0] = predicted_sale_price\n",
    "    \n",
    "    \n",
    "# 4.\n",
    "\n",
    "# For a Pandas data frame column the predicted\n",
    "# house sale price matrix has to be 1-dimensional\n",
    "preds_test_houses_dollar = preds_test_houses_dollar.reshape(-1)\n",
    "\n",
    "# Create a .csv file    \n",
    "predition_dataframe = pd.DataFrame({'Id'       :test_house_ids,\n",
    "                                    'SalePrice':preds_test_houses_dollar}\n",
    "                                  )\n",
    "# convert column \"Id\" to int64 dtype\n",
    "predition_dataframe = predition_dataframe.astype({\"Id\": int})\n",
    "print(predition_dataframe)\n",
    "# now save the Pandas dataframe to a .csv file\n",
    "PREDICTION_FILENAME = \"nn_predictions.csv\"\n",
    "predition_dataframe.to_csv(PREDICTION_FILENAME, sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good to see, that here the Nearest Neighbour regression approaches that uses (numerical + categorial features) or just (numerical feature) vectors gets a worse prediction.\n",
    "\n",
    "E.g. using numerical features the Nearest Neighbour regression aproach gave a score of 0.22729, while I got a score of 0.14177 with the MLP approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
