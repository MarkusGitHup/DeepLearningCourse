{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\">Multi-Layer Perceptron (MLP) example in Keras <br> for house sales price prediction<br>using BOTH numerical and categorial features\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    " by Prof. Dr.-Ing. Jürgen Brauer, http://www.juergenbrauer.org\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Read-in-the-data-and-deal-with-&quot;NaN&quot;-values\" data-toc-modified-id=\"Read-in-the-data-and-deal-with-&quot;NaN&quot;-values-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Read in the data and deal with \"NaN\" values</a></span></li><li><span><a href=\"#Prepare-numerical-features\" data-toc-modified-id=\"Prepare-numerical-features-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Prepare numerical features</a></span></li><li><span><a href=\"#Prepare-categorial-features\" data-toc-modified-id=\"Prepare-categorial-features-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Prepare categorial features</a></span></li><li><span><a href=\"#Combine-numerical-and-categorial-feature-columns\" data-toc-modified-id=\"Combine-numerical-and-categorial-feature-columns-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Combine numerical and categorial feature columns</a></span></li><li><span><a href=\"#Build-and-train-MLP-model\" data-toc-modified-id=\"Build-and-train-MLP-model-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Build and train MLP model</a></span></li><li><span><a href=\"#Testing-the-trained-MLP\" data-toc-modified-id=\"Testing-the-trained-MLP-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Testing the trained MLP</a></span></li><li><span><a href=\"#Predicting-house-prices-for-the-Kaggle-competition\" data-toc-modified-id=\"Predicting-house-prices-for-the-Kaggle-competition-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Predicting house prices for the Kaggle competition</a></span></li><li><span><a href=\"#Reference-algorithm:-Nearest-Neighbour-Regression\" data-toc-modified-id=\"Reference-algorithm:-Nearest-Neighbour-Regression-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Reference algorithm: Nearest Neighbour Regression</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the data and deal with \"NaN\" values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>0</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>0</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>0</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>0</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>0</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave     0      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave     0      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave     0      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave     0      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave     0      IR1   \n",
       "\n",
       "  LandContour Utilities    ...     PoolArea PoolQC Fence MiscFeature MiscVal  \\\n",
       "0         Lvl    AllPub    ...            0      0     0           0       0   \n",
       "1         Lvl    AllPub    ...            0      0     0           0       0   \n",
       "2         Lvl    AllPub    ...            0      0     0           0       0   \n",
       "3         Lvl    AllPub    ...            0      0     0           0       0   \n",
       "4         Lvl    AllPub    ...            0      0     0           0       0   \n",
       "\n",
       "  MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0      2   2008        WD         Normal     208500  \n",
       "1      5   2007        WD         Normal     181500  \n",
       "2      9   2008        WD         Normal     223500  \n",
       "3      2   2006        WD        Abnorml     140000  \n",
       "4     12   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv(\"kaggle_dataset_house_prices/train.csv\")\n",
    "test_data  = pd.read_csv(\"kaggle_dataset_house_prices/test.csv\")\n",
    "\n",
    "train_data.fillna(0, inplace=True)\n",
    "test_data.fillna(0, inplace=True)\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "np.set_printoptions(suppress=True) # do not use scientific \"e\"-notation\n",
    "\n",
    "# 1.\n",
    "# prepare Pandas DataFrames only with numerical columns\n",
    "train_data_num_only = train_data.select_dtypes(exclude=['object'])\n",
    "test_data_num_only  = test_data.select_dtypes(exclude=['object'])\n",
    "\n",
    "# 2.\n",
    "# Throw away \"Id\" and SalePrice\" column for training data\n",
    "train_input_matrix = train_data_num_only.values[:,1:37]\n",
    "train_output_matrix = train_data_num_only.values[:,37]\n",
    "train_output_matrix = train_output_matrix.reshape(-1,1)\n",
    "\n",
    "# 3.\n",
    "# Throw away \"Id\" column for test input matrix\n",
    "test_input_matrix  = test_data_num_only.values[:,1:]\n",
    "\n",
    "# 4.\n",
    "# create a MinMaxScaler with feature range [0,1]\n",
    "# and use it to normalize the train_input_matrix\n",
    "# Then use the SAME normalization to normalize test_data_matrix\n",
    "scaler_input_features = MinMaxScaler(feature_range=(0, 1))\n",
    "normalized_train_input_matrix_feats = scaler_input_features.fit_transform(train_input_matrix)\n",
    "normalized_test_input_matrix_feats = scaler_input_features.transform(test_input_matrix)\n",
    "\n",
    "# 5.\n",
    "# Also create a MinMaxScaler for the train_output_matrix,\n",
    "# which is essentially a column with the final SalePrice\n",
    "scaler_saleprice = MinMaxScaler(feature_range=(0, 1))\n",
    "normalized_train_output_matrix = scaler_saleprice.fit_transform(train_output_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare categorial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSZoning         object\n",
      "Street           object\n",
      "Alley            object\n",
      "LotShape         object\n",
      "LandContour      object\n",
      "Utilities        object\n",
      "LotConfig        object\n",
      "LandSlope        object\n",
      "Neighborhood     object\n",
      "Condition1       object\n",
      "Condition2       object\n",
      "BldgType         object\n",
      "HouseStyle       object\n",
      "RoofStyle        object\n",
      "RoofMatl         object\n",
      "Exterior1st      object\n",
      "Exterior2nd      object\n",
      "MasVnrType       object\n",
      "ExterQual        object\n",
      "ExterCond        object\n",
      "Foundation       object\n",
      "BsmtQual         object\n",
      "BsmtCond         object\n",
      "BsmtExposure     object\n",
      "BsmtFinType1     object\n",
      "BsmtFinType2     object\n",
      "Heating          object\n",
      "HeatingQC        object\n",
      "CentralAir       object\n",
      "Electrical       object\n",
      "KitchenQual      object\n",
      "Functional       object\n",
      "FireplaceQu      object\n",
      "GarageType       object\n",
      "GarageFinish     object\n",
      "GarageQual       object\n",
      "GarageCond       object\n",
      "PavedDrive       object\n",
      "PoolQC           object\n",
      "Fence            object\n",
      "MiscFeature      object\n",
      "SaleType         object\n",
      "SaleCondition    object\n",
      "dtype: object\n",
      "Shape of fused_df is (2919, 43)\n",
      "Shape of fused_df_hot_encoded is (2919, 275)\n"
     ]
    }
   ],
   "source": [
    "# 1.\n",
    "# save the house Ids, since we need them later\n",
    "# for our Kaggle submission\n",
    "test_house_ids = test_data.values[:,0] # get the IDs from the original Pandas DataFrame\n",
    "\n",
    "\n",
    "# 2.\n",
    "# get Pandas data frames without the numerical features\n",
    "gt_saleprice = train_data[\"SalePrice\"]\n",
    "train_data_cats_only = train_data.select_dtypes(exclude=['number'])\n",
    "test_data_cats_only = test_data.select_dtypes(exclude=['number'])\n",
    "\n",
    "\n",
    "# 3.\n",
    "# map each single categorial column\n",
    "# to multiple (one-hot encoded) columns\n",
    "print(train_data_cats_only.dtypes)\n",
    "frames = [train_data_cats_only, test_data_cats_only]\n",
    "fused_df = pd.concat(frames)\n",
    "print(\"Shape of fused_df is\", fused_df.shape)\n",
    "\n",
    "# now do the one-hot encoding\n",
    "fused_df_hot_encoded = pd.get_dummies(fused_df)\n",
    "print(\"Shape of fused_df_hot_encoded is\", fused_df_hot_encoded.shape)\n",
    "\n",
    "# now split the data frame into two data frames again\n",
    "# with 1460 and 1459 rows\n",
    "train_data_hot_encoded = fused_df_hot_encoded[0:1460]\n",
    "test_data_hot_encoded  = fused_df_hot_encoded[1460:]\n",
    "\n",
    "\n",
    "# 4.\n",
    "# prepare NumPy matrices for training\n",
    "# from Pandas DataFrames\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# define how NumPy shall print matrices\n",
    "np.set_printoptions(precision=4)\n",
    "np.set_printoptions(suppress=True) # do not use scientific \"e\"-notation\n",
    "\n",
    "# convert Pandas DataFrame to NumPy matrices\n",
    "# since Keras will expect NumPy matrices\n",
    "train_input_matrix_cat = train_data_hot_encoded.values\n",
    "train_output_matrix    = gt_saleprice.values\n",
    "train_output_matrix    = train_output_matrix.reshape(-1,1)\n",
    "test_input_matrix_cat  = test_data_hot_encoded.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine numerical and categorial feature columns\n",
    "\n",
    "We have now a training matrix with numerical features and a training matrix with categorial one-hot encoded features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 36 many numerical features that will be used.\n",
      "\tnormalized_train_input_matrix_feats has shape (1460, 36)\n",
      "\tnormalized_test_input_matrix_feats has shape (1459, 36)\n",
      "There are 275 many categorial features that will be used.\n",
      "\ttrain_input_matrix_cat has shape (1460, 275)\n",
      "\ttrain_input_matrix_cat has shape (1460, 275)\n",
      "all_input_feats_train has shape (1460, 311)\n",
      "all_input_feats_test has shape (1459, 311)\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\", normalized_train_input_matrix_feats.shape[1],\n",
    "      \"many numerical features that will be used.\")\n",
    "print(\"\\tnormalized_train_input_matrix_feats has shape\",\n",
    "      normalized_train_input_matrix_feats.shape)\n",
    "print(\"\\tnormalized_test_input_matrix_feats has shape\",\n",
    "      normalized_test_input_matrix_feats.shape)\n",
    "\n",
    "print(\"There are\", train_input_matrix_cat.shape[1],\n",
    "      \"many categorial features that will be used.\")\n",
    "print(\"\\ttrain_input_matrix_cat has shape\",\n",
    "      train_input_matrix_cat.shape)\n",
    "print(\"\\ttrain_input_matrix_cat has shape\",\n",
    "      train_input_matrix_cat.shape)\n",
    "\n",
    "# concatenate the two input matrices horizontally\n",
    "all_input_feats_train = np.hstack((normalized_train_input_matrix_feats,\n",
    "                                   train_input_matrix_cat))\n",
    "print(\"all_input_feats_train has shape\", all_input_feats_train.shape)\n",
    "all_input_feats_test = np.hstack((normalized_test_input_matrix_feats ,\n",
    "                                   test_input_matrix_cat))\n",
    "print(\"all_input_feats_test has shape\", all_input_feats_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and train MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input X has shape (1460, 311)\n",
      "Desired output Y has shape (1460, 1)\n",
      "Y:\n",
      " [[ 0.2411]\n",
      " [ 0.2036]\n",
      " [ 0.2619]\n",
      " ..., \n",
      " [ 0.3216]\n",
      " [ 0.1489]\n",
      " [ 0.1564]]\n",
      "Train on 1095 samples, validate on 365 samples\n",
      "Epoch 1/500\n",
      "1095/1095 [==============================] - 0s 198us/step - loss: 0.0331 - val_loss: 0.0220\n",
      "Epoch 2/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0218 - val_loss: 0.0178\n",
      "Epoch 3/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 0.0169 - val_loss: 0.0148\n",
      "Epoch 4/500\n",
      "1095/1095 [==============================] - 0s 56us/step - loss: 0.0141 - val_loss: 0.0176\n",
      "Epoch 5/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0126 - val_loss: 0.0125\n",
      "Epoch 6/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0112 - val_loss: 0.0124\n",
      "Epoch 7/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0104 - val_loss: 0.0121\n",
      "Epoch 8/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0096 - val_loss: 0.0109\n",
      "Epoch 9/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0091 - val_loss: 0.0107\n",
      "Epoch 10/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0085 - val_loss: 0.0103\n",
      "Epoch 11/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0081 - val_loss: 0.0104\n",
      "Epoch 12/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 0.0079 - val_loss: 0.0100\n",
      "Epoch 13/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0074 - val_loss: 0.0094\n",
      "Epoch 14/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0071 - val_loss: 0.0092\n",
      "Epoch 15/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0068 - val_loss: 0.0093\n",
      "Epoch 16/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0066 - val_loss: 0.0088\n",
      "Epoch 17/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0063 - val_loss: 0.0091\n",
      "Epoch 18/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0061 - val_loss: 0.0099\n",
      "Epoch 19/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0060 - val_loss: 0.0084\n",
      "Epoch 20/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0057 - val_loss: 0.0082\n",
      "Epoch 21/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0055 - val_loss: 0.0083\n",
      "Epoch 22/500\n",
      "1095/1095 [==============================] - 0s 58us/step - loss: 0.0054 - val_loss: 0.0089\n",
      "Epoch 23/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0053 - val_loss: 0.0081\n",
      "Epoch 24/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0052 - val_loss: 0.0079\n",
      "Epoch 25/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0050 - val_loss: 0.0077\n",
      "Epoch 26/500\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 0.0049 - val_loss: 0.0078\n",
      "Epoch 27/500\n",
      "1095/1095 [==============================] - 0s 60us/step - loss: 0.0048 - val_loss: 0.0076\n",
      "Epoch 28/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 0.0047 - val_loss: 0.0084\n",
      "Epoch 29/500\n",
      "1095/1095 [==============================] - 0s 56us/step - loss: 0.0046 - val_loss: 0.0075\n",
      "Epoch 30/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 0.0045 - val_loss: 0.0074\n",
      "Epoch 31/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 0.0044 - val_loss: 0.0073\n",
      "Epoch 32/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 0.0043 - val_loss: 0.0075\n",
      "Epoch 33/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0043 - val_loss: 0.0072\n",
      "Epoch 34/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 0.0041 - val_loss: 0.0072\n",
      "Epoch 35/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0041 - val_loss: 0.0074\n",
      "Epoch 36/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0041 - val_loss: 0.0071\n",
      "Epoch 37/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0040 - val_loss: 0.0070\n",
      "Epoch 38/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 0.0039 - val_loss: 0.0070\n",
      "Epoch 39/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 0.0038 - val_loss: 0.0070\n",
      "Epoch 40/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 0.0038 - val_loss: 0.0069\n",
      "Epoch 41/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0037 - val_loss: 0.0069\n",
      "Epoch 42/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0037 - val_loss: 0.0068\n",
      "Epoch 43/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 0.0036 - val_loss: 0.0067\n",
      "Epoch 44/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0035 - val_loss: 0.0067\n",
      "Epoch 45/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0035 - val_loss: 0.0068\n",
      "Epoch 46/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0034 - val_loss: 0.0068\n",
      "Epoch 47/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0034 - val_loss: 0.0067\n",
      "Epoch 48/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 0.0034 - val_loss: 0.0072\n",
      "Epoch 49/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 0.0033 - val_loss: 0.0066\n",
      "Epoch 50/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0032 - val_loss: 0.0067\n",
      "Epoch 51/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0032 - val_loss: 0.0065\n",
      "Epoch 52/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0032 - val_loss: 0.0065\n",
      "Epoch 53/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0031 - val_loss: 0.0069\n",
      "Epoch 54/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 0.0031 - val_loss: 0.0065\n",
      "Epoch 55/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0031 - val_loss: 0.0065\n",
      "Epoch 56/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0030 - val_loss: 0.0065\n",
      "Epoch 57/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0030 - val_loss: 0.0064\n",
      "Epoch 58/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 0.0029 - val_loss: 0.0063\n",
      "Epoch 59/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0029 - val_loss: 0.0063\n",
      "Epoch 60/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0029 - val_loss: 0.0063\n",
      "Epoch 61/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 0.0029 - val_loss: 0.0063\n",
      "Epoch 62/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0028 - val_loss: 0.0062\n",
      "Epoch 63/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0028 - val_loss: 0.0062\n",
      "Epoch 64/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0028 - val_loss: 0.0062\n",
      "Epoch 65/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0027 - val_loss: 0.0062\n",
      "Epoch 66/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 0.0027 - val_loss: 0.0062\n",
      "Epoch 67/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0027 - val_loss: 0.0061\n",
      "Epoch 68/500\n",
      "1095/1095 [==============================] - 0s 55us/step - loss: 0.0027 - val_loss: 0.0062\n",
      "Epoch 69/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0026 - val_loss: 0.0061\n",
      "Epoch 70/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0026 - val_loss: 0.0061\n",
      "Epoch 71/500\n",
      "1095/1095 [==============================] - 0s 57us/step - loss: 0.0026 - val_loss: 0.0062\n",
      "Epoch 72/500\n",
      "1095/1095 [==============================] - 0s 55us/step - loss: 0.0026 - val_loss: 0.0061\n",
      "Epoch 73/500\n",
      "1095/1095 [==============================] - 0s 55us/step - loss: 0.0025 - val_loss: 0.0060\n",
      "Epoch 74/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0025 - val_loss: 0.0060\n",
      "Epoch 75/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0025 - val_loss: 0.0062\n",
      "Epoch 76/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 0.0025 - val_loss: 0.0063\n",
      "Epoch 77/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0025 - val_loss: 0.0060\n",
      "Epoch 78/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0024 - val_loss: 0.0064\n",
      "Epoch 79/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 0.0024 - val_loss: 0.0060\n",
      "Epoch 80/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0024 - val_loss: 0.0059\n",
      "Epoch 81/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 0.0024 - val_loss: 0.0059\n",
      "Epoch 82/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0024 - val_loss: 0.0060\n",
      "Epoch 83/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0023 - val_loss: 0.0060\n",
      "Epoch 84/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0023 - val_loss: 0.0060\n",
      "Epoch 85/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0023 - val_loss: 0.0059\n",
      "Epoch 86/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0023 - val_loss: 0.0059\n",
      "Epoch 87/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0022 - val_loss: 0.0060\n",
      "Epoch 88/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 0.0023 - val_loss: 0.0059\n",
      "Epoch 89/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0022 - val_loss: 0.0058\n",
      "Epoch 90/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 0.0022 - val_loss: 0.0058\n",
      "Epoch 91/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0022 - val_loss: 0.0058\n",
      "Epoch 92/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0022 - val_loss: 0.0060\n",
      "Epoch 93/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0022 - val_loss: 0.0058\n",
      "Epoch 94/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 0.0021 - val_loss: 0.0058\n",
      "Epoch 95/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0021 - val_loss: 0.0058\n",
      "Epoch 96/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0021 - val_loss: 0.0058\n",
      "Epoch 97/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0021 - val_loss: 0.0058\n",
      "Epoch 98/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0021 - val_loss: 0.0057\n",
      "Epoch 99/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 0.0021 - val_loss: 0.0057\n",
      "Epoch 100/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0020 - val_loss: 0.0057\n",
      "Epoch 101/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0020 - val_loss: 0.0057\n",
      "Epoch 102/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0020 - val_loss: 0.0057\n",
      "Epoch 103/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0020 - val_loss: 0.0057\n",
      "Epoch 104/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 0.0020 - val_loss: 0.0057\n",
      "Epoch 105/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 0.0020 - val_loss: 0.0057\n",
      "Epoch 106/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0020 - val_loss: 0.0057\n",
      "Epoch 107/500\n",
      "1095/1095 [==============================] - 0s 71us/step - loss: 0.0020 - val_loss: 0.0057\n",
      "Epoch 108/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 0.0019 - val_loss: 0.0056\n",
      "Epoch 109/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 0.0019 - val_loss: 0.0058\n",
      "Epoch 110/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 0.0019 - val_loss: 0.0058\n",
      "Epoch 111/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 0.0019 - val_loss: 0.0056\n",
      "Epoch 112/500\n",
      "1095/1095 [==============================] - 0s 67us/step - loss: 0.0019 - val_loss: 0.0056\n",
      "Epoch 113/500\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 0.0019 - val_loss: 0.0057\n",
      "Epoch 114/500\n",
      "1095/1095 [==============================] - 0s 65us/step - loss: 0.0019 - val_loss: 0.0056\n",
      "Epoch 115/500\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 0.0019 - val_loss: 0.0057\n",
      "Epoch 116/500\n",
      "1095/1095 [==============================] - 0s 60us/step - loss: 0.0019 - val_loss: 0.0055\n",
      "Epoch 117/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 0.0018 - val_loss: 0.0055\n",
      "Epoch 118/500\n",
      "1095/1095 [==============================] - 0s 55us/step - loss: 0.0018 - val_loss: 0.0056\n",
      "Epoch 119/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0018 - val_loss: 0.0055\n",
      "Epoch 120/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0018 - val_loss: 0.0055\n",
      "Epoch 121/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0018 - val_loss: 0.0055\n",
      "Epoch 122/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0018 - val_loss: 0.0056\n",
      "Epoch 123/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0018 - val_loss: 0.0055\n",
      "Epoch 124/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0018 - val_loss: 0.0055\n",
      "Epoch 125/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0018 - val_loss: 0.0055\n",
      "Epoch 126/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0018 - val_loss: 0.0055\n",
      "Epoch 127/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0017 - val_loss: 0.0055\n",
      "Epoch 128/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0017 - val_loss: 0.0057\n",
      "Epoch 129/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0017 - val_loss: 0.0055\n",
      "Epoch 130/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0017 - val_loss: 0.0057\n",
      "Epoch 131/500\n",
      "1095/1095 [==============================] - 0s 55us/step - loss: 0.0017 - val_loss: 0.0056\n",
      "Epoch 132/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0017 - val_loss: 0.0054\n",
      "Epoch 133/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0017 - val_loss: 0.0055\n",
      "Epoch 134/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0017 - val_loss: 0.0054\n",
      "Epoch 135/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0017 - val_loss: 0.0054\n",
      "Epoch 136/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0017 - val_loss: 0.0054\n",
      "Epoch 137/500\n",
      "1095/1095 [==============================] - 0s 55us/step - loss: 0.0016 - val_loss: 0.0055\n",
      "Epoch 138/500\n",
      "1095/1095 [==============================] - 0s 55us/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 139/500\n",
      "1095/1095 [==============================] - 0s 58us/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 140/500\n",
      "1095/1095 [==============================] - 0s 55us/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 141/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 142/500\n",
      "1095/1095 [==============================] - 0s 55us/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 143/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 144/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 145/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 146/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 147/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 148/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 149/500\n",
      "1095/1095 [==============================] - 0s 56us/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 150/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 0.0016 - val_loss: 0.0053\n",
      "Epoch 151/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 0.0015 - val_loss: 0.0053\n",
      "Epoch 152/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 0.0015 - val_loss: 0.0054\n",
      "Epoch 153/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 0.0015 - val_loss: 0.0053\n",
      "Epoch 154/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0015 - val_loss: 0.0053\n",
      "Epoch 155/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0015 - val_loss: 0.0053\n",
      "Epoch 156/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0015 - val_loss: 0.0054\n",
      "Epoch 157/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0015 - val_loss: 0.0053\n",
      "Epoch 158/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0015 - val_loss: 0.0057\n",
      "Epoch 159/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0015 - val_loss: 0.0053\n",
      "Epoch 160/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 0.0015 - val_loss: 0.0053\n",
      "Epoch 161/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 0.0015 - val_loss: 0.0053\n",
      "Epoch 162/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0015 - val_loss: 0.0053\n",
      "Epoch 163/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0015 - val_loss: 0.0053\n",
      "Epoch 164/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0014 - val_loss: 0.0053\n",
      "Epoch 165/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 0.0014 - val_loss: 0.0053\n",
      "Epoch 166/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0014 - val_loss: 0.0053\n",
      "Epoch 167/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0014 - val_loss: 0.0054\n",
      "Epoch 168/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0014 - val_loss: 0.0053\n",
      "Epoch 169/500\n",
      "1095/1095 [==============================] - 0s 56us/step - loss: 0.0014 - val_loss: 0.0053\n",
      "Epoch 170/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0014 - val_loss: 0.0053\n",
      "Epoch 171/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0014 - val_loss: 0.0053\n",
      "Epoch 172/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0014 - val_loss: 0.0052\n",
      "Epoch 173/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 0.0014 - val_loss: 0.0053\n",
      "Epoch 174/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 0.0014 - val_loss: 0.0052\n",
      "Epoch 175/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0014 - val_loss: 0.0053\n",
      "Epoch 176/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 0.0014 - val_loss: 0.0053\n",
      "Epoch 177/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0014 - val_loss: 0.0052\n",
      "Epoch 178/500\n",
      "1095/1095 [==============================] - 0s 44us/step - loss: 0.0014 - val_loss: 0.0052\n",
      "Epoch 179/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 0.0014 - val_loss: 0.0053\n",
      "Epoch 180/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 0.0014 - val_loss: 0.0052\n",
      "Epoch 181/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 0.0013 - val_loss: 0.0052\n",
      "Epoch 182/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0013 - val_loss: 0.0052\n",
      "Epoch 183/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0013 - val_loss: 0.0052\n",
      "Epoch 184/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0013 - val_loss: 0.0052\n",
      "Epoch 185/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 0.0013 - val_loss: 0.0054\n",
      "Epoch 186/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0013 - val_loss: 0.0052\n",
      "Epoch 187/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0013 - val_loss: 0.0052\n",
      "Epoch 188/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0013 - val_loss: 0.0052\n",
      "Epoch 189/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0013 - val_loss: 0.0052\n",
      "Epoch 190/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 0.0013 - val_loss: 0.0052\n",
      "Epoch 191/500\n",
      "1095/1095 [==============================] - 0s 60us/step - loss: 0.0013 - val_loss: 0.0052\n",
      "Epoch 192/500\n",
      "1095/1095 [==============================] - 0s 57us/step - loss: 0.0013 - val_loss: 0.0052\n",
      "Epoch 193/500\n",
      "1095/1095 [==============================] - 0s 57us/step - loss: 0.0013 - val_loss: 0.0051\n",
      "Epoch 194/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0013 - val_loss: 0.0051\n",
      "Epoch 195/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0013 - val_loss: 0.0052\n",
      "Epoch 196/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0013 - val_loss: 0.0051\n",
      "Epoch 197/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0013 - val_loss: 0.0051\n",
      "Epoch 198/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0013 - val_loss: 0.0051\n",
      "Epoch 199/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0013 - val_loss: 0.0054\n",
      "Epoch 200/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0013 - val_loss: 0.0051\n",
      "Epoch 201/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 0.0012 - val_loss: 0.0051\n",
      "Epoch 202/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0012 - val_loss: 0.0051\n",
      "Epoch 203/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 0.0012 - val_loss: 0.0051\n",
      "Epoch 204/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0012 - val_loss: 0.0051\n",
      "Epoch 205/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0012 - val_loss: 0.0051\n",
      "Epoch 206/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 0.0012 - val_loss: 0.0052\n",
      "Epoch 207/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 0.0012 - val_loss: 0.0051\n",
      "Epoch 208/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0012 - val_loss: 0.0051\n",
      "Epoch 209/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0012 - val_loss: 0.0051\n",
      "Epoch 210/500\n",
      "1095/1095 [==============================] - 0s 55us/step - loss: 0.0012 - val_loss: 0.0052\n",
      "Epoch 211/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0012 - val_loss: 0.0051\n",
      "Epoch 212/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0012 - val_loss: 0.0051\n",
      "Epoch 213/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0012 - val_loss: 0.0051\n",
      "Epoch 214/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0012 - val_loss: 0.0051\n",
      "Epoch 215/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 0.0012 - val_loss: 0.0051\n",
      "Epoch 216/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0012 - val_loss: 0.0051\n",
      "Epoch 217/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0012 - val_loss: 0.0051\n",
      "Epoch 218/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0012 - val_loss: 0.0051\n",
      "Epoch 219/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0012 - val_loss: 0.0051\n",
      "Epoch 220/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 0.0012 - val_loss: 0.0051\n",
      "Epoch 221/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0012 - val_loss: 0.0051\n",
      "Epoch 222/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 0.0012 - val_loss: 0.0051\n",
      "Epoch 223/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0011 - val_loss: 0.0051\n",
      "Epoch 224/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0011 - val_loss: 0.0051\n",
      "Epoch 225/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0011 - val_loss: 0.0051\n",
      "Epoch 226/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0011 - val_loss: 0.0051\n",
      "Epoch 227/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0011 - val_loss: 0.0051\n",
      "Epoch 228/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0011 - val_loss: 0.0050\n",
      "Epoch 229/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0011 - val_loss: 0.0051\n",
      "Epoch 230/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 0.0011 - val_loss: 0.0051\n",
      "Epoch 231/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1095/1095 [==============================] - 0s 45us/step - loss: 0.0011 - val_loss: 0.0050\n",
      "Epoch 232/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0011 - val_loss: 0.0050\n",
      "Epoch 233/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0011 - val_loss: 0.0051\n",
      "Epoch 234/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0011 - val_loss: 0.0050\n",
      "Epoch 235/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0011 - val_loss: 0.0050\n",
      "Epoch 236/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0011 - val_loss: 0.0051\n",
      "Epoch 237/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0011 - val_loss: 0.0050\n",
      "Epoch 238/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0011 - val_loss: 0.0051\n",
      "Epoch 239/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0011 - val_loss: 0.0050\n",
      "Epoch 240/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 0.0011 - val_loss: 0.0050\n",
      "Epoch 241/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0011 - val_loss: 0.0051\n",
      "Epoch 242/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 0.0011 - val_loss: 0.0050\n",
      "Epoch 243/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0011 - val_loss: 0.0050\n",
      "Epoch 244/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 0.0011 - val_loss: 0.0050\n",
      "Epoch 245/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0011 - val_loss: 0.0050\n",
      "Epoch 246/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0011 - val_loss: 0.0052\n",
      "Epoch 247/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 0.0011 - val_loss: 0.0051\n",
      "Epoch 248/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0011 - val_loss: 0.0050\n",
      "Epoch 249/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0011 - val_loss: 0.0050\n",
      "Epoch 250/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0010 - val_loss: 0.0050\n",
      "Epoch 251/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0010 - val_loss: 0.0050\n",
      "Epoch 252/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0010 - val_loss: 0.0050\n",
      "Epoch 253/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 0.0010 - val_loss: 0.0050\n",
      "Epoch 254/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 0.0010 - val_loss: 0.0050\n",
      "Epoch 255/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0010 - val_loss: 0.0050\n",
      "Epoch 256/500\n",
      "1095/1095 [==============================] - 0s 44us/step - loss: 0.0010 - val_loss: 0.0051\n",
      "Epoch 257/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 0.0010 - val_loss: 0.0050\n",
      "Epoch 258/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 0.0010 - val_loss: 0.0050\n",
      "Epoch 259/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 0.0010 - val_loss: 0.0050\n",
      "Epoch 260/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 0.0010 - val_loss: 0.0050\n",
      "Epoch 261/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0010 - val_loss: 0.0050\n",
      "Epoch 262/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0010 - val_loss: 0.0050\n",
      "Epoch 263/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0010 - val_loss: 0.0050\n",
      "Epoch 264/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0010 - val_loss: 0.0050\n",
      "Epoch 265/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 9.9413e-04 - val_loss: 0.0050\n",
      "Epoch 266/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 9.9448e-04 - val_loss: 0.0050\n",
      "Epoch 267/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 9.8893e-04 - val_loss: 0.0050\n",
      "Epoch 268/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 9.9316e-04 - val_loss: 0.0050\n",
      "Epoch 269/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 9.8026e-04 - val_loss: 0.0050\n",
      "Epoch 270/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 9.9163e-04 - val_loss: 0.0049\n",
      "Epoch 271/500\n",
      "1095/1095 [==============================] - 0s 44us/step - loss: 9.7240e-04 - val_loss: 0.0049\n",
      "Epoch 272/500\n",
      "1095/1095 [==============================] - 0s 44us/step - loss: 9.7188e-04 - val_loss: 0.0049\n",
      "Epoch 273/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 9.7540e-04 - val_loss: 0.0049\n",
      "Epoch 274/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 9.6931e-04 - val_loss: 0.0050\n",
      "Epoch 275/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 9.7291e-04 - val_loss: 0.0050\n",
      "Epoch 276/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 9.6718e-04 - val_loss: 0.0049\n",
      "Epoch 277/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 9.6084e-04 - val_loss: 0.0049\n",
      "Epoch 278/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 9.5128e-04 - val_loss: 0.0049\n",
      "Epoch 279/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 9.5112e-04 - val_loss: 0.0050\n",
      "Epoch 280/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 9.5197e-04 - val_loss: 0.0049\n",
      "Epoch 281/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 9.5107e-04 - val_loss: 0.0049\n",
      "Epoch 282/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 9.4774e-04 - val_loss: 0.0050\n",
      "Epoch 283/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 9.4990e-04 - val_loss: 0.0049\n",
      "Epoch 284/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 9.4177e-04 - val_loss: 0.0049\n",
      "Epoch 285/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 9.3979e-04 - val_loss: 0.0049\n",
      "Epoch 286/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 9.2985e-04 - val_loss: 0.0049\n",
      "Epoch 287/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 9.3172e-04 - val_loss: 0.0049\n",
      "Epoch 288/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 9.3343e-04 - val_loss: 0.0049\n",
      "Epoch 289/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 9.2597e-04 - val_loss: 0.0050\n",
      "Epoch 290/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 9.2119e-04 - val_loss: 0.0049\n",
      "Epoch 291/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 9.1788e-04 - val_loss: 0.0049\n",
      "Epoch 292/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 9.1653e-04 - val_loss: 0.0049\n",
      "Epoch 293/500\n",
      "1095/1095 [==============================] - 0s 44us/step - loss: 9.1197e-04 - val_loss: 0.0049\n",
      "Epoch 294/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 9.1674e-04 - val_loss: 0.0049\n",
      "Epoch 295/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 9.1134e-04 - val_loss: 0.0049\n",
      "Epoch 296/500\n",
      "1095/1095 [==============================] - 0s 44us/step - loss: 9.0689e-04 - val_loss: 0.0049\n",
      "Epoch 297/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 9.0281e-04 - val_loss: 0.0049\n",
      "Epoch 298/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 9.0375e-04 - val_loss: 0.0049\n",
      "Epoch 299/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 9.0081e-04 - val_loss: 0.0049\n",
      "Epoch 300/500\n",
      "1095/1095 [==============================] - 0s 43us/step - loss: 8.9579e-04 - val_loss: 0.0049\n",
      "Epoch 301/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 8.9853e-04 - val_loss: 0.0049\n",
      "Epoch 302/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 8.9379e-04 - val_loss: 0.0049\n",
      "Epoch 303/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 8.9627e-04 - val_loss: 0.0049\n",
      "Epoch 304/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 8.8842e-04 - val_loss: 0.0049\n",
      "Epoch 305/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 8.8324e-04 - val_loss: 0.0049\n",
      "Epoch 306/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 8.8065e-04 - val_loss: 0.0049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 307/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 8.8394e-04 - val_loss: 0.0049\n",
      "Epoch 308/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 8.7814e-04 - val_loss: 0.0049\n",
      "Epoch 309/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 8.7378e-04 - val_loss: 0.0050\n",
      "Epoch 310/500\n",
      "1095/1095 [==============================] - 0s 44us/step - loss: 8.7684e-04 - val_loss: 0.0049\n",
      "Epoch 311/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 8.6890e-04 - val_loss: 0.0049\n",
      "Epoch 312/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 8.6500e-04 - val_loss: 0.0049\n",
      "Epoch 313/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 8.6396e-04 - val_loss: 0.0049\n",
      "Epoch 314/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 8.5988e-04 - val_loss: 0.0049\n",
      "Epoch 315/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 8.5941e-04 - val_loss: 0.0051\n",
      "Epoch 316/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 8.6946e-04 - val_loss: 0.0049\n",
      "Epoch 317/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 8.5793e-04 - val_loss: 0.0049\n",
      "Epoch 318/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 8.5621e-04 - val_loss: 0.0049\n",
      "Epoch 319/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 8.5485e-04 - val_loss: 0.0049\n",
      "Epoch 320/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 8.5337e-04 - val_loss: 0.0049\n",
      "Epoch 321/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 8.5336e-04 - val_loss: 0.0049\n",
      "Epoch 322/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 8.4526e-04 - val_loss: 0.0049\n",
      "Epoch 323/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 8.4509e-04 - val_loss: 0.0049\n",
      "Epoch 324/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 8.4594e-04 - val_loss: 0.0049\n",
      "Epoch 325/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 8.4725e-04 - val_loss: 0.0049\n",
      "Epoch 326/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 8.3263e-04 - val_loss: 0.0049\n",
      "Epoch 327/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 8.2962e-04 - val_loss: 0.0049\n",
      "Epoch 328/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 8.3602e-04 - val_loss: 0.0049\n",
      "Epoch 329/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 8.3021e-04 - val_loss: 0.0049\n",
      "Epoch 330/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 8.2266e-04 - val_loss: 0.0049\n",
      "Epoch 331/500\n",
      "1095/1095 [==============================] - 0s 44us/step - loss: 8.2798e-04 - val_loss: 0.0049\n",
      "Epoch 332/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 8.2722e-04 - val_loss: 0.0049\n",
      "Epoch 333/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 8.2001e-04 - val_loss: 0.0049\n",
      "Epoch 334/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 8.1157e-04 - val_loss: 0.0049\n",
      "Epoch 335/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 8.1291e-04 - val_loss: 0.0049\n",
      "Epoch 336/500\n",
      "1095/1095 [==============================] - 0s 44us/step - loss: 8.1603e-04 - val_loss: 0.0049\n",
      "Epoch 337/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 8.1467e-04 - val_loss: 0.0050\n",
      "Epoch 338/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 8.1727e-04 - val_loss: 0.0049\n",
      "Epoch 339/500\n",
      "1095/1095 [==============================] - 0s 44us/step - loss: 8.0797e-04 - val_loss: 0.0048\n",
      "Epoch 340/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 8.0734e-04 - val_loss: 0.0049\n",
      "Epoch 341/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 8.0381e-04 - val_loss: 0.0049\n",
      "Epoch 342/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 7.9751e-04 - val_loss: 0.0049\n",
      "Epoch 343/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 8.0295e-04 - val_loss: 0.0049\n",
      "Epoch 344/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 8.0795e-04 - val_loss: 0.0049\n",
      "Epoch 345/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 7.9747e-04 - val_loss: 0.0048\n",
      "Epoch 346/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 7.9063e-04 - val_loss: 0.0049\n",
      "Epoch 347/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 7.9118e-04 - val_loss: 0.0049\n",
      "Epoch 348/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 7.8668e-04 - val_loss: 0.0048\n",
      "Epoch 349/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 7.9131e-04 - val_loss: 0.0049\n",
      "Epoch 350/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 7.7954e-04 - val_loss: 0.0049\n",
      "Epoch 351/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 7.8397e-04 - val_loss: 0.0048\n",
      "Epoch 352/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 7.7816e-04 - val_loss: 0.0049\n",
      "Epoch 353/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 7.8246e-04 - val_loss: 0.0049\n",
      "Epoch 354/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 7.8437e-04 - val_loss: 0.0048\n",
      "Epoch 355/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 7.7329e-04 - val_loss: 0.0048\n",
      "Epoch 356/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 7.7432e-04 - val_loss: 0.0048\n",
      "Epoch 357/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 7.7101e-04 - val_loss: 0.0048\n",
      "Epoch 358/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 7.7348e-04 - val_loss: 0.0049\n",
      "Epoch 359/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 7.6923e-04 - val_loss: 0.0048\n",
      "Epoch 360/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 7.6422e-04 - val_loss: 0.0048\n",
      "Epoch 361/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 7.6280e-04 - val_loss: 0.0048\n",
      "Epoch 362/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 7.6154e-04 - val_loss: 0.0048\n",
      "Epoch 363/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 7.6318e-04 - val_loss: 0.0048\n",
      "Epoch 364/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 7.5814e-04 - val_loss: 0.0048\n",
      "Epoch 365/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 7.6103e-04 - val_loss: 0.0048\n",
      "Epoch 366/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 7.5382e-04 - val_loss: 0.0048\n",
      "Epoch 367/500\n",
      "1095/1095 [==============================] - 0s 44us/step - loss: 7.5547e-04 - val_loss: 0.0048\n",
      "Epoch 368/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 7.5788e-04 - val_loss: 0.0048\n",
      "Epoch 369/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 7.4838e-04 - val_loss: 0.0048\n",
      "Epoch 370/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 7.4833e-04 - val_loss: 0.0048\n",
      "Epoch 371/500\n",
      "1095/1095 [==============================] - 0s 44us/step - loss: 7.4460e-04 - val_loss: 0.0048\n",
      "Epoch 372/500\n",
      "1095/1095 [==============================] - 0s 44us/step - loss: 7.4473e-04 - val_loss: 0.0048\n",
      "Epoch 373/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 7.4391e-04 - val_loss: 0.0048\n",
      "Epoch 374/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 7.3643e-04 - val_loss: 0.0048\n",
      "Epoch 375/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 7.4065e-04 - val_loss: 0.0048\n",
      "Epoch 376/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 7.3792e-04 - val_loss: 0.0048\n",
      "Epoch 377/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 7.3413e-04 - val_loss: 0.0048\n",
      "Epoch 378/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 7.2975e-04 - val_loss: 0.0048\n",
      "Epoch 379/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 7.3809e-04 - val_loss: 0.0048\n",
      "Epoch 380/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 7.3305e-04 - val_loss: 0.0048\n",
      "Epoch 381/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1095/1095 [==============================] - 0s 45us/step - loss: 7.2795e-04 - val_loss: 0.0048\n",
      "Epoch 382/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 7.2459e-04 - val_loss: 0.0048\n",
      "Epoch 383/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 7.3030e-04 - val_loss: 0.0048\n",
      "Epoch 384/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 7.2643e-04 - val_loss: 0.0048\n",
      "Epoch 385/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 7.1993e-04 - val_loss: 0.0048\n",
      "Epoch 386/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 7.2321e-04 - val_loss: 0.0049\n",
      "Epoch 387/500\n",
      "1095/1095 [==============================] - 0s 44us/step - loss: 7.2580e-04 - val_loss: 0.0048\n",
      "Epoch 388/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 7.1782e-04 - val_loss: 0.0048\n",
      "Epoch 389/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 7.1838e-04 - val_loss: 0.0048\n",
      "Epoch 390/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 7.1524e-04 - val_loss: 0.0048\n",
      "Epoch 391/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 7.1068e-04 - val_loss: 0.0048\n",
      "Epoch 392/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 7.1217e-04 - val_loss: 0.0048\n",
      "Epoch 393/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 7.0987e-04 - val_loss: 0.0048\n",
      "Epoch 394/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 7.0093e-04 - val_loss: 0.0048\n",
      "Epoch 395/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 7.0473e-04 - val_loss: 0.0048\n",
      "Epoch 396/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 7.0645e-04 - val_loss: 0.0048\n",
      "Epoch 397/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 7.0075e-04 - val_loss: 0.0048\n",
      "Epoch 398/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 7.0208e-04 - val_loss: 0.0048\n",
      "Epoch 399/500\n",
      "1095/1095 [==============================] - 0s 44us/step - loss: 6.9379e-04 - val_loss: 0.0048\n",
      "Epoch 400/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 6.9677e-04 - val_loss: 0.0048\n",
      "Epoch 401/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 6.9593e-04 - val_loss: 0.0048\n",
      "Epoch 402/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 6.9581e-04 - val_loss: 0.0048\n",
      "Epoch 403/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 6.9024e-04 - val_loss: 0.0048\n",
      "Epoch 404/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 6.8808e-04 - val_loss: 0.0048\n",
      "Epoch 405/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 6.9315e-04 - val_loss: 0.0048\n",
      "Epoch 406/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 6.8555e-04 - val_loss: 0.0048\n",
      "Epoch 407/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 6.8608e-04 - val_loss: 0.0048\n",
      "Epoch 408/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 6.9091e-04 - val_loss: 0.0048\n",
      "Epoch 409/500\n",
      "1095/1095 [==============================] - 0s 44us/step - loss: 6.8431e-04 - val_loss: 0.0048\n",
      "Epoch 410/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 6.8365e-04 - val_loss: 0.0048\n",
      "Epoch 411/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 6.8395e-04 - val_loss: 0.0048\n",
      "Epoch 412/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 6.7426e-04 - val_loss: 0.0048\n",
      "Epoch 413/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 6.7822e-04 - val_loss: 0.0048\n",
      "Epoch 414/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 6.7105e-04 - val_loss: 0.0048\n",
      "Epoch 415/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 6.7231e-04 - val_loss: 0.0048\n",
      "Epoch 416/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 6.6782e-04 - val_loss: 0.0048\n",
      "Epoch 417/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 6.6902e-04 - val_loss: 0.0049\n",
      "Epoch 418/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 6.7518e-04 - val_loss: 0.0048\n",
      "Epoch 419/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 6.6837e-04 - val_loss: 0.0048\n",
      "Epoch 420/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 6.6450e-04 - val_loss: 0.0048\n",
      "Epoch 421/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 6.6537e-04 - val_loss: 0.0048\n",
      "Epoch 422/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 6.6544e-04 - val_loss: 0.0048\n",
      "Epoch 423/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 6.6313e-04 - val_loss: 0.0048\n",
      "Epoch 424/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 6.5653e-04 - val_loss: 0.0049\n",
      "Epoch 425/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 6.6001e-04 - val_loss: 0.0048\n",
      "Epoch 426/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 6.5317e-04 - val_loss: 0.0048\n",
      "Epoch 427/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 6.5303e-04 - val_loss: 0.0048\n",
      "Epoch 428/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 6.5248e-04 - val_loss: 0.0048\n",
      "Epoch 429/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 6.5116e-04 - val_loss: 0.0048\n",
      "Epoch 430/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 6.4921e-04 - val_loss: 0.0048\n",
      "Epoch 431/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 6.4945e-04 - val_loss: 0.0048\n",
      "Epoch 432/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 6.4634e-04 - val_loss: 0.0048\n",
      "Epoch 433/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 6.4829e-04 - val_loss: 0.0048\n",
      "Epoch 434/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 6.4682e-04 - val_loss: 0.0048\n",
      "Epoch 435/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 6.4336e-04 - val_loss: 0.0048\n",
      "Epoch 436/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 6.4782e-04 - val_loss: 0.0048\n",
      "Epoch 437/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 6.4335e-04 - val_loss: 0.0048\n",
      "Epoch 438/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 6.3987e-04 - val_loss: 0.0048\n",
      "Epoch 439/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 6.3676e-04 - val_loss: 0.0048\n",
      "Epoch 440/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 6.3553e-04 - val_loss: 0.0048\n",
      "Epoch 441/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 6.3678e-04 - val_loss: 0.0049\n",
      "Epoch 442/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 6.4017e-04 - val_loss: 0.0048\n",
      "Epoch 443/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 6.3383e-04 - val_loss: 0.0048\n",
      "Epoch 444/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 6.3227e-04 - val_loss: 0.0048\n",
      "Epoch 445/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 6.3107e-04 - val_loss: 0.0048\n",
      "Epoch 446/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 6.2795e-04 - val_loss: 0.0048\n",
      "Epoch 447/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 6.2644e-04 - val_loss: 0.0048\n",
      "Epoch 448/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 6.2470e-04 - val_loss: 0.0048\n",
      "Epoch 449/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 6.2576e-04 - val_loss: 0.0048\n",
      "Epoch 450/500\n",
      "1095/1095 [==============================] - 0s 56us/step - loss: 6.2207e-04 - val_loss: 0.0048\n",
      "Epoch 451/500\n",
      "1095/1095 [==============================] - 0s 60us/step - loss: 6.1986e-04 - val_loss: 0.0048\n",
      "Epoch 452/500\n",
      "1095/1095 [==============================] - 0s 55us/step - loss: 6.2197e-04 - val_loss: 0.0048\n",
      "Epoch 453/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 6.2360e-04 - val_loss: 0.0048\n",
      "Epoch 454/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 6.1617e-04 - val_loss: 0.0048\n",
      "Epoch 455/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1095/1095 [==============================] - 0s 48us/step - loss: 6.1676e-04 - val_loss: 0.0048\n",
      "Epoch 456/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 6.2112e-04 - val_loss: 0.0048\n",
      "Epoch 457/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 6.1422e-04 - val_loss: 0.0047\n",
      "Epoch 458/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 6.1405e-04 - val_loss: 0.0048\n",
      "Epoch 459/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 6.1373e-04 - val_loss: 0.0048\n",
      "Epoch 460/500\n",
      "1095/1095 [==============================] - 0s 43us/step - loss: 6.1195e-04 - val_loss: 0.0047\n",
      "Epoch 461/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 6.0496e-04 - val_loss: 0.0048\n",
      "Epoch 462/500\n",
      "1095/1095 [==============================] - 0s 55us/step - loss: 6.0806e-04 - val_loss: 0.0047\n",
      "Epoch 463/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 6.0275e-04 - val_loss: 0.0047\n",
      "Epoch 464/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 6.0708e-04 - val_loss: 0.0047\n",
      "Epoch 465/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 6.0359e-04 - val_loss: 0.0047\n",
      "Epoch 466/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 6.0940e-04 - val_loss: 0.0047\n",
      "Epoch 467/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 5.9903e-04 - val_loss: 0.0047\n",
      "Epoch 468/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 6.0092e-04 - val_loss: 0.0047\n",
      "Epoch 469/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 5.9731e-04 - val_loss: 0.0047\n",
      "Epoch 470/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 5.9599e-04 - val_loss: 0.0047\n",
      "Epoch 471/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 5.9505e-04 - val_loss: 0.0047\n",
      "Epoch 472/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 5.9458e-04 - val_loss: 0.0047\n",
      "Epoch 473/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 5.9373e-04 - val_loss: 0.0047\n",
      "Epoch 474/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 5.9132e-04 - val_loss: 0.0047\n",
      "Epoch 475/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 5.9060e-04 - val_loss: 0.0047\n",
      "Epoch 476/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 5.9386e-04 - val_loss: 0.0047\n",
      "Epoch 477/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 5.8810e-04 - val_loss: 0.0047\n",
      "Epoch 478/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 5.8733e-04 - val_loss: 0.0048\n",
      "Epoch 479/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 5.8510e-04 - val_loss: 0.0048\n",
      "Epoch 480/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 5.9024e-04 - val_loss: 0.0047\n",
      "Epoch 481/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 5.8604e-04 - val_loss: 0.0047\n",
      "Epoch 482/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 5.8121e-04 - val_loss: 0.0047\n",
      "Epoch 483/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 5.8485e-04 - val_loss: 0.0047\n",
      "Epoch 484/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 5.8499e-04 - val_loss: 0.0048\n",
      "Epoch 485/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 5.7970e-04 - val_loss: 0.0047\n",
      "Epoch 486/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 5.7933e-04 - val_loss: 0.0048\n",
      "Epoch 487/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 5.7707e-04 - val_loss: 0.0047\n",
      "Epoch 488/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 5.7556e-04 - val_loss: 0.0047\n",
      "Epoch 489/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 5.7339e-04 - val_loss: 0.0047\n",
      "Epoch 490/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 5.7487e-04 - val_loss: 0.0047\n",
      "Epoch 491/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 5.7538e-04 - val_loss: 0.0047\n",
      "Epoch 492/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 5.7056e-04 - val_loss: 0.0047\n",
      "Epoch 493/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 5.6786e-04 - val_loss: 0.0047\n",
      "Epoch 494/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 5.6876e-04 - val_loss: 0.0047\n",
      "Epoch 495/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 5.7071e-04 - val_loss: 0.0047\n",
      "Epoch 496/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 5.6377e-04 - val_loss: 0.0048\n",
      "Epoch 497/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 5.6867e-04 - val_loss: 0.0047\n",
      "Epoch 498/500\n",
      "1095/1095 [==============================] - 0s 45us/step - loss: 5.6604e-04 - val_loss: 0.0047\n",
      "Epoch 499/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 5.6441e-04 - val_loss: 0.0047\n",
      "Epoch 500/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 5.6365e-04 - val_loss: 0.0047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20c34b0f9b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers.core import Dense, Dropout\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(160, activation=\"relu\"))\n",
    "model.add(Dense(80, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "\n",
    "X = all_input_feats_train\n",
    "Y = normalized_train_output_matrix\n",
    "print(\"Input X has shape\", X.shape)\n",
    "print(\"Desired output Y has shape\", Y.shape)\n",
    "print(\"Y:\\n\", Y)\n",
    "model.fit(X,Y, validation_split=0.25, epochs=500)\n",
    "#model.fit(X,Y, epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the trained MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds_train_houses:\n",
      " [[ 0.2624]\n",
      " [ 0.2043]\n",
      " [ 0.2538]\n",
      " ..., \n",
      " [ 0.2712]\n",
      " [ 0.2131]\n",
      " [ 0.157 ]]\n",
      "preds_train_houses_dollar:\n",
      " [[ 223853.375 ]\n",
      " [ 182027.9375]\n",
      " [ 217694.5   ]\n",
      " ..., \n",
      " [ 230225.9062]\n",
      " [ 188366.7188]\n",
      " [ 147926.6719]]\n",
      "Shape of preds_train_houses is (1460, 1)\n",
      "Shape of preds_train_houses_dollar is (1460, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAooAAAJQCAYAAAAJ0UXFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20c352ac4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "preds_train_houses = model.predict(all_input_feats_train)\n",
    "print(\"preds_train_houses:\\n\", preds_train_houses)\n",
    "preds_train_houses_dollar = scaler_saleprice.inverse_transform(preds_train_houses)\n",
    "print(\"preds_train_houses_dollar:\\n\", preds_train_houses_dollar)\n",
    "print(\"Shape of preds_train_houses is\", preds_train_houses.shape)\n",
    "print(\"Shape of preds_train_houses_dollar is\", preds_train_houses_dollar.shape)\n",
    "plt.figure( figsize=(10,10) )\n",
    "plt.plot(train_output_matrix, preds_train_houses_dollar, 'ro')\n",
    "plt.xlabel('Real house price', fontsize = 10)\n",
    "plt.ylabel('Predicted house price', fontsize = 10)\n",
    "plt.grid(True)\n",
    "plt.xlim(0,900000)\n",
    "plt.ylim(0,900000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting house prices for the Kaggle competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_house_ids has shape (1459,)\n",
      "preds_test_houses_dollar has shape (1459,)\n",
      "        Id      SalePrice\n",
      "0     1461  114172.648438\n",
      "1     1462  136719.078125\n",
      "2     1463  170296.578125\n",
      "3     1464  176591.718750\n",
      "4     1465  197403.062500\n",
      "5     1466  200901.406250\n",
      "6     1467  169010.906250\n",
      "7     1468  149033.062500\n",
      "8     1469  152929.203125\n",
      "9     1470  149015.796875\n",
      "10    1471  183245.062500\n",
      "11    1472  146745.375000\n",
      "12    1473   98378.085938\n",
      "13    1474  150510.328125\n",
      "14    1475  124725.703125\n",
      "15    1476  444797.343750\n",
      "16    1477  278216.718750\n",
      "17    1478  285705.031250\n",
      "18    1479  332228.906250\n",
      "19    1480  489710.031250\n",
      "20    1481  392378.500000\n",
      "21    1482  226426.468750\n",
      "22    1483  149965.640625\n",
      "23    1484  164474.953125\n",
      "24    1485  164751.500000\n",
      "25    1486  200657.578125\n",
      "26    1487  383216.093750\n",
      "27    1488  223224.921875\n",
      "28    1489  210284.859375\n",
      "29    1490  253290.265625\n",
      "...    ...            ...\n",
      "1429  2890  103093.078125\n",
      "1430  2891  150559.500000\n",
      "1431  2892   98682.687500\n",
      "1432  2893   -7575.770508\n",
      "1433  2894   78106.093750\n",
      "1434  2895  393964.343750\n",
      "1435  2896  314129.468750\n",
      "1436  2897  171453.265625\n",
      "1437  2898  164933.906250\n",
      "1438  2899  235448.125000\n",
      "1439  2900  172816.781250\n",
      "1440  2901  224867.609375\n",
      "1441  2902  156349.046875\n",
      "1442  2903  289147.468750\n",
      "1443  2904  322156.375000\n",
      "1444  2905   34912.859375\n",
      "1445  2906  212276.578125\n",
      "1446  2907  110087.601562\n",
      "1447  2908  189352.531250\n",
      "1448  2909  251400.796875\n",
      "1449  2910   72797.242188\n",
      "1450  2911   69137.828125\n",
      "1451  2912  144305.625000\n",
      "1452  2913   22086.398438\n",
      "1453  2914   27303.271484\n",
      "1454  2915   33447.441406\n",
      "1455  2916   79290.625000\n",
      "1456  2917  171449.562500\n",
      "1457  2918  114712.757812\n",
      "1458  2919  203273.640625\n",
      "\n",
      "[1459 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# PREDICT house prices for all the test houses!\n",
    "preds_test_houses = model.predict(all_input_feats_test)\n",
    "preds_test_houses_dollar = scaler_saleprice.inverse_transform(preds_test_houses)\n",
    "\n",
    "\n",
    "# generate a Pandas dataframe\n",
    "# from the NumPy prediction_matrix\n",
    "preds_test_houses_dollar = preds_test_houses_dollar.reshape(-1)\n",
    "print(\"test_house_ids has shape\", test_house_ids.shape)\n",
    "print(\"preds_test_houses_dollar has shape\", preds_test_houses_dollar.shape)\n",
    "predition_dataframe = pd.DataFrame({'Id'       :test_house_ids,\n",
    "                                    'SalePrice':preds_test_houses_dollar}\n",
    "                                  )\n",
    "\n",
    "# convert column \"Id\" to int64 dtype\n",
    "predition_dataframe = predition_dataframe.astype({\"Id\": int})\n",
    "print(predition_dataframe)\n",
    "\n",
    "# now save the Pandas dataframe to a .csv file\n",
    "PREDICTION_FILENAME = \"my_predicted_house_prices.csv\"\n",
    "predition_dataframe.to_csv(PREDICTION_FILENAME, sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Reference algorithm: Nearest Neighbour Regression\n",
    "\n",
    "A valid question is: How good is the MLP approach?\n",
    "\n",
    "For this, we will follow a straightforward approach here: a new 311D feature vector v will be compared with all the 1460 existing 311D vectors w of the training dataset, we determine the \"most similar\" one wbest and take the house price of wbest as the predicted house price for v.\n",
    "\n",
    "This is a simple approach a real estate agent could follow:\n",
    "\n",
    "\"You want to know for what you can sell your hourse approximately? Ok, let's look in my database of houses I already sold. Your house is most similar to this one. And this house was sold for $X.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1459 for which I will predict sale prices.\n",
      "There are 1460 train data houses.\n",
      "house # 1461 --> $ 129000\n",
      "house # 1462 --> $ 158000\n",
      "house # 1463 --> $ 180000\n",
      "house # 1464 --> $ 178000\n",
      "house # 1465 --> $ 192000\n",
      "house # 1466 --> $ 189000\n",
      "house # 1467 --> $ 206000\n",
      "house # 1468 --> $ 189000\n",
      "house # 1469 --> $ 197500\n",
      "house # 1470 --> $ 97000\n",
      "house # 1471 --> $ 224000\n",
      "house # 1472 --> $ 88000\n",
      "house # 1473 --> $ 88000\n",
      "house # 1474 --> $ 146000\n",
      "house # 1475 --> $ 99500\n",
      "house # 1476 --> $ 252000\n",
      "house # 1477 --> $ 198900\n",
      "house # 1478 --> $ 256300\n",
      "house # 1479 --> $ 378500\n",
      "house # 1480 --> $ 611657\n",
      "house # 1481 --> $ 236000\n",
      "house # 1482 --> $ 192500\n",
      "house # 1483 --> $ 186000\n",
      "house # 1484 --> $ 192000\n",
      "house # 1485 --> $ 188500\n",
      "house # 1486 --> $ 189000\n",
      "house # 1487 --> $ 582933\n",
      "house # 1488 --> $ 287090\n",
      "house # 1489 --> $ 185500\n",
      "house # 1490 --> $ 220000\n",
      "house # 1491 --> $ 215200\n",
      "house # 1492 --> $ 83000\n",
      "house # 1493 --> $ 139000\n",
      "house # 1494 --> $ 236000\n",
      "house # 1495 --> $ 236000\n",
      "house # 1496 --> $ 236500\n",
      "house # 1497 --> $ 151000\n",
      "house # 1498 --> $ 151000\n",
      "house # 1499 --> $ 155000\n",
      "house # 1500 --> $ 155000\n",
      "house # 1501 --> $ 172500\n",
      "house # 1502 --> $ 151000\n",
      "house # 1503 --> $ 395192\n",
      "house # 1504 --> $ 236000\n",
      "house # 1505 --> $ 203000\n",
      "house # 1506 --> $ 185750\n",
      "house # 1507 --> $ 190000\n",
      "house # 1508 --> $ 167900\n",
      "house # 1509 --> $ 154000\n",
      "house # 1510 --> $ 139000\n",
      "house # 1511 --> $ 142000\n",
      "house # 1512 --> $ 142000\n",
      "house # 1513 --> $ 140000\n",
      "house # 1514 --> $ 179000\n",
      "house # 1515 --> $ 205000\n",
      "house # 1516 --> $ 127000\n",
      "house # 1517 --> $ 133000\n",
      "house # 1518 --> $ 124000\n",
      "house # 1519 --> $ 206000\n",
      "house # 1520 --> $ 142000\n",
      "house # 1521 --> $ 139000\n",
      "house # 1522 --> $ 159000\n",
      "house # 1523 --> $ 122000\n",
      "house # 1524 --> $ 117500\n",
      "house # 1525 --> $ 83000\n",
      "house # 1526 --> $ 80000\n",
      "house # 1527 --> $ 134800\n",
      "house # 1528 --> $ 155000\n",
      "house # 1529 --> $ 154000\n",
      "house # 1530 --> $ 162000\n",
      "house # 1531 --> $ 167500\n",
      "house # 1532 --> $ 98000\n",
      "house # 1533 --> $ 117500\n",
      "house # 1534 --> $ 119000\n",
      "house # 1535 --> $ 147000\n",
      "house # 1536 --> $ 68400\n",
      "house # 1537 --> $ 82000\n",
      "house # 1538 --> $ 147000\n",
      "house # 1539 --> $ 128000\n",
      "house # 1540 --> $ 55000\n",
      "house # 1541 --> $ 110000\n",
      "house # 1542 --> $ 161750\n",
      "house # 1543 --> $ 155000\n",
      "house # 1544 --> $ 80000\n",
      "house # 1545 --> $ 128000\n",
      "house # 1546 --> $ 165500\n",
      "house # 1547 --> $ 113000\n",
      "house # 1548 --> $ 110000\n",
      "house # 1549 --> $ 134900\n",
      "house # 1550 --> $ 166000\n",
      "house # 1551 --> $ 100000\n",
      "house # 1552 --> $ 129500\n",
      "house # 1553 --> $ 96500\n",
      "house # 1554 --> $ 143000\n",
      "house # 1555 --> $ 132000\n",
      "house # 1556 --> $ 128000\n",
      "house # 1557 --> $ 75500\n",
      "house # 1558 --> $ 83000\n",
      "house # 1559 --> $ 76500\n",
      "house # 1560 --> $ 95000\n",
      "house # 1561 --> $ 136500\n",
      "house # 1562 --> $ 144900\n",
      "house # 1563 --> $ 145000\n",
      "house # 1564 --> $ 180500\n",
      "house # 1565 --> $ 172500\n",
      "house # 1566 --> $ 260000\n",
      "house # 1567 --> $ 141000\n",
      "house # 1568 --> $ 236000\n",
      "house # 1569 --> $ 134800\n",
      "house # 1570 --> $ 132000\n",
      "house # 1571 --> $ 133000\n",
      "house # 1572 --> $ 153000\n",
      "house # 1573 --> $ 205000\n",
      "house # 1574 --> $ 60000\n",
      "house # 1575 --> $ 159000\n",
      "house # 1576 --> $ 290000\n",
      "house # 1577 --> $ 186000\n",
      "house # 1578 --> $ 105900\n",
      "house # 1579 --> $ 147000\n",
      "house # 1580 --> $ 222500\n",
      "house # 1581 --> $ 154000\n",
      "house # 1582 --> $ 139000\n",
      "house # 1583 --> $ 395192\n",
      "house # 1584 --> $ 189000\n",
      "house # 1585 --> $ 143000\n",
      "house # 1586 --> $ 84900\n",
      "house # 1587 --> $ 122000\n",
      "house # 1588 --> $ 145000\n",
      "house # 1589 --> $ 55993\n",
      "house # 1590 --> $ 155000\n",
      "house # 1591 --> $ 115000\n",
      "house # 1592 --> $ 144500\n",
      "house # 1593 --> $ 149000\n",
      "house # 1594 --> $ 100000\n",
      "house # 1595 --> $ 76500\n",
      "house # 1596 --> $ 135000\n",
      "house # 1597 --> $ 154300\n",
      "house # 1598 --> $ 230000\n",
      "house # 1599 --> $ 147000\n",
      "house # 1600 --> $ 192000\n",
      "house # 1601 --> $ 60000\n",
      "house # 1602 --> $ 125500\n",
      "house # 1603 --> $ 83000\n",
      "house # 1604 --> $ 275000\n",
      "house # 1605 --> $ 236000\n",
      "house # 1606 --> $ 215000\n",
      "house # 1607 --> $ 124000\n",
      "house # 1608 --> $ 306000\n",
      "house # 1609 --> $ 186000\n",
      "house # 1610 --> $ 200500\n",
      "house # 1611 --> $ 107000\n",
      "house # 1612 --> $ 186000\n",
      "house # 1613 --> $ 189000\n",
      "house # 1614 --> $ 180500\n",
      "house # 1615 --> $ 97000\n",
      "house # 1616 --> $ 97000\n",
      "house # 1617 --> $ 83500\n",
      "house # 1618 --> $ 130000\n",
      "house # 1619 --> $ 128200\n",
      "house # 1620 --> $ 202900\n",
      "house # 1621 --> $ 200500\n",
      "house # 1622 --> $ 98000\n",
      "house # 1623 --> $ 220000\n",
      "house # 1624 --> $ 215000\n",
      "house # 1625 --> $ 127500\n",
      "house # 1626 --> $ 157000\n",
      "house # 1627 --> $ 188500\n",
      "house # 1628 --> $ 225000\n",
      "house # 1629 --> $ 168500\n",
      "house # 1630 --> $ 278000\n",
      "house # 1631 --> $ 181000\n",
      "house # 1632 --> $ 181000\n",
      "house # 1633 --> $ 180000\n",
      "house # 1634 --> $ 223500\n",
      "house # 1635 --> $ 244000\n",
      "house # 1636 --> $ 165000\n",
      "house # 1637 --> $ 171000\n",
      "house # 1638 --> $ 192000\n",
      "house # 1639 --> $ 165000\n",
      "house # 1640 --> $ 225000\n",
      "house # 1641 --> $ 157000\n",
      "house # 1642 --> $ 188500\n",
      "house # 1643 --> $ 253000\n",
      "house # 1644 --> $ 194500\n",
      "house # 1645 --> $ 275000\n",
      "house # 1646 --> $ 157000\n",
      "house # 1647 --> $ 165000\n",
      "house # 1648 --> $ 167000\n",
      "house # 1649 --> $ 143500\n",
      "house # 1650 --> $ 123000\n",
      "house # 1651 --> $ 130000\n",
      "house # 1652 --> $ 85400\n",
      "house # 1653 --> $ 85400\n",
      "house # 1654 --> $ 148500\n",
      "house # 1655 --> $ 147000\n",
      "house # 1656 --> $ 148500\n",
      "house # 1657 --> $ 147000\n",
      "house # 1658 --> $ 148500\n",
      "house # 1659 --> $ 140000\n",
      "house # 1660 --> $ 153000\n",
      "house # 1661 --> $ 305000\n",
      "house # 1662 --> $ 335000\n",
      "house # 1663 --> $ 315000\n",
      "house # 1664 --> $ 395192\n",
      "house # 1665 --> $ 255500\n",
      "house # 1666 --> $ 306000\n",
      "house # 1667 --> $ 335000\n",
      "house # 1668 --> $ 337500\n",
      "house # 1669 --> $ 320000\n",
      "house # 1670 --> $ 320000\n",
      "house # 1671 --> $ 203000\n",
      "house # 1672 --> $ 501837\n",
      "house # 1673 --> $ 325000\n",
      "house # 1674 --> $ 255500\n",
      "house # 1675 --> $ 230000\n",
      "house # 1676 --> $ 207500\n",
      "house # 1677 --> $ 207500\n",
      "house # 1678 --> $ 395192\n",
      "house # 1679 --> $ 402861\n",
      "house # 1680 --> $ 319900\n",
      "house # 1681 --> $ 192500\n",
      "house # 1682 --> $ 297000\n",
      "house # 1683 --> $ 160200\n",
      "house # 1684 --> $ 179000\n",
      "house # 1685 --> $ 179000\n",
      "house # 1686 --> $ 160200\n",
      "house # 1687 --> $ 188500\n",
      "house # 1688 --> $ 179000\n",
      "house # 1689 --> $ 179000\n",
      "house # 1690 --> $ 244000\n",
      "house # 1691 --> $ 184100\n",
      "house # 1692 --> $ 244000\n",
      "house # 1693 --> $ 167500\n",
      "house # 1694 --> $ 188500\n",
      "house # 1695 --> $ 174000\n",
      "house # 1696 --> $ 350000\n",
      "house # 1697 --> $ 178000\n",
      "house # 1698 --> $ 305000\n",
      "house # 1699 --> $ 213250\n",
      "house # 1700 --> $ 213000\n",
      "house # 1701 --> $ 261500\n",
      "house # 1702 --> $ 201000\n",
      "house # 1703 --> $ 372500\n",
      "house # 1704 --> $ 306000\n",
      "house # 1705 --> $ 190000\n",
      "house # 1706 --> $ 377500\n",
      "house # 1707 --> $ 233170\n",
      "house # 1708 --> $ 185500\n",
      "house # 1709 --> $ 255500\n",
      "house # 1710 --> $ 264132\n",
      "house # 1711 --> $ 335000\n",
      "house # 1712 --> $ 213000\n",
      "house # 1713 --> $ 280000\n",
      "house # 1714 --> $ 229456\n",
      "house # 1715 --> $ 173500\n",
      "house # 1716 --> $ 155000\n",
      "house # 1717 --> $ 159000\n",
      "house # 1718 --> $ 93500\n",
      "house # 1719 --> $ 197900\n",
      "house # 1720 --> $ 194500\n",
      "house # 1721 --> $ 145000\n",
      "house # 1722 --> $ 93500\n",
      "house # 1723 --> $ 184900\n",
      "house # 1724 --> $ 245350\n",
      "house # 1725 --> $ 214900\n",
      "house # 1726 --> $ 185000\n",
      "house # 1727 --> $ 153900\n",
      "house # 1728 --> $ 227000\n",
      "house # 1729 --> $ 200500\n",
      "house # 1730 --> $ 235000\n",
      "house # 1731 --> $ 123000\n",
      "house # 1732 --> $ 139000\n",
      "house # 1733 --> $ 100000\n",
      "house # 1734 --> $ 100000\n",
      "house # 1735 --> $ 136500\n",
      "house # 1736 --> $ 117500\n",
      "house # 1737 --> $ 196000\n",
      "house # 1738 --> $ 175000\n",
      "house # 1739 --> $ 250000\n",
      "house # 1740 --> $ 208900\n",
      "house # 1741 --> $ 274000\n",
      "house # 1742 --> $ 172500\n",
      "house # 1743 --> $ 172500\n",
      "house # 1744 --> $ 222000\n",
      "house # 1745 --> $ 145000\n",
      "house # 1746 --> $ 174000\n",
      "house # 1747 --> $ 206900\n",
      "house # 1748 --> $ 178000\n",
      "house # 1749 --> $ 167900\n",
      "house # 1750 --> $ 130000\n",
      "house # 1751 --> $ 201800\n",
      "house # 1752 --> $ 117500\n",
      "house # 1753 --> $ 167000\n",
      "house # 1754 --> $ 193500\n",
      "house # 1755 --> $ 167900\n",
      "house # 1756 --> $ 180500\n",
      "house # 1757 --> $ 127500\n",
      "house # 1758 --> $ 142000\n",
      "house # 1759 --> $ 139600\n",
      "house # 1760 --> $ 197500\n",
      "house # 1761 --> $ 166000\n",
      "house # 1762 --> $ 155000\n",
      "house # 1763 --> $ 125000\n",
      "house # 1764 --> $ 128000\n",
      "house # 1765 --> $ 139000\n",
      "house # 1766 --> $ 167500\n",
      "house # 1767 --> $ 257500\n",
      "house # 1768 --> $ 124500\n",
      "house # 1769 --> $ 143000\n",
      "house # 1770 --> $ 136500\n",
      "house # 1771 --> $ 117500\n",
      "house # 1772 --> $ 117500\n",
      "house # 1773 --> $ 62383\n",
      "house # 1774 --> $ 135000\n",
      "house # 1775 --> $ 129500\n",
      "house # 1776 --> $ 112000\n",
      "house # 1777 --> $ 113000\n",
      "house # 1778 --> $ 109000\n",
      "house # 1779 --> $ 66500\n",
      "house # 1780 --> $ 227000\n",
      "house # 1781 --> $ 128900\n",
      "house # 1782 --> $ 109500\n",
      "house # 1783 --> $ 128000\n",
      "house # 1784 --> $ 133000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "house # 1785 --> $ 100000\n",
      "house # 1786 --> $ 115000\n",
      "house # 1787 --> $ 128000\n",
      "house # 1788 --> $ 98000\n",
      "house # 1789 --> $ 105500\n",
      "house # 1790 --> $ 105000\n",
      "house # 1791 --> $ 200000\n",
      "house # 1792 --> $ 167000\n",
      "house # 1793 --> $ 142000\n",
      "house # 1794 --> $ 163500\n",
      "house # 1795 --> $ 139000\n",
      "house # 1796 --> $ 125000\n",
      "house # 1797 --> $ 100000\n",
      "house # 1798 --> $ 139000\n",
      "house # 1799 --> $ 113000\n",
      "house # 1800 --> $ 132500\n",
      "house # 1801 --> $ 133000\n",
      "house # 1802 --> $ 130000\n",
      "house # 1803 --> $ 154000\n",
      "house # 1804 --> $ 137000\n",
      "house # 1805 --> $ 139000\n",
      "house # 1806 --> $ 133000\n",
      "house # 1807 --> $ 124500\n",
      "house # 1808 --> $ 140000\n",
      "house # 1809 --> $ 94000\n",
      "house # 1810 --> $ 174500\n",
      "house # 1811 --> $ 55000\n",
      "house # 1812 --> $ 79000\n",
      "house # 1813 --> $ 110000\n",
      "house # 1814 --> $ 100000\n",
      "house # 1815 --> $ 82000\n",
      "house # 1816 --> $ 102776\n",
      "house # 1817 --> $ 68400\n",
      "house # 1818 --> $ 184000\n",
      "house # 1819 --> $ 117000\n",
      "house # 1820 --> $ 37900\n",
      "house # 1821 --> $ 133000\n",
      "house # 1822 --> $ 110000\n",
      "house # 1823 --> $ 67000\n",
      "house # 1824 --> $ 121000\n",
      "house # 1825 --> $ 169000\n",
      "house # 1826 --> $ 78000\n",
      "house # 1827 --> $ 119000\n",
      "house # 1828 --> $ 132500\n",
      "house # 1829 --> $ 81000\n",
      "house # 1830 --> $ 140000\n",
      "house # 1831 --> $ 128000\n",
      "house # 1832 --> $ 118000\n",
      "house # 1833 --> $ 139000\n",
      "house # 1834 --> $ 119000\n",
      "house # 1835 --> $ 135900\n",
      "house # 1836 --> $ 78000\n",
      "house # 1837 --> $ 109500\n",
      "house # 1838 --> $ 125000\n",
      "house # 1839 --> $ 158500\n",
      "house # 1840 --> $ 179000\n",
      "house # 1841 --> $ 173000\n",
      "house # 1842 --> $ 98000\n",
      "house # 1843 --> $ 157000\n",
      "house # 1844 --> $ 148000\n",
      "house # 1845 --> $ 139500\n",
      "house # 1846 --> $ 180000\n",
      "house # 1847 --> $ 158000\n",
      "house # 1848 --> $ 80000\n",
      "house # 1849 --> $ 98300\n",
      "house # 1850 --> $ 129500\n",
      "house # 1851 --> $ 174500\n",
      "house # 1852 --> $ 140000\n",
      "house # 1853 --> $ 162000\n",
      "house # 1854 --> $ 137000\n",
      "house # 1855 --> $ 154000\n",
      "house # 1856 --> $ 213000\n",
      "house # 1857 --> $ 157500\n",
      "house # 1858 --> $ 142953\n",
      "house # 1859 --> $ 118858\n",
      "house # 1860 --> $ 153337\n",
      "house # 1861 --> $ 118858\n",
      "house # 1862 --> $ 305000\n",
      "house # 1863 --> $ 305000\n",
      "house # 1864 --> $ 305000\n",
      "house # 1865 --> $ 402861\n",
      "house # 1866 --> $ 297000\n",
      "house # 1867 --> $ 232000\n",
      "house # 1868 --> $ 280000\n",
      "house # 1869 --> $ 260000\n",
      "house # 1870 --> $ 194500\n",
      "house # 1871 --> $ 287090\n",
      "house # 1872 --> $ 179000\n",
      "house # 1873 --> $ 188000\n",
      "house # 1874 --> $ 152000\n",
      "house # 1875 --> $ 185000\n",
      "house # 1876 --> $ 215000\n",
      "house # 1877 --> $ 194500\n",
      "house # 1878 --> $ 240000\n",
      "house # 1879 --> $ 100000\n",
      "house # 1880 --> $ 140000\n",
      "house # 1881 --> $ 297000\n",
      "house # 1882 --> $ 236000\n",
      "house # 1883 --> $ 173000\n",
      "house # 1884 --> $ 190000\n",
      "house # 1885 --> $ 260000\n",
      "house # 1886 --> $ 203000\n",
      "house # 1887 --> $ 203000\n",
      "house # 1888 --> $ 274900\n",
      "house # 1889 --> $ 179000\n",
      "house # 1890 --> $ 139000\n",
      "house # 1891 --> $ 148000\n",
      "house # 1892 --> $ 139000\n",
      "house # 1893 --> $ 124500\n",
      "house # 1894 --> $ 118000\n",
      "house # 1895 --> $ 132000\n",
      "house # 1896 --> $ 177500\n",
      "house # 1897 --> $ 160000\n",
      "house # 1898 --> $ 154900\n",
      "house # 1899 --> $ 147000\n",
      "house # 1900 --> $ 161000\n",
      "house # 1901 --> $ 140000\n",
      "house # 1902 --> $ 82000\n",
      "house # 1903 --> $ 176000\n",
      "house # 1904 --> $ 123000\n",
      "house # 1905 --> $ 161500\n",
      "house # 1906 --> $ 161000\n",
      "house # 1907 --> $ 163000\n",
      "house # 1908 --> $ 124000\n",
      "house # 1909 --> $ 124000\n",
      "house # 1910 --> $ 124000\n",
      "house # 1911 --> $ 212000\n",
      "house # 1912 --> $ 335000\n",
      "house # 1913 --> $ 191000\n",
      "house # 1914 --> $ 68500\n",
      "house # 1915 --> $ 392500\n",
      "house # 1916 --> $ 60000\n",
      "house # 1917 --> $ 239500\n",
      "house # 1918 --> $ 131400\n",
      "house # 1919 --> $ 245500\n",
      "house # 1920 --> $ 187100\n",
      "house # 1921 --> $ 337500\n",
      "house # 1922 --> $ 372402\n",
      "house # 1923 --> $ 185000\n",
      "house # 1924 --> $ 212000\n",
      "house # 1925 --> $ 190000\n",
      "house # 1926 --> $ 337500\n",
      "house # 1927 --> $ 139000\n",
      "house # 1928 --> $ 157000\n",
      "house # 1929 --> $ 117500\n",
      "house # 1930 --> $ 133900\n",
      "house # 1931 --> $ 137000\n",
      "house # 1932 --> $ 159500\n",
      "house # 1933 --> $ 155000\n",
      "house # 1934 --> $ 174000\n",
      "house # 1935 --> $ 194500\n",
      "house # 1936 --> $ 187000\n",
      "house # 1937 --> $ 194500\n",
      "house # 1938 --> $ 214000\n",
      "house # 1939 --> $ 286000\n",
      "house # 1940 --> $ 196000\n",
      "house # 1941 --> $ 181000\n",
      "house # 1942 --> $ 224900\n",
      "house # 1943 --> $ 328900\n",
      "house # 1944 --> $ 325000\n",
      "house # 1945 --> $ 297000\n",
      "house # 1946 --> $ 110000\n",
      "house # 1947 --> $ 207000\n",
      "house # 1948 --> $ 126500\n",
      "house # 1949 --> $ 203000\n",
      "house # 1950 --> $ 173000\n",
      "house # 1951 --> $ 301500\n",
      "house # 1952 --> $ 171500\n",
      "house # 1953 --> $ 184000\n",
      "house # 1954 --> $ 192000\n",
      "house # 1955 --> $ 154000\n",
      "house # 1956 --> $ 228000\n",
      "house # 1957 --> $ 174000\n",
      "house # 1958 --> $ 341000\n",
      "house # 1959 --> $ 185000\n",
      "house # 1960 --> $ 109008\n",
      "house # 1961 --> $ 112500\n",
      "house # 1962 --> $ 118000\n",
      "house # 1963 --> $ 113000\n",
      "house # 1964 --> $ 106000\n",
      "house # 1965 --> $ 106000\n",
      "house # 1966 --> $ 143750\n",
      "house # 1967 --> $ 372402\n",
      "house # 1968 --> $ 350000\n",
      "house # 1969 --> $ 315000\n",
      "house # 1970 --> $ 426000\n",
      "house # 1971 --> $ 337000\n",
      "house # 1972 --> $ 426000\n",
      "house # 1973 --> $ 325000\n",
      "house # 1974 --> $ 284000\n",
      "house # 1975 --> $ 611657\n",
      "house # 1976 --> $ 293077\n",
      "house # 1977 --> $ 315000\n",
      "house # 1978 --> $ 325000\n",
      "house # 1979 --> $ 318061\n",
      "house # 1980 --> $ 191000\n",
      "house # 1981 --> $ 277500\n",
      "house # 1982 --> $ 207000\n",
      "house # 1983 --> $ 191000\n",
      "house # 1984 --> $ 179400\n",
      "house # 1985 --> $ 207000\n",
      "house # 1986 --> $ 207500\n",
      "house # 1987 --> $ 159895\n",
      "house # 1988 --> $ 181000\n",
      "house # 1989 --> $ 165400\n",
      "house # 1990 --> $ 236000\n",
      "house # 1991 --> $ 254000\n",
      "house # 1992 --> $ 250000\n",
      "house # 1993 --> $ 168000\n",
      "house # 1994 --> $ 236000\n",
      "house # 1995 --> $ 181000\n",
      "house # 1996 --> $ 250000\n",
      "house # 1997 --> $ 228500\n",
      "house # 1998 --> $ 440000\n",
      "house # 1999 --> $ 305000\n",
      "house # 2000 --> $ 325000\n",
      "house # 2001 --> $ 284000\n",
      "house # 2002 --> $ 293077\n",
      "house # 2003 --> $ 232000\n",
      "house # 2004 --> $ 339750\n",
      "house # 2005 --> $ 200141\n"
     ]
    }
   ],
   "source": [
    "# 1.\n",
    "# Show nr of test and train houses\n",
    "nr_test_houses = all_input_feats_test.shape[0]\n",
    "print(\"There are\", nr_test_houses,\n",
    "      \"for which I will predict sale prices.\")\n",
    "nr_train_houses = all_input_feats_train.shape[0]\n",
    "print(\"There are\", nr_train_houses,\n",
    "      \"train data houses.\")\n",
    "\n",
    "# 2.\n",
    "# Define a function to measure the\n",
    "# distance between two feature vectors\n",
    "def get_feature_vec_distance(v,w):\n",
    "    \n",
    "    return np.linalg.norm(v-w)\n",
    "    \n",
    "\n",
    "# 3.\n",
    "# Loop over all 1459 test houses\n",
    "preds_test_houses_dollar = np.zeros((nr_test_houses,1))\n",
    "for test_house_nr in range(0,nr_test_houses):\n",
    "    \n",
    "    # 3.1 get the feature vector of the test house\n",
    "    \n",
    "    # Use numerical + categorial feature vector\n",
    "    #v = all_input_feats_test[test_house_nr,:]\n",
    "    \n",
    "    # Just use numerical feature vector\n",
    "    v = normalized_test_input_matrix_feats[test_house_nr,:]\n",
    "    \n",
    "    \n",
    "    # 3.2 compare v with all 1460 train houses\n",
    "    min_dist = -1.0\n",
    "    predicted_sale_price = -1.0\n",
    "    for train_house_nr in range(0,nr_train_houses):\n",
    "        \n",
    "        # get the feature vector of the train house\n",
    "        \n",
    "        # Use numerical + categorial feature vector\n",
    "        #w = all_input_feats_train[train_house_nr,:]\n",
    "        \n",
    "        # Just use numerical feature vector\n",
    "        w = normalized_train_input_matrix_feats[train_house_nr,:]\n",
    "        \n",
    "        # compare vector v and w\n",
    "        distance = get_feature_vec_distance(v,w)\n",
    "        \n",
    "        # found a vector w that is more similar to v?\n",
    "        if (train_house_nr==0 or distance<min_dist):\n",
    "            min_dist = distance\n",
    "            predicted_sale_price =\\\n",
    "                train_output_matrix[train_house_nr][0]\n",
    "                \n",
    "    # 3.3 show predicted sale price for current test house\n",
    "    print(\"house #\", test_house_ids[test_house_nr],\n",
    "          \"--> $\", predicted_sale_price)\n",
    "    \n",
    "    # 3.4 store the predicted house price\n",
    "    preds_test_houses_dollar[test_house_nr][0] = predicted_sale_price\n",
    "    \n",
    "    \n",
    "# 4.\n",
    "\n",
    "# For a Pandas data frame column the predicted\n",
    "# house sale price matrix has to be 1-dimensional\n",
    "preds_test_houses_dollar = preds_test_houses_dollar.reshape(-1)\n",
    "\n",
    "# Create a .csv file    \n",
    "predition_dataframe = pd.DataFrame({'Id'       :test_house_ids,\n",
    "                                    'SalePrice':preds_test_houses_dollar}\n",
    "                                  )\n",
    "# convert column \"Id\" to int64 dtype\n",
    "predition_dataframe = predition_dataframe.astype({\"Id\": int})\n",
    "print(predition_dataframe)\n",
    "# now save the Pandas dataframe to a .csv file\n",
    "PREDICTION_FILENAME = \"nn_predictions.csv\"\n",
    "predition_dataframe.to_csv(PREDICTION_FILENAME, sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good to see, that here the Nearest Neighbour regression approaches that uses (numerical + categorial features) or just (numerical feature) vectors gets a worse prediction.\n",
    "\n",
    "E.g. using numerical features the Nearest Neighbour regression aproach gave a score of 0.22729, while I got a score of 0.14177 with the MLP approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
