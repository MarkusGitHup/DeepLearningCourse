{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\">Multi-Layer Perceptron (MLP) example in Keras <br> for house sales price prediction<br>using BOTH numerical and categorial features\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    " by Prof. Dr.-Ing. Jürgen Brauer, http://www.juergenbrauer.org\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the data and deal with \"NaN\" values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>0</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>0</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>0</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>0</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>0</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave     0      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave     0      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave     0      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave     0      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave     0      IR1   \n",
       "\n",
       "  LandContour Utilities    ...     PoolArea PoolQC Fence MiscFeature MiscVal  \\\n",
       "0         Lvl    AllPub    ...            0      0     0           0       0   \n",
       "1         Lvl    AllPub    ...            0      0     0           0       0   \n",
       "2         Lvl    AllPub    ...            0      0     0           0       0   \n",
       "3         Lvl    AllPub    ...            0      0     0           0       0   \n",
       "4         Lvl    AllPub    ...            0      0     0           0       0   \n",
       "\n",
       "  MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0      2   2008        WD         Normal     208500  \n",
       "1      5   2007        WD         Normal     181500  \n",
       "2      9   2008        WD         Normal     223500  \n",
       "3      2   2006        WD        Abnorml     140000  \n",
       "4     12   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv(\"kaggle_dataset_house_prices/train.csv\")\n",
    "test_data  = pd.read_csv(\"kaggle_dataset_house_prices/test.csv\")\n",
    "\n",
    "train_data.fillna(0, inplace=True)\n",
    "test_data.fillna(0, inplace=True)\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "np.set_printoptions(suppress=True) # do not use scientific \"e\"-notation\n",
    "\n",
    "# 1.\n",
    "# prepare Pandas DataFrames only with numerical columns\n",
    "train_data_num_only = train_data.select_dtypes(exclude=['object'])\n",
    "test_data_num_only  = test_data.select_dtypes(exclude=['object'])\n",
    "\n",
    "# 2.\n",
    "# Throw away \"Id\" and SalePrice\" column for training data\n",
    "train_input_matrix = train_data_num_only.values[:,1:37]\n",
    "train_output_matrix = train_data_num_only.values[:,37]\n",
    "train_output_matrix = train_output_matrix.reshape(-1,1)\n",
    "\n",
    "# 3.\n",
    "# Throw away \"Id\" column for test input matrix\n",
    "test_input_matrix  = test_data_num_only.values[:,1:]\n",
    "\n",
    "# 4.\n",
    "# create a MinMaxScaler with feature range [0,1]\n",
    "# and use it to normalize the train_input_matrix\n",
    "# Then use the SAME normalization to normalize test_data_matrix\n",
    "scaler_input_features = MinMaxScaler(feature_range=(0, 1))\n",
    "normalized_train_input_matrix_feats = scaler_input_features.fit_transform(train_input_matrix)\n",
    "normalized_test_input_matrix_feats = scaler_input_features.transform(test_input_matrix)\n",
    "\n",
    "# 5.\n",
    "# Also create a MinMaxScaler for the train_output_matrix,\n",
    "# which is essentially a column with the final SalePrice\n",
    "scaler_saleprice = MinMaxScaler(feature_range=(0, 1))\n",
    "normalized_train_output_matrix = scaler_saleprice.fit_transform(train_output_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare categorial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSZoning         object\n",
      "Street           object\n",
      "Alley            object\n",
      "LotShape         object\n",
      "LandContour      object\n",
      "Utilities        object\n",
      "LotConfig        object\n",
      "LandSlope        object\n",
      "Neighborhood     object\n",
      "Condition1       object\n",
      "Condition2       object\n",
      "BldgType         object\n",
      "HouseStyle       object\n",
      "RoofStyle        object\n",
      "RoofMatl         object\n",
      "Exterior1st      object\n",
      "Exterior2nd      object\n",
      "MasVnrType       object\n",
      "ExterQual        object\n",
      "ExterCond        object\n",
      "Foundation       object\n",
      "BsmtQual         object\n",
      "BsmtCond         object\n",
      "BsmtExposure     object\n",
      "BsmtFinType1     object\n",
      "BsmtFinType2     object\n",
      "Heating          object\n",
      "HeatingQC        object\n",
      "CentralAir       object\n",
      "Electrical       object\n",
      "KitchenQual      object\n",
      "Functional       object\n",
      "FireplaceQu      object\n",
      "GarageType       object\n",
      "GarageFinish     object\n",
      "GarageQual       object\n",
      "GarageCond       object\n",
      "PavedDrive       object\n",
      "PoolQC           object\n",
      "Fence            object\n",
      "MiscFeature      object\n",
      "SaleType         object\n",
      "SaleCondition    object\n",
      "dtype: object\n",
      "Shape of fused_df is (2919, 43)\n",
      "Shape of fused_df_hot_encoded is (2919, 275)\n"
     ]
    }
   ],
   "source": [
    "# 1.\n",
    "# save the house Ids, since we need them later\n",
    "# for our Kaggle submission\n",
    "test_house_ids = test_data.values[:,0] # get the IDs from the original Pandas DataFrame\n",
    "\n",
    "\n",
    "# 2.\n",
    "# get Pandas data frames without the numerical features\n",
    "gt_saleprice = train_data[\"SalePrice\"]\n",
    "train_data_cats_only = train_data.select_dtypes(exclude=['number'])\n",
    "test_data_cats_only = test_data.select_dtypes(exclude=['number'])\n",
    "\n",
    "\n",
    "# 3.\n",
    "# map each single categorial column\n",
    "# to multiple (one-hot encoded) columns\n",
    "print(train_data_cats_only.dtypes)\n",
    "frames = [train_data_cats_only, test_data_cats_only]\n",
    "fused_df = pd.concat(frames)\n",
    "print(\"Shape of fused_df is\", fused_df.shape)\n",
    "\n",
    "# now do the one-hot encoding\n",
    "fused_df_hot_encoded = pd.get_dummies(fused_df)\n",
    "print(\"Shape of fused_df_hot_encoded is\", fused_df_hot_encoded.shape)\n",
    "\n",
    "# now split the data frame into two data frames again\n",
    "# with 1460 and 1459 rows\n",
    "train_data_hot_encoded = fused_df_hot_encoded[0:1460]\n",
    "test_data_hot_encoded  = fused_df_hot_encoded[1460:]\n",
    "\n",
    "\n",
    "# 4.\n",
    "# prepare NumPy matrices for training\n",
    "# from Pandas DataFrames\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# define how NumPy shall print matrices\n",
    "np.set_printoptions(precision=4)\n",
    "np.set_printoptions(suppress=True) # do not use scientific \"e\"-notation\n",
    "\n",
    "# convert Pandas DataFrame to NumPy matrices\n",
    "# since Keras will expect NumPy matrices\n",
    "train_input_matrix_cat = train_data_hot_encoded.values\n",
    "train_output_matrix    = gt_saleprice.values\n",
    "train_output_matrix    = train_output_matrix.reshape(-1,1)\n",
    "test_input_matrix_cat  = test_data_hot_encoded.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine numerical and categorial feature columns\n",
    "\n",
    "We have now a training matrix with numerical features and a training matrix with categorial one-hot encoded features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 36 many numerical features that will be used.\n",
      "\tnormalized_train_input_matrix_feats has shape (1460, 36)\n",
      "\tnormalized_test_input_matrix_feats has shape (1459, 36)\n",
      "There are 275 many categorial features that will be used.\n",
      "\ttrain_input_matrix_cat has shape (1460, 275)\n",
      "\ttrain_input_matrix_cat has shape (1460, 275)\n",
      "all_input_feats_train has shape (1460, 311)\n",
      "all_input_feats_test has shape (1459, 311)\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\", normalized_train_input_matrix_feats.shape[1],\n",
    "      \"many numerical features that will be used.\")\n",
    "print(\"\\tnormalized_train_input_matrix_feats has shape\",\n",
    "      normalized_train_input_matrix_feats.shape)\n",
    "print(\"\\tnormalized_test_input_matrix_feats has shape\",\n",
    "      normalized_test_input_matrix_feats.shape)\n",
    "\n",
    "print(\"There are\", train_input_matrix_cat.shape[1],\n",
    "      \"many categorial features that will be used.\")\n",
    "print(\"\\ttrain_input_matrix_cat has shape\",\n",
    "      train_input_matrix_cat.shape)\n",
    "print(\"\\ttrain_input_matrix_cat has shape\",\n",
    "      train_input_matrix_cat.shape)\n",
    "\n",
    "# concatenate the two input matrices horizontally\n",
    "all_input_feats_train = np.hstack((normalized_train_input_matrix_feats,\n",
    "                                   train_input_matrix_cat))\n",
    "print(\"all_input_feats_train has shape\", all_input_feats_train.shape)\n",
    "all_input_feats_test = np.hstack((normalized_test_input_matrix_feats ,\n",
    "                                   test_input_matrix_cat))\n",
    "print(\"all_input_feats_test has shape\", all_input_feats_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and train MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input X has shape (1460, 311)\n",
      "Desired output Y has shape (1460, 1)\n",
      "Y:\n",
      " [[0.2411]\n",
      " [0.2036]\n",
      " [0.2619]\n",
      " ...\n",
      " [0.3216]\n",
      " [0.1489]\n",
      " [0.1564]]\n",
      "Train on 1095 samples, validate on 365 samples\n",
      "Epoch 1/500\n",
      "1095/1095 [==============================] - 0s 149us/step - loss: 0.0460 - val_loss: 0.0234\n",
      "Epoch 2/500\n",
      "1095/1095 [==============================] - 0s 69us/step - loss: 0.0205 - val_loss: 0.0187\n",
      "Epoch 3/500\n",
      "1095/1095 [==============================] - 0s 68us/step - loss: 0.0162 - val_loss: 0.0156\n",
      "Epoch 4/500\n",
      "1095/1095 [==============================] - 0s 72us/step - loss: 0.0138 - val_loss: 0.0143\n",
      "Epoch 5/500\n",
      "1095/1095 [==============================] - 0s 71us/step - loss: 0.0120 - val_loss: 0.0129\n",
      "Epoch 6/500\n",
      "1095/1095 [==============================] - 0s 69us/step - loss: 0.0109 - val_loss: 0.0118\n",
      "Epoch 7/500\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 0.0101 - val_loss: 0.0112\n",
      "Epoch 8/500\n",
      "1095/1095 [==============================] - 0s 68us/step - loss: 0.0093 - val_loss: 0.0111\n",
      "Epoch 9/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 0.0087 - val_loss: 0.0102\n",
      "Epoch 10/500\n",
      "1095/1095 [==============================] - 0s 65us/step - loss: 0.0083 - val_loss: 0.0098\n",
      "Epoch 11/500\n",
      "1095/1095 [==============================] - 0s 72us/step - loss: 0.0079 - val_loss: 0.0095\n",
      "Epoch 12/500\n",
      "1095/1095 [==============================] - 0s 78us/step - loss: 0.0075 - val_loss: 0.0093\n",
      "Epoch 13/500\n",
      "1095/1095 [==============================] - 0s 75us/step - loss: 0.0072 - val_loss: 0.0090\n",
      "Epoch 14/500\n",
      "1095/1095 [==============================] - 0s 65us/step - loss: 0.0070 - val_loss: 0.0087\n",
      "Epoch 15/500\n",
      "1095/1095 [==============================] - 0s 65us/step - loss: 0.0067 - val_loss: 0.0085\n",
      "Epoch 16/500\n",
      "1095/1095 [==============================] - 0s 65us/step - loss: 0.0065 - val_loss: 0.0084\n",
      "Epoch 17/500\n",
      "1095/1095 [==============================] - 0s 80us/step - loss: 0.0063 - val_loss: 0.0086\n",
      "Epoch 18/500\n",
      "1095/1095 [==============================] - 0s 81us/step - loss: 0.0062 - val_loss: 0.0081\n",
      "Epoch 19/500\n",
      "1095/1095 [==============================] - 0s 85us/step - loss: 0.0060 - val_loss: 0.0079\n",
      "Epoch 20/500\n",
      "1095/1095 [==============================] - 0s 71us/step - loss: 0.0058 - val_loss: 0.0078\n",
      "Epoch 21/500\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 0.0057 - val_loss: 0.0077\n",
      "Epoch 22/500\n",
      "1095/1095 [==============================] - 0s 70us/step - loss: 0.0055 - val_loss: 0.0076\n",
      "Epoch 23/500\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 0.0054 - val_loss: 0.0077\n",
      "Epoch 24/500\n",
      "1095/1095 [==============================] - 0s 68us/step - loss: 0.0053 - val_loss: 0.0074\n",
      "Epoch 25/500\n",
      "1095/1095 [==============================] - 0s 69us/step - loss: 0.0052 - val_loss: 0.0074\n",
      "Epoch 26/500\n",
      "1095/1095 [==============================] - 0s 73us/step - loss: 0.0051 - val_loss: 0.0072\n",
      "Epoch 27/500\n",
      "1095/1095 [==============================] - 0s 73us/step - loss: 0.0050 - val_loss: 0.0071\n",
      "Epoch 28/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 0.0049 - val_loss: 0.0071\n",
      "Epoch 29/500\n",
      "1095/1095 [==============================] - 0s 71us/step - loss: 0.0048 - val_loss: 0.0070\n",
      "Epoch 30/500\n",
      "1095/1095 [==============================] - 0s 60us/step - loss: 0.0047 - val_loss: 0.0070\n",
      "Epoch 31/500\n",
      "1095/1095 [==============================] - 0s 77us/step - loss: 0.0046 - val_loss: 0.0068\n",
      "Epoch 32/500\n",
      "1095/1095 [==============================] - 0s 76us/step - loss: 0.0046 - val_loss: 0.0067\n",
      "Epoch 33/500\n",
      "1095/1095 [==============================] - 0s 71us/step - loss: 0.0045 - val_loss: 0.0066\n",
      "Epoch 34/500\n",
      "1095/1095 [==============================] - 0s 77us/step - loss: 0.0044 - val_loss: 0.0066\n",
      "Epoch 35/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 0.0043 - val_loss: 0.0065\n",
      "Epoch 36/500\n",
      "1095/1095 [==============================] - 0s 66us/step - loss: 0.0043 - val_loss: 0.0065\n",
      "Epoch 37/500\n",
      "1095/1095 [==============================] - 0s 66us/step - loss: 0.0042 - val_loss: 0.0064\n",
      "Epoch 38/500\n",
      "1095/1095 [==============================] - 0s 77us/step - loss: 0.0041 - val_loss: 0.0064\n",
      "Epoch 39/500\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 0.0041 - val_loss: 0.0063\n",
      "Epoch 40/500\n",
      "1095/1095 [==============================] - 0s 86us/step - loss: 0.0040 - val_loss: 0.0063\n",
      "Epoch 41/500\n",
      "1095/1095 [==============================] - 0s 75us/step - loss: 0.0040 - val_loss: 0.0062\n",
      "Epoch 42/500\n",
      "1095/1095 [==============================] - 0s 72us/step - loss: 0.0039 - val_loss: 0.0062\n",
      "Epoch 43/500\n",
      "1095/1095 [==============================] - 0s 75us/step - loss: 0.0039 - val_loss: 0.0061\n",
      "Epoch 44/500\n",
      "1095/1095 [==============================] - 0s 72us/step - loss: 0.0038 - val_loss: 0.0062\n",
      "Epoch 45/500\n",
      "1095/1095 [==============================] - 0s 74us/step - loss: 0.0038 - val_loss: 0.0060\n",
      "Epoch 46/500\n",
      "1095/1095 [==============================] - 0s 79us/step - loss: 0.0037 - val_loss: 0.0061\n",
      "Epoch 47/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 0.0037 - val_loss: 0.0061\n",
      "Epoch 48/500\n",
      "1095/1095 [==============================] - 0s 60us/step - loss: 0.0036 - val_loss: 0.0060\n",
      "Epoch 49/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0036 - val_loss: 0.0059\n",
      "Epoch 50/500\n",
      "1095/1095 [==============================] - 0s 57us/step - loss: 0.0036 - val_loss: 0.0060\n",
      "Epoch 51/500\n",
      "1095/1095 [==============================] - 0s 77us/step - loss: 0.0035 - val_loss: 0.0060\n",
      "Epoch 52/500\n",
      "1095/1095 [==============================] - 0s 71us/step - loss: 0.0035 - val_loss: 0.0059\n",
      "Epoch 53/500\n",
      "1095/1095 [==============================] - 0s 70us/step - loss: 0.0034 - val_loss: 0.0058\n",
      "Epoch 54/500\n",
      "1095/1095 [==============================] - 0s 66us/step - loss: 0.0034 - val_loss: 0.0057\n",
      "Epoch 55/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 0.0034 - val_loss: 0.0062\n",
      "Epoch 56/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 0.0033 - val_loss: 0.0057\n",
      "Epoch 57/500\n",
      "1095/1095 [==============================] - 0s 67us/step - loss: 0.0033 - val_loss: 0.0057\n",
      "Epoch 58/500\n",
      "1095/1095 [==============================] - 0s 60us/step - loss: 0.0032 - val_loss: 0.0059\n",
      "Epoch 59/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0032 - val_loss: 0.0056\n",
      "Epoch 60/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0032 - val_loss: 0.0056\n",
      "Epoch 61/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0032 - val_loss: 0.0056\n",
      "Epoch 62/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0032 - val_loss: 0.0055\n",
      "Epoch 63/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0031 - val_loss: 0.0056\n",
      "Epoch 64/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0031 - val_loss: 0.0055\n",
      "Epoch 65/500\n",
      "1095/1095 [==============================] - 0s 55us/step - loss: 0.0031 - val_loss: 0.0055\n",
      "Epoch 66/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0030 - val_loss: 0.0055\n",
      "Epoch 67/500\n",
      "1095/1095 [==============================] - 0s 66us/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 68/500\n",
      "1095/1095 [==============================] - 0s 76us/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 69/500\n",
      "1095/1095 [==============================] - 0s 76us/step - loss: 0.0029 - val_loss: 0.0054\n",
      "Epoch 70/500\n",
      "1095/1095 [==============================] - 0s 65us/step - loss: 0.0029 - val_loss: 0.0054\n",
      "Epoch 71/500\n",
      "1095/1095 [==============================] - 0s 68us/step - loss: 0.0029 - val_loss: 0.0054\n",
      "Epoch 72/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 0.0029 - val_loss: 0.0054\n",
      "Epoch 73/500\n",
      "1095/1095 [==============================] - 0s 60us/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 74/500\n",
      "1095/1095 [==============================] - 0s 60us/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 75/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 0.0028 - val_loss: 0.0053\n",
      "Epoch 76/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 77/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1095/1095 [==============================] - 0s 66us/step - loss: 0.0027 - val_loss: 0.0055\n",
      "Epoch 78/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 0.0027 - val_loss: 0.0052\n",
      "Epoch 79/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0027 - val_loss: 0.0053\n",
      "Epoch 80/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0027 - val_loss: 0.0052\n",
      "Epoch 81/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0027 - val_loss: 0.0054\n",
      "Epoch 82/500\n",
      "1095/1095 [==============================] - 0s 68us/step - loss: 0.0026 - val_loss: 0.0052\n",
      "Epoch 83/500\n",
      "1095/1095 [==============================] - 0s 55us/step - loss: 0.0026 - val_loss: 0.0052\n",
      "Epoch 84/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0026 - val_loss: 0.0052\n",
      "Epoch 85/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 0.0026 - val_loss: 0.0053\n",
      "Epoch 86/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0025 - val_loss: 0.0051\n",
      "Epoch 87/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0025 - val_loss: 0.0052\n",
      "Epoch 88/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 0.0025 - val_loss: 0.0051\n",
      "Epoch 89/500\n",
      "1095/1095 [==============================] - 0s 79us/step - loss: 0.0025 - val_loss: 0.0051\n",
      "Epoch 90/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0025 - val_loss: 0.0051\n",
      "Epoch 91/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0025 - val_loss: 0.0051\n",
      "Epoch 92/500\n",
      "1095/1095 [==============================] - 0s 70us/step - loss: 0.0025 - val_loss: 0.0051\n",
      "Epoch 93/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0024 - val_loss: 0.0052\n",
      "Epoch 94/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0024 - val_loss: 0.0051\n",
      "Epoch 95/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0024 - val_loss: 0.0050\n",
      "Epoch 96/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0024 - val_loss: 0.0050\n",
      "Epoch 97/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0024 - val_loss: 0.0051\n",
      "Epoch 98/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0024 - val_loss: 0.0050\n",
      "Epoch 99/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0023 - val_loss: 0.0050\n",
      "Epoch 100/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0023 - val_loss: 0.0050\n",
      "Epoch 101/500\n",
      "1095/1095 [==============================] - 0s 70us/step - loss: 0.0023 - val_loss: 0.0050\n",
      "Epoch 102/500\n",
      "1095/1095 [==============================] - 0s 75us/step - loss: 0.0023 - val_loss: 0.0049\n",
      "Epoch 103/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0023 - val_loss: 0.0049\n",
      "Epoch 104/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0023 - val_loss: 0.0050\n",
      "Epoch 105/500\n",
      "1095/1095 [==============================] - 0s 73us/step - loss: 0.0022 - val_loss: 0.0049\n",
      "Epoch 106/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 0.0022 - val_loss: 0.0049\n",
      "Epoch 107/500\n",
      "1095/1095 [==============================] - 0s 55us/step - loss: 0.0022 - val_loss: 0.0049\n",
      "Epoch 108/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0022 - val_loss: 0.0049\n",
      "Epoch 109/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0022 - val_loss: 0.0049\n",
      "Epoch 110/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0022 - val_loss: 0.0049\n",
      "Epoch 111/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0022 - val_loss: 0.0049\n",
      "Epoch 112/500\n",
      "1095/1095 [==============================] - 0s 68us/step - loss: 0.0022 - val_loss: 0.0048\n",
      "Epoch 113/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0021 - val_loss: 0.0048\n",
      "Epoch 114/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0021 - val_loss: 0.0048\n",
      "Epoch 115/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0021 - val_loss: 0.0048\n",
      "Epoch 116/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0021 - val_loss: 0.0049\n",
      "Epoch 117/500\n",
      "1095/1095 [==============================] - 0s 73us/step - loss: 0.0021 - val_loss: 0.0048\n",
      "Epoch 118/500\n",
      "1095/1095 [==============================] - 0s 78us/step - loss: 0.0021 - val_loss: 0.0048\n",
      "Epoch 119/500\n",
      "1095/1095 [==============================] - 0s 58us/step - loss: 0.0021 - val_loss: 0.0048\n",
      "Epoch 120/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0021 - val_loss: 0.0048\n",
      "Epoch 121/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 0.0021 - val_loss: 0.0048\n",
      "Epoch 122/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 0.0020 - val_loss: 0.0049\n",
      "Epoch 123/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0020 - val_loss: 0.0048\n",
      "Epoch 124/500\n",
      "1095/1095 [==============================] - 0s 60us/step - loss: 0.0020 - val_loss: 0.0047\n",
      "Epoch 125/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0020 - val_loss: 0.0048\n",
      "Epoch 126/500\n",
      "1095/1095 [==============================] - 0s 80us/step - loss: 0.0020 - val_loss: 0.0047\n",
      "Epoch 127/500\n",
      "1095/1095 [==============================] - 0s 73us/step - loss: 0.0020 - val_loss: 0.0047\n",
      "Epoch 128/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 0.0020 - val_loss: 0.0047\n",
      "Epoch 129/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0020 - val_loss: 0.0047\n",
      "Epoch 130/500\n",
      "1095/1095 [==============================] - 0s 71us/step - loss: 0.0020 - val_loss: 0.0047\n",
      "Epoch 131/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0019 - val_loss: 0.0047\n",
      "Epoch 132/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0019 - val_loss: 0.0047\n",
      "Epoch 133/500\n",
      "1095/1095 [==============================] - 0s 96us/step - loss: 0.0019 - val_loss: 0.0047\n",
      "Epoch 134/500\n",
      "1095/1095 [==============================] - 0s 73us/step - loss: 0.0019 - val_loss: 0.0049\n",
      "Epoch 135/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0019 - val_loss: 0.0047\n",
      "Epoch 136/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0019 - val_loss: 0.0047\n",
      "Epoch 137/500\n",
      "1095/1095 [==============================] - 0s 58us/step - loss: 0.0019 - val_loss: 0.0047\n",
      "Epoch 138/500\n",
      "1095/1095 [==============================] - 0s 74us/step - loss: 0.0019 - val_loss: 0.0047\n",
      "Epoch 139/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0019 - val_loss: 0.0046\n",
      "Epoch 140/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0019 - val_loss: 0.0046\n",
      "Epoch 141/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0019 - val_loss: 0.0046\n",
      "Epoch 142/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 0.0018 - val_loss: 0.0046\n",
      "Epoch 143/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0018 - val_loss: 0.0046\n",
      "Epoch 144/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0018 - val_loss: 0.0047\n",
      "Epoch 145/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0018 - val_loss: 0.0046\n",
      "Epoch 146/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0018 - val_loss: 0.0046\n",
      "Epoch 147/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0018 - val_loss: 0.0046\n",
      "Epoch 148/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0018 - val_loss: 0.0046\n",
      "Epoch 149/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0018 - val_loss: 0.0046\n",
      "Epoch 150/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0018 - val_loss: 0.0046\n",
      "Epoch 151/500\n",
      "1095/1095 [==============================] - 0s 73us/step - loss: 0.0018 - val_loss: 0.0046\n",
      "Epoch 152/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0018 - val_loss: 0.0045\n",
      "Epoch 153/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0018 - val_loss: 0.0046\n",
      "Epoch 154/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0017 - val_loss: 0.0046\n",
      "Epoch 155/500\n",
      "1095/1095 [==============================] - 0s 82us/step - loss: 0.0017 - val_loss: 0.0045\n",
      "Epoch 156/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0017 - val_loss: 0.0046\n",
      "Epoch 157/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0017 - val_loss: 0.0045\n",
      "Epoch 158/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0017 - val_loss: 0.0045\n",
      "Epoch 159/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0017 - val_loss: 0.0046\n",
      "Epoch 160/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0017 - val_loss: 0.0045\n",
      "Epoch 161/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0017 - val_loss: 0.0045\n",
      "Epoch 162/500\n",
      "1095/1095 [==============================] - 0s 67us/step - loss: 0.0017 - val_loss: 0.0046\n",
      "Epoch 163/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0017 - val_loss: 0.0045\n",
      "Epoch 164/500\n",
      "1095/1095 [==============================] - 0s 82us/step - loss: 0.0017 - val_loss: 0.0045\n",
      "Epoch 165/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0017 - val_loss: 0.0045\n",
      "Epoch 166/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0017 - val_loss: 0.0045\n",
      "Epoch 167/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0016 - val_loss: 0.0045\n",
      "Epoch 168/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0016 - val_loss: 0.0045\n",
      "Epoch 169/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0016 - val_loss: 0.0045\n",
      "Epoch 170/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0016 - val_loss: 0.0045\n",
      "Epoch 171/500\n",
      "1095/1095 [==============================] - 0s 73us/step - loss: 0.0016 - val_loss: 0.0045\n",
      "Epoch 172/500\n",
      "1095/1095 [==============================] - 0s 70us/step - loss: 0.0016 - val_loss: 0.0045\n",
      "Epoch 173/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 0.0016 - val_loss: 0.0044\n",
      "Epoch 174/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0016 - val_loss: 0.0045\n",
      "Epoch 175/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 0.0016 - val_loss: 0.0045\n",
      "Epoch 176/500\n",
      "1095/1095 [==============================] - 0s 65us/step - loss: 0.0016 - val_loss: 0.0046\n",
      "Epoch 177/500\n",
      "1095/1095 [==============================] - 0s 66us/step - loss: 0.0016 - val_loss: 0.0045\n",
      "Epoch 178/500\n",
      "1095/1095 [==============================] - 0s 68us/step - loss: 0.0016 - val_loss: 0.0044\n",
      "Epoch 179/500\n",
      "1095/1095 [==============================] - 0s 58us/step - loss: 0.0016 - val_loss: 0.0045\n",
      "Epoch 180/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0016 - val_loss: 0.0046\n",
      "Epoch 181/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0016 - val_loss: 0.0044\n",
      "Epoch 182/500\n",
      "1095/1095 [==============================] - 0s 69us/step - loss: 0.0015 - val_loss: 0.0044\n",
      "Epoch 183/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0015 - val_loss: 0.0044\n",
      "Epoch 184/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 0.0015 - val_loss: 0.0044\n",
      "Epoch 185/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0015 - val_loss: 0.0045\n",
      "Epoch 186/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0015 - val_loss: 0.0044\n",
      "Epoch 187/500\n",
      "1095/1095 [==============================] - 0s 72us/step - loss: 0.0015 - val_loss: 0.0045\n",
      "Epoch 188/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0015 - val_loss: 0.0044\n",
      "Epoch 189/500\n",
      "1095/1095 [==============================] - 0s 69us/step - loss: 0.0015 - val_loss: 0.0044\n",
      "Epoch 190/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0015 - val_loss: 0.0044\n",
      "Epoch 191/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0015 - val_loss: 0.0044\n",
      "Epoch 192/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0015 - val_loss: 0.0044\n",
      "Epoch 193/500\n",
      "1095/1095 [==============================] - 0s 71us/step - loss: 0.0015 - val_loss: 0.0044\n",
      "Epoch 194/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0015 - val_loss: 0.0044\n",
      "Epoch 195/500\n",
      "1095/1095 [==============================] - 0s 72us/step - loss: 0.0015 - val_loss: 0.0044\n",
      "Epoch 196/500\n",
      "1095/1095 [==============================] - 0s 72us/step - loss: 0.0015 - val_loss: 0.0044\n",
      "Epoch 197/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0015 - val_loss: 0.0044\n",
      "Epoch 198/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0015 - val_loss: 0.0044\n",
      "Epoch 199/500\n",
      "1095/1095 [==============================] - 0s 96us/step - loss: 0.0014 - val_loss: 0.0044\n",
      "Epoch 200/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0014 - val_loss: 0.0044\n",
      "Epoch 201/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0014 - val_loss: 0.0043\n",
      "Epoch 202/500\n",
      "1095/1095 [==============================] - 0s 72us/step - loss: 0.0014 - val_loss: 0.0043\n",
      "Epoch 203/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0014 - val_loss: 0.0044\n",
      "Epoch 204/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0014 - val_loss: 0.0044\n",
      "Epoch 205/500\n",
      "1095/1095 [==============================] - 0s 73us/step - loss: 0.0014 - val_loss: 0.0044\n",
      "Epoch 206/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0014 - val_loss: 0.0043\n",
      "Epoch 207/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0014 - val_loss: 0.0043\n",
      "Epoch 208/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0014 - val_loss: 0.0044\n",
      "Epoch 209/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0014 - val_loss: 0.0043\n",
      "Epoch 210/500\n",
      "1095/1095 [==============================] - 0s 74us/step - loss: 0.0014 - val_loss: 0.0043\n",
      "Epoch 211/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0014 - val_loss: 0.0043\n",
      "Epoch 212/500\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 0.0014 - val_loss: 0.0044\n",
      "Epoch 213/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0014 - val_loss: 0.0043\n",
      "Epoch 214/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0014 - val_loss: 0.0043\n",
      "Epoch 215/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0014 - val_loss: 0.0043\n",
      "Epoch 216/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0014 - val_loss: 0.0043\n",
      "Epoch 217/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0014 - val_loss: 0.0043\n",
      "Epoch 218/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0014 - val_loss: 0.0044\n",
      "Epoch 219/500\n",
      "1095/1095 [==============================] - 0s 90us/step - loss: 0.0014 - val_loss: 0.0043\n",
      "Epoch 220/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0013 - val_loss: 0.0043\n",
      "Epoch 221/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0013 - val_loss: 0.0043\n",
      "Epoch 222/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0013 - val_loss: 0.0043\n",
      "Epoch 223/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0013 - val_loss: 0.0043\n",
      "Epoch 224/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0013 - val_loss: 0.0043\n",
      "Epoch 225/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0013 - val_loss: 0.0043\n",
      "Epoch 226/500\n",
      "1095/1095 [==============================] - 0s 75us/step - loss: 0.0013 - val_loss: 0.0043\n",
      "Epoch 227/500\n",
      "1095/1095 [==============================] - 0s 74us/step - loss: 0.0013 - val_loss: 0.0043\n",
      "Epoch 228/500\n",
      "1095/1095 [==============================] - 0s 69us/step - loss: 0.0013 - val_loss: 0.0044\n",
      "Epoch 229/500\n",
      "1095/1095 [==============================] - 0s 79us/step - loss: 0.0013 - val_loss: 0.0043\n",
      "Epoch 230/500\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 0.0013 - val_loss: 0.0043\n",
      "Epoch 231/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0013 - val_loss: 0.0043\n",
      "Epoch 232/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0013 - val_loss: 0.0043\n",
      "Epoch 233/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0013 - val_loss: 0.0042\n",
      "Epoch 234/500\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 0.0013 - val_loss: 0.0043\n",
      "Epoch 235/500\n",
      "1095/1095 [==============================] - 0s 67us/step - loss: 0.0013 - val_loss: 0.0042\n",
      "Epoch 236/500\n",
      "1095/1095 [==============================] - 0s 68us/step - loss: 0.0013 - val_loss: 0.0043\n",
      "Epoch 237/500\n",
      "1095/1095 [==============================] - 0s 76us/step - loss: 0.0013 - val_loss: 0.0042\n",
      "Epoch 238/500\n",
      "1095/1095 [==============================] - 0s 76us/step - loss: 0.0013 - val_loss: 0.0042\n",
      "Epoch 239/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 0.0013 - val_loss: 0.0042\n",
      "Epoch 240/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0013 - val_loss: 0.0042\n",
      "Epoch 241/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0012 - val_loss: 0.0042\n",
      "Epoch 242/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0012 - val_loss: 0.0042\n",
      "Epoch 243/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0012 - val_loss: 0.0043\n",
      "Epoch 244/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0012 - val_loss: 0.0042\n",
      "Epoch 245/500\n",
      "1095/1095 [==============================] - 0s 56us/step - loss: 0.0012 - val_loss: 0.0042\n",
      "Epoch 246/500\n",
      "1095/1095 [==============================] - 0s 69us/step - loss: 0.0012 - val_loss: 0.0042\n",
      "Epoch 247/500\n",
      "1095/1095 [==============================] - 0s 72us/step - loss: 0.0012 - val_loss: 0.0042\n",
      "Epoch 248/500\n",
      "1095/1095 [==============================] - 0s 77us/step - loss: 0.0012 - val_loss: 0.0042\n",
      "Epoch 249/500\n",
      "1095/1095 [==============================] - 0s 66us/step - loss: 0.0012 - val_loss: 0.0042\n",
      "Epoch 250/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 0.0012 - val_loss: 0.0042\n",
      "Epoch 251/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0012 - val_loss: 0.0042\n",
      "Epoch 252/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0012 - val_loss: 0.0042\n",
      "Epoch 253/500\n",
      "1095/1095 [==============================] - 0s 46us/step - loss: 0.0012 - val_loss: 0.0042\n",
      "Epoch 254/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0012 - val_loss: 0.0042\n",
      "Epoch 255/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0012 - val_loss: 0.0042\n",
      "Epoch 256/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0012 - val_loss: 0.0042\n",
      "Epoch 257/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 0.0012 - val_loss: 0.0042\n",
      "Epoch 258/500\n",
      "1095/1095 [==============================] - 0s 82us/step - loss: 0.0012 - val_loss: 0.0042\n",
      "Epoch 259/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0012 - val_loss: 0.0042\n",
      "Epoch 260/500\n",
      "1095/1095 [==============================] - 0s 55us/step - loss: 0.0012 - val_loss: 0.0042\n",
      "Epoch 261/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0012 - val_loss: 0.0042\n",
      "Epoch 262/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0012 - val_loss: 0.0042\n",
      "Epoch 263/500\n",
      "1095/1095 [==============================] - 0s 58us/step - loss: 0.0012 - val_loss: 0.0042\n",
      "Epoch 264/500\n",
      "1095/1095 [==============================] - 0s 85us/step - loss: 0.0012 - val_loss: 0.0042\n",
      "Epoch 265/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0012 - val_loss: 0.0042\n",
      "Epoch 266/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0012 - val_loss: 0.0042\n",
      "Epoch 267/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0011 - val_loss: 0.0042\n",
      "Epoch 268/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0011 - val_loss: 0.0042\n",
      "Epoch 269/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0011 - val_loss: 0.0042\n",
      "Epoch 270/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0011 - val_loss: 0.0042\n",
      "Epoch 271/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0011 - val_loss: 0.0042\n",
      "Epoch 272/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0011 - val_loss: 0.0042\n",
      "Epoch 273/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0011 - val_loss: 0.0042\n",
      "Epoch 274/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 0.0011 - val_loss: 0.0042\n",
      "Epoch 275/500\n",
      "1095/1095 [==============================] - 0s 73us/step - loss: 0.0011 - val_loss: 0.0042\n",
      "Epoch 276/500\n",
      "1095/1095 [==============================] - 0s 76us/step - loss: 0.0011 - val_loss: 0.0042\n",
      "Epoch 277/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0011 - val_loss: 0.0041\n",
      "Epoch 278/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0011 - val_loss: 0.0042\n",
      "Epoch 279/500\n",
      "1095/1095 [==============================] - 0s 93us/step - loss: 0.0011 - val_loss: 0.0041\n",
      "Epoch 280/500\n",
      "1095/1095 [==============================] - 0s 58us/step - loss: 0.0011 - val_loss: 0.0041\n",
      "Epoch 281/500\n",
      "1095/1095 [==============================] - 0s 84us/step - loss: 0.0011 - val_loss: 0.0041\n",
      "Epoch 282/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0011 - val_loss: 0.0041\n",
      "Epoch 283/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0011 - val_loss: 0.0042\n",
      "Epoch 284/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 0.0011 - val_loss: 0.0041\n",
      "Epoch 285/500\n",
      "1095/1095 [==============================] - 0s 74us/step - loss: 0.0011 - val_loss: 0.0042\n",
      "Epoch 286/500\n",
      "1095/1095 [==============================] - 0s 68us/step - loss: 0.0011 - val_loss: 0.0041\n",
      "Epoch 287/500\n",
      "1095/1095 [==============================] - 0s 75us/step - loss: 0.0011 - val_loss: 0.0041\n",
      "Epoch 288/500\n",
      "1095/1095 [==============================] - 0s 56us/step - loss: 0.0011 - val_loss: 0.0041\n",
      "Epoch 289/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 0.0011 - val_loss: 0.0043\n",
      "Epoch 290/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0011 - val_loss: 0.0041\n",
      "Epoch 291/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0011 - val_loss: 0.0041\n",
      "Epoch 292/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0011 - val_loss: 0.0041\n",
      "Epoch 293/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0011 - val_loss: 0.0041\n",
      "Epoch 294/500\n",
      "1095/1095 [==============================] - 0s 60us/step - loss: 0.0011 - val_loss: 0.0041\n",
      "Epoch 295/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0011 - val_loss: 0.0042\n",
      "Epoch 296/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0011 - val_loss: 0.0041\n",
      "Epoch 297/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0011 - val_loss: 0.0041\n",
      "Epoch 298/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 0.0010 - val_loss: 0.0041\n",
      "Epoch 299/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0010 - val_loss: 0.0041\n",
      "Epoch 300/500\n",
      "1095/1095 [==============================] - 0s 56us/step - loss: 0.0010 - val_loss: 0.0041\n",
      "Epoch 301/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 0.0010 - val_loss: 0.0041\n",
      "Epoch 302/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 0.0010 - val_loss: 0.0041\n",
      "Epoch 303/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 0.0010 - val_loss: 0.0041\n",
      "Epoch 304/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 0.0010 - val_loss: 0.0041\n",
      "Epoch 305/500\n",
      "1095/1095 [==============================] - 0s 55us/step - loss: 0.0010 - val_loss: 0.0041\n",
      "Epoch 306/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 0.0010 - val_loss: 0.0041\n",
      "Epoch 307/500\n",
      "1095/1095 [==============================] - 0s 68us/step - loss: 0.0010 - val_loss: 0.0043\n",
      "Epoch 308/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1095/1095 [==============================] - 0s 76us/step - loss: 0.0010 - val_loss: 0.0041\n",
      "Epoch 309/500\n",
      "1095/1095 [==============================] - 0s 69us/step - loss: 0.0010 - val_loss: 0.0041\n",
      "Epoch 310/500\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 0.0010 - val_loss: 0.0041\n",
      "Epoch 311/500\n",
      "1095/1095 [==============================] - 0s 58us/step - loss: 0.0010 - val_loss: 0.0041\n",
      "Epoch 312/500\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 0.0010 - val_loss: 0.0041\n",
      "Epoch 313/500\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 0.0010 - val_loss: 0.0042\n",
      "Epoch 314/500\n",
      "1095/1095 [==============================] - 0s 68us/step - loss: 0.0010 - val_loss: 0.0041\n",
      "Epoch 315/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 9.9961e-04 - val_loss: 0.0041\n",
      "Epoch 316/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 9.9550e-04 - val_loss: 0.0041\n",
      "Epoch 317/500\n",
      "1095/1095 [==============================] - 0s 65us/step - loss: 9.9361e-04 - val_loss: 0.0041\n",
      "Epoch 318/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 9.9055e-04 - val_loss: 0.0041\n",
      "Epoch 319/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 9.8983e-04 - val_loss: 0.0041\n",
      "Epoch 320/500\n",
      "1095/1095 [==============================] - 0s 68us/step - loss: 9.8652e-04 - val_loss: 0.0041\n",
      "Epoch 321/500\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 9.8295e-04 - val_loss: 0.0041\n",
      "Epoch 322/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 9.7939e-04 - val_loss: 0.0041\n",
      "Epoch 323/500\n",
      "1095/1095 [==============================] - 0s 65us/step - loss: 9.7681e-04 - val_loss: 0.0041\n",
      "Epoch 324/500\n",
      "1095/1095 [==============================] - 0s 56us/step - loss: 9.7334e-04 - val_loss: 0.0041\n",
      "Epoch 325/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 9.7024e-04 - val_loss: 0.0041\n",
      "Epoch 326/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 9.6880e-04 - val_loss: 0.0041\n",
      "Epoch 327/500\n",
      "1095/1095 [==============================] - 0s 76us/step - loss: 9.6792e-04 - val_loss: 0.0041\n",
      "Epoch 328/500\n",
      "1095/1095 [==============================] - 0s 70us/step - loss: 9.6665e-04 - val_loss: 0.0041\n",
      "Epoch 329/500\n",
      "1095/1095 [==============================] - 0s 96us/step - loss: 9.6099e-04 - val_loss: 0.0041\n",
      "Epoch 330/500\n",
      "1095/1095 [==============================] - 0s 100us/step - loss: 9.5957e-04 - val_loss: 0.0041\n",
      "Epoch 331/500\n",
      "1095/1095 [==============================] - 0s 91us/step - loss: 9.5293e-04 - val_loss: 0.0041\n",
      "Epoch 332/500\n",
      "1095/1095 [==============================] - 0s 69us/step - loss: 9.5156e-04 - val_loss: 0.0041\n",
      "Epoch 333/500\n",
      "1095/1095 [==============================] - 0s 56us/step - loss: 9.5083e-04 - val_loss: 0.0041\n",
      "Epoch 334/500\n",
      "1095/1095 [==============================] - 0s 58us/step - loss: 9.4781e-04 - val_loss: 0.0041\n",
      "Epoch 335/500\n",
      "1095/1095 [==============================] - 0s 86us/step - loss: 9.4766e-04 - val_loss: 0.0041\n",
      "Epoch 336/500\n",
      "1095/1095 [==============================] - 0s 106us/step - loss: 9.4470e-04 - val_loss: 0.0041\n",
      "Epoch 337/500\n",
      "1095/1095 [==============================] - 0s 104us/step - loss: 9.4013e-04 - val_loss: 0.0042\n",
      "Epoch 338/500\n",
      "1095/1095 [==============================] - 0s 111us/step - loss: 9.4200e-04 - val_loss: 0.0041\n",
      "Epoch 339/500\n",
      "1095/1095 [==============================] - 0s 99us/step - loss: 9.3807e-04 - val_loss: 0.0041\n",
      "Epoch 340/500\n",
      "1095/1095 [==============================] - 0s 91us/step - loss: 9.3343e-04 - val_loss: 0.0042\n",
      "Epoch 341/500\n",
      "1095/1095 [==============================] - 0s 75us/step - loss: 9.3249e-04 - val_loss: 0.0041\n",
      "Epoch 342/500\n",
      "1095/1095 [==============================] - 0s 85us/step - loss: 9.3113e-04 - val_loss: 0.0040\n",
      "Epoch 343/500\n",
      "1095/1095 [==============================] - 0s 76us/step - loss: 9.2063e-04 - val_loss: 0.0041\n",
      "Epoch 344/500\n",
      "1095/1095 [==============================] - 0s 87us/step - loss: 9.2544e-04 - val_loss: 0.0041\n",
      "Epoch 345/500\n",
      "1095/1095 [==============================] - 0s 60us/step - loss: 9.2054e-04 - val_loss: 0.0041\n",
      "Epoch 346/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 9.1771e-04 - val_loss: 0.0040\n",
      "Epoch 347/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 9.1850e-04 - val_loss: 0.0041\n",
      "Epoch 348/500\n",
      "1095/1095 [==============================] - 0s 80us/step - loss: 9.1438e-04 - val_loss: 0.0041\n",
      "Epoch 349/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 9.1108e-04 - val_loss: 0.0041\n",
      "Epoch 350/500\n",
      "1095/1095 [==============================] - 0s 77us/step - loss: 9.0752e-04 - val_loss: 0.0041\n",
      "Epoch 351/500\n",
      "1095/1095 [==============================] - 0s 85us/step - loss: 9.0621e-04 - val_loss: 0.0040\n",
      "Epoch 352/500\n",
      "1095/1095 [==============================] - 0s 90us/step - loss: 9.0595e-04 - val_loss: 0.0040\n",
      "Epoch 353/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 8.9866e-04 - val_loss: 0.0040\n",
      "Epoch 354/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 9.0447e-04 - val_loss: 0.0041\n",
      "Epoch 355/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 8.9662e-04 - val_loss: 0.0041\n",
      "Epoch 356/500\n",
      "1095/1095 [==============================] - 0s 80us/step - loss: 8.9674e-04 - val_loss: 0.0041\n",
      "Epoch 357/500\n",
      "1095/1095 [==============================] - 0s 78us/step - loss: 8.9276e-04 - val_loss: 0.0040\n",
      "Epoch 358/500\n",
      "1095/1095 [==============================] - 0s 80us/step - loss: 8.9081e-04 - val_loss: 0.0041\n",
      "Epoch 359/500\n",
      "1095/1095 [==============================] - 0s 66us/step - loss: 8.9245e-04 - val_loss: 0.0041\n",
      "Epoch 360/500\n",
      "1095/1095 [==============================] - 0s 90us/step - loss: 8.8851e-04 - val_loss: 0.0041\n",
      "Epoch 361/500\n",
      "1095/1095 [==============================] - 0s 66us/step - loss: 8.7968e-04 - val_loss: 0.0040\n",
      "Epoch 362/500\n",
      "1095/1095 [==============================] - 0s 80us/step - loss: 8.8114e-04 - val_loss: 0.0041\n",
      "Epoch 363/500\n",
      "1095/1095 [==============================] - 0s 101us/step - loss: 8.7874e-04 - val_loss: 0.0041\n",
      "Epoch 364/500\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 8.7839e-04 - val_loss: 0.0041\n",
      "Epoch 365/500\n",
      "1095/1095 [==============================] - 0s 86us/step - loss: 8.7642e-04 - val_loss: 0.0040\n",
      "Epoch 366/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 8.7253e-04 - val_loss: 0.0041\n",
      "Epoch 367/500\n",
      "1095/1095 [==============================] - 0s 70us/step - loss: 8.7084e-04 - val_loss: 0.0040\n",
      "Epoch 368/500\n",
      "1095/1095 [==============================] - 0s 76us/step - loss: 8.7047e-04 - val_loss: 0.0040\n",
      "Epoch 369/500\n",
      "1095/1095 [==============================] - 0s 89us/step - loss: 8.6517e-04 - val_loss: 0.0040\n",
      "Epoch 370/500\n",
      "1095/1095 [==============================] - 0s 88us/step - loss: 8.6558e-04 - val_loss: 0.0040\n",
      "Epoch 371/500\n",
      "1095/1095 [==============================] - 0s 69us/step - loss: 8.6252e-04 - val_loss: 0.0041\n",
      "Epoch 372/500\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 8.6696e-04 - val_loss: 0.0041\n",
      "Epoch 373/500\n",
      "1095/1095 [==============================] - 0s 78us/step - loss: 8.6000e-04 - val_loss: 0.0040\n",
      "Epoch 374/500\n",
      "1095/1095 [==============================] - 0s 90us/step - loss: 8.5743e-04 - val_loss: 0.0041\n",
      "Epoch 375/500\n",
      "1095/1095 [==============================] - 0s 94us/step - loss: 8.5431e-04 - val_loss: 0.0040\n",
      "Epoch 376/500\n",
      "1095/1095 [==============================] - 0s 69us/step - loss: 8.5416e-04 - val_loss: 0.0040\n",
      "Epoch 377/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 8.4978e-04 - val_loss: 0.0041\n",
      "Epoch 378/500\n",
      "1095/1095 [==============================] - 0s 68us/step - loss: 8.4898e-04 - val_loss: 0.0041\n",
      "Epoch 379/500\n",
      "1095/1095 [==============================] - 0s 69us/step - loss: 8.4529e-04 - val_loss: 0.0040\n",
      "Epoch 380/500\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 8.4291e-04 - val_loss: 0.0041\n",
      "Epoch 381/500\n",
      "1095/1095 [==============================] - 0s 74us/step - loss: 8.3871e-04 - val_loss: 0.0040\n",
      "Epoch 382/500\n",
      "1095/1095 [==============================] - 0s 57us/step - loss: 8.4120e-04 - val_loss: 0.0040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 383/500\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 8.3533e-04 - val_loss: 0.0041\n",
      "Epoch 384/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 8.3318e-04 - val_loss: 0.0040\n",
      "Epoch 385/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 8.3597e-04 - val_loss: 0.0040\n",
      "Epoch 386/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 8.2910e-04 - val_loss: 0.0040\n",
      "Epoch 387/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 8.2913e-04 - val_loss: 0.0040\n",
      "Epoch 388/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 8.2703e-04 - val_loss: 0.0040\n",
      "Epoch 389/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 8.2518e-04 - val_loss: 0.0040\n",
      "Epoch 390/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 8.2583e-04 - val_loss: 0.0040\n",
      "Epoch 391/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 8.2108e-04 - val_loss: 0.0040\n",
      "Epoch 392/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 8.1748e-04 - val_loss: 0.0040\n",
      "Epoch 393/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 8.1710e-04 - val_loss: 0.0040\n",
      "Epoch 394/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 8.1533e-04 - val_loss: 0.0040\n",
      "Epoch 395/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 8.1325e-04 - val_loss: 0.0040\n",
      "Epoch 396/500\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 8.1011e-04 - val_loss: 0.0040\n",
      "Epoch 397/500\n",
      "1095/1095 [==============================] - 0s 69us/step - loss: 8.0594e-04 - val_loss: 0.0041\n",
      "Epoch 398/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 8.0923e-04 - val_loss: 0.0041\n",
      "Epoch 399/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 8.0689e-04 - val_loss: 0.0040\n",
      "Epoch 400/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 8.0285e-04 - val_loss: 0.0040\n",
      "Epoch 401/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 8.0573e-04 - val_loss: 0.0040\n",
      "Epoch 402/500\n",
      "1095/1095 [==============================] - 0s 74us/step - loss: 7.9528e-04 - val_loss: 0.0040\n",
      "Epoch 403/500\n",
      "1095/1095 [==============================] - 0s 68us/step - loss: 7.9876e-04 - val_loss: 0.0040\n",
      "Epoch 404/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 7.9531e-04 - val_loss: 0.0040\n",
      "Epoch 405/500\n",
      "1095/1095 [==============================] - 0s 75us/step - loss: 7.9298e-04 - val_loss: 0.0040\n",
      "Epoch 406/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 7.9168e-04 - val_loss: 0.0040\n",
      "Epoch 407/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 7.8830e-04 - val_loss: 0.0040\n",
      "Epoch 408/500\n",
      "1095/1095 [==============================] - 0s 69us/step - loss: 7.8800e-04 - val_loss: 0.0040\n",
      "Epoch 409/500\n",
      "1095/1095 [==============================] - 0s 85us/step - loss: 7.8671e-04 - val_loss: 0.0040\n",
      "Epoch 410/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 7.8588e-04 - val_loss: 0.0040\n",
      "Epoch 411/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 7.8171e-04 - val_loss: 0.0040\n",
      "Epoch 412/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 7.8303e-04 - val_loss: 0.0040\n",
      "Epoch 413/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 7.7802e-04 - val_loss: 0.0040\n",
      "Epoch 414/500\n",
      "1095/1095 [==============================] - 0s 56us/step - loss: 7.7774e-04 - val_loss: 0.0040\n",
      "Epoch 415/500\n",
      "1095/1095 [==============================] - 0s 57us/step - loss: 7.7562e-04 - val_loss: 0.0040\n",
      "Epoch 416/500\n",
      "1095/1095 [==============================] - 0s 60us/step - loss: 7.7335e-04 - val_loss: 0.0040\n",
      "Epoch 417/500\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 7.7436e-04 - val_loss: 0.0040\n",
      "Epoch 418/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 7.6866e-04 - val_loss: 0.0041\n",
      "Epoch 419/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 7.7229e-04 - val_loss: 0.0040\n",
      "Epoch 420/500\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 7.6649e-04 - val_loss: 0.0040\n",
      "Epoch 421/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 7.6558e-04 - val_loss: 0.0040\n",
      "Epoch 422/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 7.6435e-04 - val_loss: 0.0040\n",
      "Epoch 423/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 7.6161e-04 - val_loss: 0.0040\n",
      "Epoch 424/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 7.6074e-04 - val_loss: 0.0040\n",
      "Epoch 425/500\n",
      "1095/1095 [==============================] - 0s 57us/step - loss: 7.5818e-04 - val_loss: 0.0040\n",
      "Epoch 426/500\n",
      "1095/1095 [==============================] - 0s 68us/step - loss: 7.5466e-04 - val_loss: 0.0040\n",
      "Epoch 427/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 7.5473e-04 - val_loss: 0.0040\n",
      "Epoch 428/500\n",
      "1095/1095 [==============================] - 0s 56us/step - loss: 7.5409e-04 - val_loss: 0.0040\n",
      "Epoch 429/500\n",
      "1095/1095 [==============================] - 0s 73us/step - loss: 7.5242e-04 - val_loss: 0.0040\n",
      "Epoch 430/500\n",
      "1095/1095 [==============================] - 0s 90us/step - loss: 7.5203e-04 - val_loss: 0.0040\n",
      "Epoch 431/500\n",
      "1095/1095 [==============================] - 0s 66us/step - loss: 7.4813e-04 - val_loss: 0.0040\n",
      "Epoch 432/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 7.4850e-04 - val_loss: 0.0040\n",
      "Epoch 433/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 7.4261e-04 - val_loss: 0.0040\n",
      "Epoch 434/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 7.4234e-04 - val_loss: 0.0040\n",
      "Epoch 435/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 7.4295e-04 - val_loss: 0.0040\n",
      "Epoch 436/500\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 7.4029e-04 - val_loss: 0.0040\n",
      "Epoch 437/500\n",
      "1095/1095 [==============================] - 0s 71us/step - loss: 7.3798e-04 - val_loss: 0.0040\n",
      "Epoch 438/500\n",
      "1095/1095 [==============================] - 0s 65us/step - loss: 7.3817e-04 - val_loss: 0.0040\n",
      "Epoch 439/500\n",
      "1095/1095 [==============================] - 0s 74us/step - loss: 7.3320e-04 - val_loss: 0.0040\n",
      "Epoch 440/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 7.3185e-04 - val_loss: 0.0040\n",
      "Epoch 441/500\n",
      "1095/1095 [==============================] - 0s 55us/step - loss: 7.3294e-04 - val_loss: 0.0040\n",
      "Epoch 442/500\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 7.3219e-04 - val_loss: 0.0040\n",
      "Epoch 443/500\n",
      "1095/1095 [==============================] - 0s 57us/step - loss: 7.2872e-04 - val_loss: 0.0040\n",
      "Epoch 444/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 7.2663e-04 - val_loss: 0.0040\n",
      "Epoch 445/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 7.2842e-04 - val_loss: 0.0040\n",
      "Epoch 446/500\n",
      "1095/1095 [==============================] - 0s 58us/step - loss: 7.2597e-04 - val_loss: 0.0040\n",
      "Epoch 447/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 7.2287e-04 - val_loss: 0.0040\n",
      "Epoch 448/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 7.2206e-04 - val_loss: 0.0040\n",
      "Epoch 449/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 7.2258e-04 - val_loss: 0.0040\n",
      "Epoch 450/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 7.1664e-04 - val_loss: 0.0040\n",
      "Epoch 451/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 7.1800e-04 - val_loss: 0.0040\n",
      "Epoch 452/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 7.1377e-04 - val_loss: 0.0040\n",
      "Epoch 453/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 7.1471e-04 - val_loss: 0.0040\n",
      "Epoch 454/500\n",
      "1095/1095 [==============================] - 0s 87us/step - loss: 7.1057e-04 - val_loss: 0.0040\n",
      "Epoch 455/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 7.1151e-04 - val_loss: 0.0040\n",
      "Epoch 456/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 7.1023e-04 - val_loss: 0.0040\n",
      "Epoch 457/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1095/1095 [==============================] - 0s 51us/step - loss: 7.0757e-04 - val_loss: 0.0040\n",
      "Epoch 458/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 7.0594e-04 - val_loss: 0.0040\n",
      "Epoch 459/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 7.0082e-04 - val_loss: 0.0040\n",
      "Epoch 460/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 7.0201e-04 - val_loss: 0.0040\n",
      "Epoch 461/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 7.0137e-04 - val_loss: 0.0040\n",
      "Epoch 462/500\n",
      "1095/1095 [==============================] - 0s 54us/step - loss: 6.9991e-04 - val_loss: 0.0040\n",
      "Epoch 463/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 6.9717e-04 - val_loss: 0.0040\n",
      "Epoch 464/500\n",
      "1095/1095 [==============================] - 0s 69us/step - loss: 6.9494e-04 - val_loss: 0.0040\n",
      "Epoch 465/500\n",
      "1095/1095 [==============================] - 0s 67us/step - loss: 6.9536e-04 - val_loss: 0.0041\n",
      "Epoch 466/500\n",
      "1095/1095 [==============================] - 0s 73us/step - loss: 6.9837e-04 - val_loss: 0.0041\n",
      "Epoch 467/500\n",
      "1095/1095 [==============================] - 0s 67us/step - loss: 6.9175e-04 - val_loss: 0.0040\n",
      "Epoch 468/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 6.8934e-04 - val_loss: 0.0040\n",
      "Epoch 469/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 6.8786e-04 - val_loss: 0.0040\n",
      "Epoch 470/500\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 6.8862e-04 - val_loss: 0.0040\n",
      "Epoch 471/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 6.8513e-04 - val_loss: 0.0040\n",
      "Epoch 472/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 6.8122e-04 - val_loss: 0.0040\n",
      "Epoch 473/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 6.7968e-04 - val_loss: 0.0040\n",
      "Epoch 474/500\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 6.8409e-04 - val_loss: 0.0039\n",
      "Epoch 475/500\n",
      "1095/1095 [==============================] - 0s 55us/step - loss: 6.8169e-04 - val_loss: 0.0040\n",
      "Epoch 476/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 6.7648e-04 - val_loss: 0.0040\n",
      "Epoch 477/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 6.7702e-04 - val_loss: 0.0040\n",
      "Epoch 478/500\n",
      "1095/1095 [==============================] - 0s 66us/step - loss: 6.7829e-04 - val_loss: 0.0040\n",
      "Epoch 479/500\n",
      "1095/1095 [==============================] - 0s 77us/step - loss: 6.7488e-04 - val_loss: 0.0040\n",
      "Epoch 480/500\n",
      "1095/1095 [==============================] - 0s 55us/step - loss: 6.7171e-04 - val_loss: 0.0040\n",
      "Epoch 481/500\n",
      "1095/1095 [==============================] - 0s 60us/step - loss: 6.7128e-04 - val_loss: 0.0040\n",
      "Epoch 482/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 6.6828e-04 - val_loss: 0.0040\n",
      "Epoch 483/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 6.6795e-04 - val_loss: 0.0040\n",
      "Epoch 484/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 6.6591e-04 - val_loss: 0.0040\n",
      "Epoch 485/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 6.6536e-04 - val_loss: 0.0040\n",
      "Epoch 486/500\n",
      "1095/1095 [==============================] - 0s 52us/step - loss: 6.6495e-04 - val_loss: 0.0040\n",
      "Epoch 487/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 6.6001e-04 - val_loss: 0.0040\n",
      "Epoch 488/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 6.6169e-04 - val_loss: 0.0040\n",
      "Epoch 489/500\n",
      "1095/1095 [==============================] - 0s 60us/step - loss: 6.5897e-04 - val_loss: 0.0040\n",
      "Epoch 490/500\n",
      "1095/1095 [==============================] - 0s 79us/step - loss: 6.5946e-04 - val_loss: 0.0040\n",
      "Epoch 491/500\n",
      "1095/1095 [==============================] - 0s 51us/step - loss: 6.5897e-04 - val_loss: 0.0040\n",
      "Epoch 492/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 6.5809e-04 - val_loss: 0.0040\n",
      "Epoch 493/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 6.5234e-04 - val_loss: 0.0040\n",
      "Epoch 494/500\n",
      "1095/1095 [==============================] - 0s 49us/step - loss: 6.5310e-04 - val_loss: 0.0039\n",
      "Epoch 495/500\n",
      "1095/1095 [==============================] - 0s 50us/step - loss: 6.5092e-04 - val_loss: 0.0040\n",
      "Epoch 496/500\n",
      "1095/1095 [==============================] - 0s 53us/step - loss: 6.4919e-04 - val_loss: 0.0040\n",
      "Epoch 497/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 6.4820e-04 - val_loss: 0.0040\n",
      "Epoch 498/500\n",
      "1095/1095 [==============================] - 0s 74us/step - loss: 6.4694e-04 - val_loss: 0.0040\n",
      "Epoch 499/500\n",
      "1095/1095 [==============================] - 0s 48us/step - loss: 6.4552e-04 - val_loss: 0.0040\n",
      "Epoch 500/500\n",
      "1095/1095 [==============================] - 0s 47us/step - loss: 6.4268e-04 - val_loss: 0.0040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x252c41c39b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers.core import Dense, Dropout\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(160, activation=\"relu\"))\n",
    "model.add(Dense(80, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "\n",
    "X = all_input_feats_train\n",
    "Y = normalized_train_output_matrix\n",
    "print(\"Input X has shape\", X.shape)\n",
    "print(\"Desired output Y has shape\", Y.shape)\n",
    "print(\"Y:\\n\", Y)\n",
    "model.fit(X,Y, validation_split=0.25, epochs=500)\n",
    "#model.fit(X,Y, epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the trained MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds_train_houses:\n",
      " [[0.2423]\n",
      " [0.2114]\n",
      " [0.253 ]\n",
      " ...\n",
      " [0.2793]\n",
      " [0.2213]\n",
      " [0.2024]]\n",
      "preds_train_houses_dollar:\n",
      " [[209375.94]\n",
      " [187104.12]\n",
      " [217066.92]\n",
      " ...\n",
      " [236010.22]\n",
      " [194243.67]\n",
      " [180641.25]]\n",
      "Shape of preds_train_houses is (1460, 1)\n",
      "Shape of preds_train_houses_dollar is (1460, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAooAAAJQCAYAAAAJ0UXFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlBNFMzMzMyv1/6wwP8YKLItEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x252c4a5e438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "preds_train_houses = model.predict(all_input_feats_train)\n",
    "print(\"preds_train_houses:\\n\", preds_train_houses)\n",
    "preds_train_houses_dollar = scaler_saleprice.inverse_transform(preds_train_houses)\n",
    "print(\"preds_train_houses_dollar:\\n\", preds_train_houses_dollar)\n",
    "print(\"Shape of preds_train_houses is\", preds_train_houses.shape)\n",
    "print(\"Shape of preds_train_houses_dollar is\", preds_train_houses_dollar.shape)\n",
    "plt.figure( figsize=(10,10) )\n",
    "plt.plot(train_output_matrix, preds_train_houses_dollar, 'ro')\n",
    "plt.xlabel('Real house price', fontsize = 10)\n",
    "plt.ylabel('Predicted house price', fontsize = 10)\n",
    "plt.grid(True)\n",
    "plt.xlim(0,900000)\n",
    "plt.ylim(0,900000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting house prices for the Kaggle competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_house_ids has shape (1459,)\n",
      "preds_test_houses_dollar has shape (1459,)\n",
      "        Id      SalePrice\n",
      "0     1461  109957.828125\n",
      "1     1462  144082.906250\n",
      "2     1463  234615.765625\n",
      "3     1464  224013.843750\n",
      "4     1465  193984.140625\n",
      "5     1466  207130.468750\n",
      "6     1467  186883.484375\n",
      "7     1468  161307.515625\n",
      "8     1469  157833.125000\n",
      "9     1470  167598.593750\n",
      "10    1471  195199.781250\n",
      "11    1472   89974.460938\n",
      "12    1473  106834.273438\n",
      "13    1474  140639.687500\n",
      "14    1475  143669.296875\n",
      "15    1476  367585.187500\n",
      "16    1477  181497.328125\n",
      "17    1478  249759.890625\n",
      "18    1479  350281.000000\n",
      "19    1480  521254.906250\n",
      "20    1481  364770.062500\n",
      "21    1482  163822.234375\n",
      "22    1483  147752.968750\n",
      "23    1484  169166.343750\n",
      "24    1485  145594.625000\n",
      "25    1486  170220.203125\n",
      "26    1487  285415.250000\n",
      "27    1488  243512.515625\n",
      "28    1489  177572.468750\n",
      "29    1490  247905.046875\n",
      "...    ...            ...\n",
      "1429  2890   81050.734375\n",
      "1430  2891  141085.296875\n",
      "1431  2892   45323.332031\n",
      "1432  2893   18964.207031\n",
      "1433  2894   78030.890625\n",
      "1434  2895  364000.562500\n",
      "1435  2896  274219.250000\n",
      "1436  2897  194193.046875\n",
      "1437  2898  130935.109375\n",
      "1438  2899  192226.796875\n",
      "1439  2900  180559.203125\n",
      "1440  2901  250108.218750\n",
      "1441  2902  163953.734375\n",
      "1442  2903  303992.218750\n",
      "1443  2904  382304.343750\n",
      "1444  2905  150235.218750\n",
      "1445  2906  136941.390625\n",
      "1446  2907  121489.554688\n",
      "1447  2908  145247.703125\n",
      "1448  2909  170868.828125\n",
      "1449  2910   46814.996094\n",
      "1450  2911   88926.007812\n",
      "1451  2912  188262.843750\n",
      "1452  2913   51724.132812\n",
      "1453  2914   36996.046875\n",
      "1454  2915   82513.953125\n",
      "1455  2916   66621.593750\n",
      "1456  2917  166763.750000\n",
      "1457  2918   44492.636719\n",
      "1458  2919  193564.265625\n",
      "\n",
      "[1459 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# PREDICT house prices for all the test houses!\n",
    "preds_test_houses = model.predict(all_input_feats_test)\n",
    "preds_test_houses_dollar = scaler_saleprice.inverse_transform(preds_test_houses)\n",
    "\n",
    "\n",
    "# generate a Pandas dataframe\n",
    "# from the NumPy prediction_matrix\n",
    "preds_test_houses_dollar = preds_test_houses_dollar.reshape(-1)\n",
    "print(\"test_house_ids has shape\", test_house_ids.shape)\n",
    "print(\"preds_test_houses_dollar has shape\", preds_test_houses_dollar.shape)\n",
    "predition_dataframe = pd.DataFrame({'Id'       :test_house_ids,\n",
    "                                    'SalePrice':preds_test_houses_dollar}\n",
    "                                  )\n",
    "\n",
    "# convert column \"Id\" to int64 dtype\n",
    "predition_dataframe = predition_dataframe.astype({\"Id\": int})\n",
    "print(predition_dataframe)\n",
    "\n",
    "# now save the Pandas dataframe to a .csv file\n",
    "PREDICTION_FILENAME = \"my_predicted_house_prices.csv\"\n",
    "predition_dataframe.to_csv(PREDICTION_FILENAME, sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Reference algorithm: Nearest Neighbour Regression\n",
    "\n",
    "A valid question is: How good is the MLP approach?\n",
    "\n",
    "For this, we will follow a straightforward approach here: a new 311D feature vector v will be compared with all the 1460 existing 311D vectors w of the training dataset, we determine the \"most similar\" one wbest and take the house price of wbest as the predicted house price for v.\n",
    "\n",
    "This is a simple approach a real estate agent could follow:\n",
    "\n",
    "\"You want to know for what you can sell your hourse approximately? Ok, let's look in my database of houses I already sold. Your house is most similar to this one. And this house was sold for $X.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1459 for which I will predict sale prices.\n",
      "There are 1460 train data houses.\n",
      "house # 1461 --> $ 129000\n",
      "house # 1462 --> $ 158000\n",
      "house # 1463 --> $ 180000\n",
      "house # 1464 --> $ 178000\n",
      "house # 1465 --> $ 192000\n",
      "house # 1466 --> $ 189000\n",
      "house # 1467 --> $ 206000\n",
      "house # 1468 --> $ 189000\n",
      "house # 1469 --> $ 197500\n",
      "house # 1470 --> $ 97000\n",
      "house # 1471 --> $ 224000\n",
      "house # 1472 --> $ 88000\n",
      "house # 1473 --> $ 88000\n",
      "house # 1474 --> $ 146000\n",
      "house # 1475 --> $ 99500\n",
      "house # 1476 --> $ 252000\n",
      "house # 1477 --> $ 198900\n",
      "house # 1478 --> $ 256300\n",
      "house # 1479 --> $ 378500\n",
      "house # 1480 --> $ 611657\n",
      "house # 1481 --> $ 236000\n",
      "house # 1482 --> $ 192500\n",
      "house # 1483 --> $ 186000\n",
      "house # 1484 --> $ 192000\n",
      "house # 1485 --> $ 188500\n",
      "house # 1486 --> $ 189000\n",
      "house # 1487 --> $ 582933\n",
      "house # 1488 --> $ 287090\n",
      "house # 1489 --> $ 185500\n",
      "house # 1490 --> $ 220000\n",
      "house # 1491 --> $ 215200\n",
      "house # 1492 --> $ 83000\n",
      "house # 1493 --> $ 139000\n",
      "house # 1494 --> $ 236000\n",
      "house # 1495 --> $ 236000\n",
      "house # 1496 --> $ 236500\n",
      "house # 1497 --> $ 151000\n",
      "house # 1498 --> $ 151000\n",
      "house # 1499 --> $ 155000\n",
      "house # 1500 --> $ 155000\n",
      "house # 1501 --> $ 172500\n",
      "house # 1502 --> $ 151000\n",
      "house # 1503 --> $ 395192\n",
      "house # 1504 --> $ 236000\n",
      "house # 1505 --> $ 203000\n",
      "house # 1506 --> $ 185750\n",
      "house # 1507 --> $ 190000\n",
      "house # 1508 --> $ 167900\n",
      "house # 1509 --> $ 154000\n",
      "house # 1510 --> $ 139000\n",
      "house # 1511 --> $ 142000\n",
      "house # 1512 --> $ 142000\n",
      "house # 1513 --> $ 140000\n",
      "house # 1514 --> $ 179000\n",
      "house # 1515 --> $ 205000\n",
      "house # 1516 --> $ 127000\n",
      "house # 1517 --> $ 133000\n",
      "house # 1518 --> $ 124000\n",
      "house # 1519 --> $ 206000\n",
      "house # 1520 --> $ 142000\n",
      "house # 1521 --> $ 139000\n",
      "house # 1522 --> $ 159000\n",
      "house # 1523 --> $ 122000\n",
      "house # 1524 --> $ 117500\n",
      "house # 1525 --> $ 83000\n",
      "house # 1526 --> $ 80000\n",
      "house # 1527 --> $ 134800\n",
      "house # 1528 --> $ 155000\n",
      "house # 1529 --> $ 154000\n",
      "house # 1530 --> $ 162000\n",
      "house # 1531 --> $ 167500\n",
      "house # 1532 --> $ 98000\n",
      "house # 1533 --> $ 117500\n",
      "house # 1534 --> $ 119000\n",
      "house # 1535 --> $ 147000\n",
      "house # 1536 --> $ 68400\n",
      "house # 1537 --> $ 82000\n",
      "house # 1538 --> $ 147000\n",
      "house # 1539 --> $ 128000\n",
      "house # 1540 --> $ 55000\n",
      "house # 1541 --> $ 110000\n",
      "house # 1542 --> $ 161750\n",
      "house # 1543 --> $ 155000\n",
      "house # 1544 --> $ 80000\n",
      "house # 1545 --> $ 128000\n",
      "house # 1546 --> $ 165500\n",
      "house # 1547 --> $ 113000\n",
      "house # 1548 --> $ 110000\n",
      "house # 1549 --> $ 134900\n",
      "house # 1550 --> $ 166000\n",
      "house # 1551 --> $ 100000\n",
      "house # 1552 --> $ 129500\n",
      "house # 1553 --> $ 96500\n",
      "house # 1554 --> $ 143000\n",
      "house # 1555 --> $ 132000\n",
      "house # 1556 --> $ 128000\n",
      "house # 1557 --> $ 75500\n",
      "house # 1558 --> $ 83000\n",
      "house # 1559 --> $ 76500\n",
      "house # 1560 --> $ 95000\n",
      "house # 1561 --> $ 136500\n",
      "house # 1562 --> $ 144900\n",
      "house # 1563 --> $ 145000\n",
      "house # 1564 --> $ 180500\n",
      "house # 1565 --> $ 172500\n",
      "house # 1566 --> $ 260000\n",
      "house # 1567 --> $ 141000\n",
      "house # 1568 --> $ 236000\n",
      "house # 1569 --> $ 134800\n",
      "house # 1570 --> $ 132000\n",
      "house # 1571 --> $ 133000\n",
      "house # 1572 --> $ 153000\n",
      "house # 1573 --> $ 205000\n",
      "house # 1574 --> $ 60000\n",
      "house # 1575 --> $ 159000\n",
      "house # 1576 --> $ 290000\n",
      "house # 1577 --> $ 186000\n",
      "house # 1578 --> $ 105900\n",
      "house # 1579 --> $ 147000\n",
      "house # 1580 --> $ 222500\n",
      "house # 1581 --> $ 154000\n",
      "house # 1582 --> $ 139000\n",
      "house # 1583 --> $ 395192\n",
      "house # 1584 --> $ 189000\n",
      "house # 1585 --> $ 143000\n",
      "house # 1586 --> $ 84900\n",
      "house # 1587 --> $ 122000\n",
      "house # 1588 --> $ 145000\n",
      "house # 1589 --> $ 55993\n",
      "house # 1590 --> $ 155000\n",
      "house # 1591 --> $ 115000\n",
      "house # 1592 --> $ 144500\n",
      "house # 1593 --> $ 149000\n",
      "house # 1594 --> $ 100000\n",
      "house # 1595 --> $ 76500\n",
      "house # 1596 --> $ 135000\n",
      "house # 1597 --> $ 154300\n",
      "house # 1598 --> $ 230000\n",
      "house # 1599 --> $ 147000\n",
      "house # 1600 --> $ 192000\n",
      "house # 1601 --> $ 60000\n",
      "house # 1602 --> $ 125500\n",
      "house # 1603 --> $ 83000\n",
      "house # 1604 --> $ 275000\n",
      "house # 1605 --> $ 236000\n",
      "house # 1606 --> $ 215000\n",
      "house # 1607 --> $ 124000\n",
      "house # 1608 --> $ 306000\n",
      "house # 1609 --> $ 186000\n",
      "house # 1610 --> $ 200500\n",
      "house # 1611 --> $ 107000\n",
      "house # 1612 --> $ 186000\n",
      "house # 1613 --> $ 189000\n",
      "house # 1614 --> $ 180500\n",
      "house # 1615 --> $ 97000\n",
      "house # 1616 --> $ 97000\n",
      "house # 1617 --> $ 83500\n",
      "house # 1618 --> $ 130000\n",
      "house # 1619 --> $ 128200\n",
      "house # 1620 --> $ 202900\n",
      "house # 1621 --> $ 200500\n",
      "house # 1622 --> $ 98000\n",
      "house # 1623 --> $ 220000\n",
      "house # 1624 --> $ 215000\n",
      "house # 1625 --> $ 127500\n",
      "house # 1626 --> $ 157000\n",
      "house # 1627 --> $ 188500\n",
      "house # 1628 --> $ 225000\n",
      "house # 1629 --> $ 168500\n",
      "house # 1630 --> $ 278000\n",
      "house # 1631 --> $ 181000\n",
      "house # 1632 --> $ 181000\n",
      "house # 1633 --> $ 180000\n",
      "house # 1634 --> $ 223500\n",
      "house # 1635 --> $ 244000\n",
      "house # 1636 --> $ 165000\n",
      "house # 1637 --> $ 171000\n",
      "house # 1638 --> $ 192000\n",
      "house # 1639 --> $ 165000\n",
      "house # 1640 --> $ 225000\n",
      "house # 1641 --> $ 157000\n",
      "house # 1642 --> $ 188500\n",
      "house # 1643 --> $ 253000\n",
      "house # 1644 --> $ 194500\n",
      "house # 1645 --> $ 275000\n",
      "house # 1646 --> $ 157000\n",
      "house # 1647 --> $ 165000\n",
      "house # 1648 --> $ 167000\n",
      "house # 1649 --> $ 143500\n",
      "house # 1650 --> $ 123000\n",
      "house # 1651 --> $ 130000\n",
      "house # 1652 --> $ 85400\n",
      "house # 1653 --> $ 85400\n",
      "house # 1654 --> $ 148500\n",
      "house # 1655 --> $ 147000\n",
      "house # 1656 --> $ 148500\n",
      "house # 1657 --> $ 147000\n",
      "house # 1658 --> $ 148500\n",
      "house # 1659 --> $ 140000\n",
      "house # 1660 --> $ 153000\n",
      "house # 1661 --> $ 305000\n",
      "house # 1662 --> $ 335000\n",
      "house # 1663 --> $ 315000\n",
      "house # 1664 --> $ 395192\n",
      "house # 1665 --> $ 255500\n",
      "house # 1666 --> $ 306000\n",
      "house # 1667 --> $ 335000\n",
      "house # 1668 --> $ 337500\n",
      "house # 1669 --> $ 320000\n",
      "house # 1670 --> $ 320000\n",
      "house # 1671 --> $ 203000\n",
      "house # 1672 --> $ 501837\n",
      "house # 1673 --> $ 325000\n",
      "house # 1674 --> $ 255500\n",
      "house # 1675 --> $ 230000\n",
      "house # 1676 --> $ 207500\n",
      "house # 1677 --> $ 207500\n",
      "house # 1678 --> $ 395192\n",
      "house # 1679 --> $ 402861\n",
      "house # 1680 --> $ 319900\n",
      "house # 1681 --> $ 192500\n",
      "house # 1682 --> $ 297000\n",
      "house # 1683 --> $ 160200\n",
      "house # 1684 --> $ 179000\n",
      "house # 1685 --> $ 179000\n",
      "house # 1686 --> $ 160200\n",
      "house # 1687 --> $ 188500\n",
      "house # 1688 --> $ 179000\n",
      "house # 1689 --> $ 179000\n",
      "house # 1690 --> $ 244000\n",
      "house # 1691 --> $ 184100\n",
      "house # 1692 --> $ 244000\n",
      "house # 1693 --> $ 167500\n",
      "house # 1694 --> $ 188500\n",
      "house # 1695 --> $ 174000\n",
      "house # 1696 --> $ 350000\n",
      "house # 1697 --> $ 178000\n",
      "house # 1698 --> $ 305000\n",
      "house # 1699 --> $ 213250\n",
      "house # 1700 --> $ 213000\n",
      "house # 1701 --> $ 261500\n",
      "house # 1702 --> $ 201000\n",
      "house # 1703 --> $ 372500\n",
      "house # 1704 --> $ 306000\n",
      "house # 1705 --> $ 190000\n",
      "house # 1706 --> $ 377500\n",
      "house # 1707 --> $ 233170\n",
      "house # 1708 --> $ 185500\n",
      "house # 1709 --> $ 255500\n",
      "house # 1710 --> $ 264132\n",
      "house # 1711 --> $ 335000\n",
      "house # 1712 --> $ 213000\n",
      "house # 1713 --> $ 280000\n",
      "house # 1714 --> $ 229456\n",
      "house # 1715 --> $ 173500\n",
      "house # 1716 --> $ 155000\n",
      "house # 1717 --> $ 159000\n",
      "house # 1718 --> $ 93500\n",
      "house # 1719 --> $ 197900\n",
      "house # 1720 --> $ 194500\n",
      "house # 1721 --> $ 145000\n",
      "house # 1722 --> $ 93500\n",
      "house # 1723 --> $ 184900\n",
      "house # 1724 --> $ 245350\n",
      "house # 1725 --> $ 214900\n",
      "house # 1726 --> $ 185000\n",
      "house # 1727 --> $ 153900\n",
      "house # 1728 --> $ 227000\n",
      "house # 1729 --> $ 200500\n",
      "house # 1730 --> $ 235000\n",
      "house # 1731 --> $ 123000\n",
      "house # 1732 --> $ 139000\n",
      "house # 1733 --> $ 100000\n",
      "house # 1734 --> $ 100000\n",
      "house # 1735 --> $ 136500\n",
      "house # 1736 --> $ 117500\n",
      "house # 1737 --> $ 196000\n",
      "house # 1738 --> $ 175000\n",
      "house # 1739 --> $ 250000\n",
      "house # 1740 --> $ 208900\n",
      "house # 1741 --> $ 274000\n",
      "house # 1742 --> $ 172500\n",
      "house # 1743 --> $ 172500\n",
      "house # 1744 --> $ 222000\n",
      "house # 1745 --> $ 145000\n",
      "house # 1746 --> $ 174000\n",
      "house # 1747 --> $ 206900\n",
      "house # 1748 --> $ 178000\n",
      "house # 1749 --> $ 167900\n",
      "house # 1750 --> $ 130000\n",
      "house # 1751 --> $ 201800\n",
      "house # 1752 --> $ 117500\n",
      "house # 1753 --> $ 167000\n",
      "house # 1754 --> $ 193500\n",
      "house # 1755 --> $ 167900\n",
      "house # 1756 --> $ 180500\n",
      "house # 1757 --> $ 127500\n",
      "house # 1758 --> $ 142000\n",
      "house # 1759 --> $ 139600\n",
      "house # 1760 --> $ 197500\n",
      "house # 1761 --> $ 166000\n",
      "house # 1762 --> $ 155000\n",
      "house # 1763 --> $ 125000\n",
      "house # 1764 --> $ 128000\n",
      "house # 1765 --> $ 139000\n",
      "house # 1766 --> $ 167500\n",
      "house # 1767 --> $ 257500\n",
      "house # 1768 --> $ 124500\n",
      "house # 1769 --> $ 143000\n",
      "house # 1770 --> $ 136500\n",
      "house # 1771 --> $ 117500\n",
      "house # 1772 --> $ 117500\n",
      "house # 1773 --> $ 62383\n",
      "house # 1774 --> $ 135000\n",
      "house # 1775 --> $ 129500\n",
      "house # 1776 --> $ 112000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "house # 1777 --> $ 113000\n",
      "house # 1778 --> $ 109000\n",
      "house # 1779 --> $ 66500\n",
      "house # 1780 --> $ 227000\n",
      "house # 1781 --> $ 128900\n",
      "house # 1782 --> $ 109500\n",
      "house # 1783 --> $ 128000\n",
      "house # 1784 --> $ 133000\n",
      "house # 1785 --> $ 100000\n",
      "house # 1786 --> $ 115000\n",
      "house # 1787 --> $ 128000\n",
      "house # 1788 --> $ 98000\n",
      "house # 1789 --> $ 105500\n",
      "house # 1790 --> $ 105000\n",
      "house # 1791 --> $ 200000\n",
      "house # 1792 --> $ 167000\n",
      "house # 1793 --> $ 142000\n",
      "house # 1794 --> $ 163500\n",
      "house # 1795 --> $ 139000\n",
      "house # 1796 --> $ 125000\n",
      "house # 1797 --> $ 100000\n",
      "house # 1798 --> $ 139000\n",
      "house # 1799 --> $ 113000\n",
      "house # 1800 --> $ 132500\n",
      "house # 1801 --> $ 133000\n",
      "house # 1802 --> $ 130000\n",
      "house # 1803 --> $ 154000\n",
      "house # 1804 --> $ 137000\n",
      "house # 1805 --> $ 139000\n",
      "house # 1806 --> $ 133000\n",
      "house # 1807 --> $ 124500\n",
      "house # 1808 --> $ 140000\n",
      "house # 1809 --> $ 94000\n",
      "house # 1810 --> $ 174500\n",
      "house # 1811 --> $ 55000\n",
      "house # 1812 --> $ 79000\n",
      "house # 1813 --> $ 110000\n",
      "house # 1814 --> $ 100000\n",
      "house # 1815 --> $ 82000\n",
      "house # 1816 --> $ 102776\n",
      "house # 1817 --> $ 68400\n",
      "house # 1818 --> $ 184000\n",
      "house # 1819 --> $ 117000\n",
      "house # 1820 --> $ 37900\n",
      "house # 1821 --> $ 133000\n",
      "house # 1822 --> $ 110000\n",
      "house # 1823 --> $ 67000\n",
      "house # 1824 --> $ 121000\n",
      "house # 1825 --> $ 169000\n",
      "house # 1826 --> $ 78000\n",
      "house # 1827 --> $ 119000\n",
      "house # 1828 --> $ 132500\n",
      "house # 1829 --> $ 81000\n",
      "house # 1830 --> $ 140000\n",
      "house # 1831 --> $ 128000\n",
      "house # 1832 --> $ 118000\n",
      "house # 1833 --> $ 139000\n",
      "house # 1834 --> $ 119000\n",
      "house # 1835 --> $ 135900\n",
      "house # 1836 --> $ 78000\n",
      "house # 1837 --> $ 109500\n",
      "house # 1838 --> $ 125000\n",
      "house # 1839 --> $ 158500\n",
      "house # 1840 --> $ 179000\n",
      "house # 1841 --> $ 173000\n",
      "house # 1842 --> $ 98000\n",
      "house # 1843 --> $ 157000\n",
      "house # 1844 --> $ 148000\n",
      "house # 1845 --> $ 139500\n",
      "house # 1846 --> $ 180000\n",
      "house # 1847 --> $ 158000\n",
      "house # 1848 --> $ 80000\n",
      "house # 1849 --> $ 98300\n",
      "house # 1850 --> $ 129500\n",
      "house # 1851 --> $ 174500\n",
      "house # 1852 --> $ 140000\n",
      "house # 1853 --> $ 162000\n",
      "house # 1854 --> $ 137000\n",
      "house # 1855 --> $ 154000\n",
      "house # 1856 --> $ 213000\n",
      "house # 1857 --> $ 157500\n",
      "house # 1858 --> $ 142953\n",
      "house # 1859 --> $ 118858\n",
      "house # 1860 --> $ 153337\n",
      "house # 1861 --> $ 118858\n",
      "house # 1862 --> $ 305000\n",
      "house # 1863 --> $ 305000\n",
      "house # 1864 --> $ 305000\n",
      "house # 1865 --> $ 402861\n",
      "house # 1866 --> $ 297000\n",
      "house # 1867 --> $ 232000\n",
      "house # 1868 --> $ 280000\n",
      "house # 1869 --> $ 260000\n",
      "house # 1870 --> $ 194500\n",
      "house # 1871 --> $ 287090\n",
      "house # 1872 --> $ 179000\n",
      "house # 1873 --> $ 188000\n",
      "house # 1874 --> $ 152000\n",
      "house # 1875 --> $ 185000\n",
      "house # 1876 --> $ 215000\n",
      "house # 1877 --> $ 194500\n",
      "house # 1878 --> $ 240000\n",
      "house # 1879 --> $ 100000\n",
      "house # 1880 --> $ 140000\n",
      "house # 1881 --> $ 297000\n",
      "house # 1882 --> $ 236000\n",
      "house # 1883 --> $ 173000\n",
      "house # 1884 --> $ 190000\n",
      "house # 1885 --> $ 260000\n",
      "house # 1886 --> $ 203000\n",
      "house # 1887 --> $ 203000\n",
      "house # 1888 --> $ 274900\n",
      "house # 1889 --> $ 179000\n",
      "house # 1890 --> $ 139000\n",
      "house # 1891 --> $ 148000\n",
      "house # 1892 --> $ 139000\n",
      "house # 1893 --> $ 124500\n",
      "house # 1894 --> $ 118000\n",
      "house # 1895 --> $ 132000\n",
      "house # 1896 --> $ 177500\n",
      "house # 1897 --> $ 160000\n",
      "house # 1898 --> $ 154900\n",
      "house # 1899 --> $ 147000\n",
      "house # 1900 --> $ 161000\n",
      "house # 1901 --> $ 140000\n",
      "house # 1902 --> $ 82000\n",
      "house # 1903 --> $ 176000\n",
      "house # 1904 --> $ 123000\n",
      "house # 1905 --> $ 161500\n",
      "house # 1906 --> $ 161000\n",
      "house # 1907 --> $ 163000\n",
      "house # 1908 --> $ 124000\n",
      "house # 1909 --> $ 124000\n",
      "house # 1910 --> $ 124000\n",
      "house # 1911 --> $ 212000\n",
      "house # 1912 --> $ 335000\n",
      "house # 1913 --> $ 191000\n",
      "house # 1914 --> $ 68500\n",
      "house # 1915 --> $ 392500\n",
      "house # 1916 --> $ 60000\n",
      "house # 1917 --> $ 239500\n",
      "house # 1918 --> $ 131400\n",
      "house # 1919 --> $ 245500\n",
      "house # 1920 --> $ 187100\n",
      "house # 1921 --> $ 337500\n",
      "house # 1922 --> $ 372402\n",
      "house # 1923 --> $ 185000\n",
      "house # 1924 --> $ 212000\n",
      "house # 1925 --> $ 190000\n",
      "house # 1926 --> $ 337500\n",
      "house # 1927 --> $"
     ]
    }
   ],
   "source": [
    "# 1.\n",
    "# Show nr of test and train houses\n",
    "nr_test_houses = all_input_feats_test.shape[0]\n",
    "print(\"There are\", nr_test_houses,\n",
    "      \"for which I will predict sale prices.\")\n",
    "nr_train_houses = all_input_feats_train.shape[0]\n",
    "print(\"There are\", nr_train_houses,\n",
    "      \"train data houses.\")\n",
    "\n",
    "# 2.\n",
    "# Define a function to measure the\n",
    "# distance between two feature vectors\n",
    "def get_feature_vec_distance(v,w):\n",
    "    \n",
    "    return np.linalg.norm(v-w)\n",
    "    \n",
    "\n",
    "# 3.\n",
    "# Loop over all 1459 test houses\n",
    "preds_test_houses_dollar = np.zeros((nr_test_houses,1))\n",
    "for test_house_nr in range(0,nr_test_houses):\n",
    "    \n",
    "    # 3.1 get the feature vector of the test house\n",
    "    \n",
    "    # Use numerical + categorial feature vector\n",
    "    #v = all_input_feats_test[test_house_nr,:]\n",
    "    \n",
    "    # Just use numerical feature vector\n",
    "    v = normalized_test_input_matrix_feats[test_house_nr,:]\n",
    "    \n",
    "    \n",
    "    # 3.2 compare v with all 1460 train houses\n",
    "    min_dist = -1.0\n",
    "    predicted_sale_price = -1.0\n",
    "    for train_house_nr in range(0,nr_train_houses):\n",
    "        \n",
    "        # get the feature vector of the train house\n",
    "        \n",
    "        # Use numerical + categorial feature vector\n",
    "        #w = all_input_feats_train[train_house_nr,:]\n",
    "        \n",
    "        # Just use numerical feature vector\n",
    "        w = normalized_train_input_matrix_feats[train_house_nr,:]\n",
    "        \n",
    "        # compare vector v and w\n",
    "        distance = get_feature_vec_distance(v,w)\n",
    "        \n",
    "        # found a vector w that is more similar to v?\n",
    "        if (train_house_nr==0 or distance<min_dist):\n",
    "            min_dist = distance\n",
    "            predicted_sale_price =\\\n",
    "                train_output_matrix[train_house_nr][0]\n",
    "                \n",
    "    # 3.3 show predicted sale price for current test house\n",
    "    print(\"house #\", test_house_ids[test_house_nr],\n",
    "          \"--> $\", predicted_sale_price)\n",
    "    \n",
    "    # 3.4 store the predicted house price\n",
    "    preds_test_houses_dollar[test_house_nr][0] = predicted_sale_price\n",
    "    \n",
    "    \n",
    "# 4.\n",
    "\n",
    "# For a Pandas data frame column the predicted\n",
    "# house sale price matrix has to be 1-dimensional\n",
    "preds_test_houses_dollar = preds_test_houses_dollar.reshape(-1)\n",
    "\n",
    "# Create a .csv file    \n",
    "predition_dataframe = pd.DataFrame({'Id'       :test_house_ids,\n",
    "                                    'SalePrice':preds_test_houses_dollar}\n",
    "                                  )\n",
    "# convert column \"Id\" to int64 dtype\n",
    "predition_dataframe = predition_dataframe.astype({\"Id\": int})\n",
    "print(predition_dataframe)\n",
    "# now save the Pandas dataframe to a .csv file\n",
    "PREDICTION_FILENAME = \"nn_predictions.csv\"\n",
    "predition_dataframe.to_csv(PREDICTION_FILENAME, sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good to see, that here the Nearest Neighbour regression approaches that uses (numerical + categorial features) or just (numerical feature) vectors gets a worse prediction.\n",
    "\n",
    "E.g. using numerical features the Nearest Neighbour regression aproach gave a score of 0.22729, while I got a score of 0.14177 with the MLP approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
