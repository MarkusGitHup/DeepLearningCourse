{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Are-all-libraries-that-are-needed-available?\" data-toc-modified-id=\"Are-all-libraries-that-are-needed-available?-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Are all libraries that are needed available?</a></span></li><li><span><a href=\"#Prepare-an-image-provider-class\" data-toc-modified-id=\"Prepare-an-image-provider-class-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Prepare an image provider class</a></span></li><li><span><a href=\"#Build-a-Convolutional-Neural-Network-(CNN)\" data-toc-modified-id=\"Build-a-Convolutional-Neural-Network-(CNN)-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Build a Convolutional Neural Network (CNN)</a></span></li><li><span><a href=\"#Training-the-CNN\" data-toc-modified-id=\"Training-the-CNN-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Training the CNN</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The goal of this notebook is to show you how to build a simple Convolutional Neural Network (CNN) for classification in Keras.\n",
    "\n",
    "For this, first we need data! Search for open datasets of free images that show cars and bikes. E.g., Google has published the Open Image Dataset V4:\n",
    "\n",
    "https://storage.googleapis.com/openimages/web/visualizer/index.htm\n",
    "\n",
    "Prepare a folder data with the following structure:\n",
    "\n",
    "    data\n",
    "        train\n",
    "            car\n",
    "            bike\n",
    "        test\n",
    "            car\n",
    "            bike\n",
    "            \n",
    "and store at least some\n",
    "- hundreds-1000 of images for each object category in the training subfolders\n",
    "- some hundreds of images for each object category in the test subfolders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Are all libraries that are needed available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your NumPy version is:      1.16.2\n",
      "Your TensorFlow version is: 1.13.1\n",
      "Your Keras version is:      2.2.4\n",
      "Your OpenCV version is:     4.1.0\n",
      "Your Matplotlib version is: 2.0.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print( \"Your NumPy version is:      \" + np.__version__ )\n",
    "print( \"Your TensorFlow version is: \" + tf.__version__)\n",
    "print( \"Your Keras version is:      \" + keras.__version__ )\n",
    "print( \"Your OpenCV version is:     \" + cv2.__version__ )\n",
    "print( \"Your Matplotlib version is: \" + matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare an image provider class\n",
    "\n",
    "We define a class ``image_provider`` that will give us a convenient access to the images.\n",
    "\n",
    "Given a root folder (e.g. \"C:\\\\data\"), it automatically determines which subfolders are there (e.g. \"C:\\\\data\\\\car\" and \"C:\\\\data\\\\bike\"). Each subfolder is assumed to be one of the categories we are interested in (e.g. \"car\" and \"bike\").\n",
    "\n",
    "An image provide object then stores a list of all training items:\n",
    "    \n",
    "        [filename1, class_id, class_name, teacher_vec]\n",
    "        [filename2, class_id, class_name, teacher_vec]\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isdir, isfile, join\n",
    "\n",
    "IMG_SIZE = (100,100)\n",
    "    \n",
    "class image_provider:\n",
    "    \n",
    "    #\n",
    "    # Traverses all subfolders of the specified root_folder\n",
    "    # and generates a list of the form:\n",
    "    #\n",
    "    # [ [\"data/bikes/jfksdj43.jpg\", \"bikes\",\n",
    "    #   [\"data/cars/bvcnm401.jpg\", \"cars\"],\n",
    "    #   ...\n",
    "    # ]\n",
    "    #\n",
    "    def __init__(self, root_folder):\n",
    "        \n",
    "        self.all_training_items = []\n",
    "       \n",
    "        class_names = \\\n",
    "            [d for d in listdir(root_folder)\n",
    "             if isdir(os.path.join(root_folder,d))]\n",
    "\n",
    "        print(\"Under folder\\n\\t\", root_folder,\n",
    "              \"\\nI found the following subfolders/classes:\")\n",
    "        print(class_names)\n",
    "        \n",
    "        self.nr_classes = len(class_names)\n",
    "        \n",
    "        # For each subfolder ...\n",
    "        for class_id, class_name in enumerate(class_names):\n",
    "            \n",
    "            subfolder_name = root_folder + \"/\" + class_name + \"/\"\n",
    "            \n",
    "            filenames = \\\n",
    "                [subfolder_name + f\n",
    "                 for f in listdir(subfolder_name) if isfile(join(subfolder_name, f))]\n",
    "            \n",
    "            print(\"{} files in subfolder {}\".format(len(filenames), subfolder_name) )\n",
    "            \n",
    "            # For each image filename in current subfolder ...\n",
    "            for filename in filenames:\n",
    "                \n",
    "                teacher_vec = np.zeros( self.nr_classes )\n",
    "                teacher_vec[class_id] = 1.0\n",
    "                \n",
    "                self.all_training_items.append(\n",
    "                    [filename, class_id, class_name, teacher_vec] )              \n",
    "        \n",
    "        self.nr_images = len(self.all_training_items)\n",
    "        print(\"There are {} images in total available.\".format(self.nr_images))\n",
    "        \n",
    "        print(\"Here are the first 3 entries of the training items list generated:\")\n",
    "        print(self.all_training_items[:3])\n",
    "        \n",
    "    \n",
    "    \n",
    "    #   \n",
    "    # Given an absolute filename,\n",
    "    # load the image in using OpenCV,\n",
    "    # then convert it to usual RGB color channel order\n",
    "    # and scale values to be in range [0,1]\n",
    "    #\n",
    "    def load_image(self, absolute_filename):\n",
    "        \n",
    "        image = cv2.imread(absolute_filename)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "        image = cv2.resize(image, IMG_SIZE, interpolation=cv2.INTER_AREA)\n",
    "        image = image * (1.0 / 255.0)\n",
    "        \n",
    "        return image\n",
    "        \n",
    "        \n",
    "       \n",
    "    #\n",
    "    # Return the image from the dataset\n",
    "    # with the specified index\n",
    "    #\n",
    "    def get_specific_image(self, idx):\n",
    "        \n",
    "        image_filename  = self.all_training_items[idx][0]\n",
    "        class_id        = self.all_training_items[idx][1]\n",
    "        class_name      = self.all_training_items[idx][2]\n",
    "        teacher_vec     = self.all_training_items[idx][3]\n",
    "        \n",
    "        image = self.load_image(image_filename)\n",
    "        \n",
    "        return image, class_id, class_name, teacher_vec\n",
    "    \n",
    "    \n",
    "    #\n",
    "    # Return an OpenCV image and the class label\n",
    "    # where the image is chosen randomly from the\n",
    "    # list of all images.\n",
    "    #\n",
    "    def get_random_image(self):\n",
    "        \n",
    "        rnd_idx = np.random.randint(0, self.nr_images)\n",
    "        return self.get_specific_image( rnd_idx )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the image provider class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under folder\n",
      "\t V:\\01_job\\12_datasets\\01_imagenet_cars_vs_bikes\\train \n",
      "I found the following subfolders/classes:\n",
      "['bikes', 'cars']\n",
      "1000 files in subfolder V:\\01_job\\12_datasets\\01_imagenet_cars_vs_bikes\\train/bikes/\n",
      "1000 files in subfolder V:\\01_job\\12_datasets\\01_imagenet_cars_vs_bikes\\train/cars/\n",
      "There are 2000 images in total available.\n",
      "Here are the first 3 entries of the training items list generated:\n",
      "[['V:\\\\01_job\\\\12_datasets\\\\01_imagenet_cars_vs_bikes\\\\train/bikes/0001.JPEG', 0, 'bikes', array([1., 0.])], ['V:\\\\01_job\\\\12_datasets\\\\01_imagenet_cars_vs_bikes\\\\train/bikes/0002.JPEG', 0, 'bikes', array([1., 0.])], ['V:\\\\01_job\\\\12_datasets\\\\01_imagenet_cars_vs_bikes\\\\train/bikes/0003.JPEG', 0, 'bikes', array([1., 0.])]]\n"
     ]
    }
   ],
   "source": [
    "train_folder = \"V:\\\\01_job\\\\12_datasets\\\\01_imagenet_cars_vs_bikes\\\\train\"\n",
    "my_image_provider = image_provider( train_folder )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us retrieve randomly one of the images and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image has type <class 'numpy.ndarray'>\n",
      "image has shape (100, 100, 3)\n",
      "teacher vec: [1. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZZfK/wfLwFysnr/LQQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x202c23c9ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, class_id, class_name, teacher_vec = \\\n",
    "    my_image_provider.get_random_image()\n",
    "    \n",
    "THE_INPUT_IMG_SHAPE = image.shape\n",
    "    \n",
    "print(\"image has type\", type(image))\n",
    "print(\"image has shape\", image.shape)\n",
    "print(\"teacher vec:\", teacher_vec)\n",
    "plt.imshow(image)\n",
    "plt.title(\"One of the images with label {}\".format(class_name))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Convolutional Neural Network (CNN)\n",
    "\n",
    "Now we define a simple CNN in Keras.\n",
    "\n",
    "What we need to specify is the input shape of a single image:\n",
    "\n",
    "    (image_height, image_width, nr_color_channels)\n",
    "    \n",
    "Further, we need to specify how many output neurons the CNN shall have. This corresponds to the number of object classes we want to discriminate (classifiy) with the CNN:\n",
    "\n",
    "    e.g., nr_output_neurons = 2 (\"car\" vs. \"bike\")\n",
    "    e.g., nr_output_neurons = 3 (\"car\" vs. \"bike\" vs. \"truck\")\n",
    "    \n",
    "And then there are a lot of \"hyperparameters\" (model parameters we have to set manually):\n",
    "\n",
    "- how many conv layers?\n",
    "- how many pooling layers?\n",
    "- order of layers?\n",
    "- for each conv layer:\n",
    "  - nr of filters?\n",
    "  - kernel side length?\n",
    "  - kernel stride?\n",
    "  - activation function to use?\n",
    "- for each max-pooling layer:\n",
    "  - kernel side length?\n",
    "  - kernel stride?\n",
    "- for the final MLP at the end:\n",
    "  - nr of layers?\n",
    "  - for each MLP layer:\n",
    "    - nr of neurons?\n",
    "    - activation function to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 49, 49, 32)        1568      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18432)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 36866     \n",
      "=================================================================\n",
      "Total params: 38,434\n",
      "Trainable params: 38,434\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "\n",
    "def build_cnn_model(input_shape_of_single_image, nr_output_neurons):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # 1. Define the feature hierarchy:\n",
    "    \n",
    "    # Layer 1\n",
    "    nr_filter       = 32\n",
    "    kernel_side_len = 4\n",
    "    kernel_stride   = 2\n",
    "    model.add(Conv2D(nr_filter,\n",
    "                     kernel_size=(kernel_side_len, kernel_side_len),\n",
    "                     strides=(kernel_stride, kernel_stride),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape_of_single_image)\n",
    "             )\n",
    "    \n",
    "    # Layer 2\n",
    "    kernel_side_len = 2\n",
    "    kernel_stride   = 2\n",
    "    model.add(MaxPooling2D(pool_size=(kernel_side_len, kernel_side_len),\n",
    "                           strides=(kernel_stride, kernel_stride))\n",
    "             )\n",
    "\n",
    "    # 2. Define the MLP part:\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(nr_output_neurons, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# end build_a_cnn_model\n",
    "\n",
    "\n",
    "\n",
    "# 1. Clear the last Keras session\n",
    "#    This will clear the underlying TensorFlow graph\n",
    "K.clear_session()\n",
    "\n",
    "\n",
    "# 2. Create a CNN model\n",
    "my_cnn = build_cnn_model(THE_INPUT_IMG_SHAPE, my_image_provider.nr_classes)\n",
    "\n",
    "\n",
    "# 3. Show the model\n",
    "my_cnn.summary()\n",
    "\n",
    "\n",
    "# 4. Build model and configure model for training\n",
    "my_cnn.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the CNN\n",
    "\n",
    "Now we will train the CNN. We will use \"Stochastic Gradient Descent\" (SGD) with just one training image per training step for the sake of simplicity (corresponds to mini batch with batch size 1). \n",
    "\n",
    "But note, that training could also be done using the \"mini batch\" approach. This means that in each training step we approximate the gradient for gradient descent step using e.g. 16,32,or 64 images.\n",
    "\n",
    "This is normally much faster, however, it also means that we have to put all the images in one large 4D training array (img_nr, img_height, img_width, nr_channels). And this can make problems when we have a large dataset due to out-of-memory errors on the GPU or CPU being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training step  0\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.6376\n",
      "training step  1\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1598\n",
      "training step  2\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0239\n",
      "training step  3\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0055\n",
      "training step  4\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.9974\n",
      "training step  5\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.3778\n",
      "training step  6\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.4051\n",
      "training step  7\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0201\n",
      "training step  8\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.7683\n",
      "training step  9\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3110\n",
      "training step  10\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8301\n",
      "training step  11\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3304\n",
      "training step  12\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3268\n",
      "training step  13\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.0297\n",
      "training step  14\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0757\n",
      "training step  15\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1436\n",
      "training step  16\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0179\n",
      "training step  17\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0197\n",
      "training step  18\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.7900\n",
      "training step  19\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0234\n",
      "training step  20\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0342\n",
      "training step  21\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0367\n",
      "training step  22\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0453\n",
      "training step  23\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0351\n",
      "training step  24\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0369\n",
      "training step  25\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0116\n",
      "training step  26\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0378\n",
      "training step  27\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.5250\n",
      "training step  28\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0070\n",
      "training step  29\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0344\n",
      "training step  30\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.7126\n",
      "training step  31\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0810\n",
      "training step  32\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0945\n",
      "training step  33\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9556\n",
      "training step  34\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.9719\n",
      "training step  35\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.6332\n",
      "training step  36\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.5091\n",
      "training step  37\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3373\n",
      "training step  38\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3355\n",
      "training step  39\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0057\n",
      "training step  40\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4691\n",
      "training step  41\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4739\n",
      "training step  42\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8815\n",
      "training step  43\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4474\n",
      "training step  44\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9520\n",
      "training step  45\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5498\n",
      "training step  46\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6315\n",
      "training step  47\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8223\n",
      "training step  48\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7711\n",
      "training step  49\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5827\n",
      "training step  50\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7669\n",
      "training step  51\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7350\n",
      "training step  52\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6238\n",
      "training step  53\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7499\n",
      "training step  54\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6356\n",
      "training step  55\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7001\n",
      "training step  56\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.3296\n",
      "training step  57\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7291\n",
      "training step  58\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7227\n",
      "training step  59\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6324\n",
      "training step  60\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7697\n",
      "training step  61\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6415\n",
      "training step  62\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6422\n",
      "training step  63\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8160\n",
      "training step  64\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6483\n",
      "training step  65\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6951\n",
      "training step  66\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6801\n",
      "training step  67\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8554\n",
      "training step  68\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7024\n",
      "training step  69\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7978\n",
      "training step  70\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4772\n",
      "training step  71\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6820\n",
      "training step  72\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5869\n",
      "training step  73\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6978\n",
      "training step  74\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6802\n",
      "training step  75\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7040\n",
      "training step  76\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5688\n",
      "training step  77\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7227\n",
      "training step  78\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7419\n",
      "training step  79\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4539\n",
      "training step  80\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6078\n",
      "training step  81\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5071\n",
      "training step  82\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4189\n",
      "training step  83\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4741\n",
      "training step  84\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4614\n",
      "training step  85\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2454\n",
      "training step  86\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2194\n",
      "training step  87\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1978\n",
      "training step  88\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1117\n",
      "training step  89\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1850\n",
      "training step  90\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0676\n",
      "training step  91\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0386\n",
      "training step  92\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.3237\n",
      "training step  93\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0444\n",
      "training step  94\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0374\n",
      "training step  95\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0388\n",
      "training step  96\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.2974\n",
      "training step  97\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1667\n",
      "training step  98\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8921\n",
      "training step  99\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.3077\n",
      "training step  100\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.5131\n",
      "training step  101\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0791\n",
      "training step  102\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7786\n",
      "training step  103\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7695\n",
      "training step  104\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7173\n",
      "training step  105\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6825\n",
      "training step  106\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7214\n",
      "training step  107\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6406\n",
      "training step  108\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6354\n",
      "training step  109\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6982\n",
      "training step  110\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6416\n",
      "training step  111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6815\n",
      "training step  112\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6942\n",
      "training step  113\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7597\n",
      "training step  114\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7011\n",
      "training step  115\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0861\n",
      "training step  116\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7099\n",
      "training step  117\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7218\n",
      "training step  118\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7311\n",
      "training step  119\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7000\n",
      "training step  120\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7196\n",
      "training step  121\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6490\n",
      "training step  122\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7098\n",
      "training step  123\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7209\n",
      "training step  124\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5476\n",
      "training step  125\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7093\n",
      "training step  126\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6576\n",
      "training step  127\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6521\n",
      "training step  128\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7298\n",
      "training step  129\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6221\n",
      "training step  130\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7018\n",
      "training step  131\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7278\n",
      "training step  132\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5845\n",
      "training step  133\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5768\n",
      "training step  134\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8172\n",
      "training step  135\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7468\n",
      "training step  136\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4479\n",
      "training step  137\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5431\n",
      "training step  138\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5658\n",
      "training step  139\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5866\n",
      "training step  140\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5163\n",
      "training step  141\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3545\n",
      "training step  142\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2763\n",
      "training step  143\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2273\n",
      "training step  144\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1837\n",
      "training step  145\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.9008\n",
      "training step  146\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.0531\n",
      "training step  147\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8216\n",
      "training step  148\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2211\n",
      "training step  149\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3049\n",
      "training step  150\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9639\n",
      "training step  151\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0130\n",
      "training step  152\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5598\n",
      "training step  153\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8431\n",
      "training step  154\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8813\n",
      "training step  155\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6699\n",
      "training step  156\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6679\n",
      "training step  157\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6767\n",
      "training step  158\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7070\n",
      "training step  159\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6537\n",
      "training step  160\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6555\n",
      "training step  161\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5748\n",
      "training step  162\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6944\n",
      "training step  163\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5069\n",
      "training step  164\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6615\n",
      "training step  165\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7478\n",
      "training step  166\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7331\n",
      "training step  167\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6992\n",
      "training step  168\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6846\n",
      "training step  169\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6729\n",
      "training step  170\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6201\n",
      "training step  171\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6036\n",
      "training step  172\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6346\n",
      "training step  173\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training step  174\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6225\n",
      "training step  175\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8249\n",
      "training step  176\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6845\n",
      "training step  177\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6821\n",
      "training step  178\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6945\n",
      "training step  179\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7187\n",
      "training step  180\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4768\n",
      "training step  181\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6363\n",
      "training step  182\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5050\n",
      "training step  183\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3136\n",
      "training step  184\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7533\n",
      "training step  185\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6475\n",
      "training step  186\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6872\n",
      "training step  187\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6879\n",
      "training step  188\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6456\n",
      "training step  189\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7390\n",
      "training step  190\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4352\n",
      "training step  191\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5960\n",
      "training step  192\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5519\n",
      "training step  193\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3435\n",
      "training step  194\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5674\n",
      "training step  195\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1960\n",
      "training step  196\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1401\n",
      "training step  197\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.0837\n",
      "training step  198\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1026\n",
      "training step  199\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9319\n",
      "training step  200\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7877\n",
      "training step  201\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4249\n",
      "training step  202\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7750\n",
      "training step  203\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5708\n",
      "training step  204\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6822\n",
      "training step  205\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6208\n",
      "training step  206\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6982\n",
      "training step  207\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6568\n",
      "training step  208\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5524\n",
      "training step  209\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6736\n",
      "training step  210\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7205\n",
      "training step  211\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7078\n",
      "training step  212\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6464\n",
      "training step  213\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6969\n",
      "training step  214\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7328\n",
      "training step  215\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6454\n",
      "training step  216\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6783\n",
      "training step  217\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6655\n",
      "training step  218\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7496\n",
      "training step  219\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7275\n",
      "training step  220\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6454\n",
      "training step  221\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6516\n",
      "training step  222\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6084\n",
      "training step  223\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7381\n",
      "training step  224\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5908\n",
      "training step  225\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6219\n",
      "training step  226\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7521\n",
      "training step  227\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7084\n",
      "training step  228\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4407\n",
      "training step  229\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6538\n",
      "training step  230\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7277\n",
      "training step  231\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7079\n",
      "training step  232\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7693\n",
      "training step  233\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7343\n",
      "training step  234\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6079\n",
      "training step  235\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7192\n",
      "training step  236\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5929\n",
      "training step  237\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7093\n",
      "training step  238\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6813\n",
      "training step  239\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2478\n",
      "training step  240\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6038\n",
      "training step  241\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7455\n",
      "training step  242\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6541\n",
      "training step  243\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7377\n",
      "training step  244\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6908\n",
      "training step  245\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6747\n",
      "training step  246\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6548\n",
      "training step  247\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5721\n",
      "training step  248\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6833\n",
      "training step  249\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7979\n",
      "training step  250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6638\n",
      "training step  251\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7277\n",
      "training step  252\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5175\n",
      "training step  253\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6021\n",
      "training step  254\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4714\n",
      "training step  255\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6375\n",
      "training step  256\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7543\n",
      "training step  257\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6908\n",
      "training step  258\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7392\n",
      "training step  259\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5628\n",
      "training step  260\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6940\n",
      "training step  261\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6577\n",
      "training step  262\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6868\n",
      "training step  263\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6465\n",
      "training step  264\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6727\n",
      "training step  265\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6965\n",
      "training step  266\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7220\n",
      "training step  267\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6712\n",
      "training step  268\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7553\n",
      "training step  269\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3625\n",
      "training step  270\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4986\n",
      "training step  271\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6867\n",
      "training step  272\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6837\n",
      "training step  273\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6795\n",
      "training step  274\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8186\n",
      "training step  275\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6175\n",
      "training step  276\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5523\n",
      "training step  277\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5779\n",
      "training step  278\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7060\n",
      "training step  279\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4222\n",
      "training step  280\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5227\n",
      "training step  281\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5850\n",
      "training step  282\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7827\n",
      "training step  283\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7342\n",
      "training step  284\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7885\n",
      "training step  285\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5278\n",
      "training step  286\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3953\n",
      "training step  287\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7302\n",
      "training step  288\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9532\n",
      "training step  289\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4753\n",
      "training step  290\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6118\n",
      "training step  291\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2925\n",
      "training step  292\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6151\n",
      "training step  293\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6695\n",
      "training step  294\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1354\n",
      "training step  295\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8201\n",
      "training step  296\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6914\n",
      "training step  297\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5266\n",
      "training step  298\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3287\n",
      "training step  299\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6265\n",
      "training step  300\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7002\n",
      "training step  301\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6150\n",
      "training step  302\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9164\n",
      "training step  303\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6820\n",
      "training step  304\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4030\n",
      "training step  305\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5338\n",
      "training step  306\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5448\n",
      "training step  307\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7590\n",
      "training step  308\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6488\n",
      "training step  309\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6360\n",
      "training step  310\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9299\n",
      "training step  311\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6006\n",
      "training step  312\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4984\n",
      "training step  313\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5150\n",
      "training step  314\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6256\n",
      "training step  315\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6682\n",
      "training step  316\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5148\n",
      "training step  317\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "training step  318\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6400\n",
      "training step  319\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4830\n",
      "training step  320\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4901\n",
      "training step  321\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8181\n",
      "training step  322\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6143\n",
      "training step  323\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0208\n",
      "training step  324\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7589\n",
      "training step  325\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5208\n",
      "training step  326\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5787\n",
      "training step  327\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7093\n",
      "training step  328\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4068\n",
      "training step  329\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5741\n",
      "training step  330\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5664\n",
      "training step  331\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4319\n",
      "training step  332\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3086\n",
      "training step  333\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1291\n",
      "training step  334\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1976\n",
      "training step  335\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4238\n",
      "training step  336\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3354\n",
      "training step  337\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4657\n",
      "training step  338\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1755\n",
      "training step  339\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1639\n",
      "training step  340\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1068\n",
      "training step  341\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.8085\n",
      "training step  342\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1385\n",
      "training step  343\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.5815\n",
      "training step  344\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3014\n",
      "training step  345\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1330\n",
      "training step  346\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training step  347\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1643\n",
      "training step  348\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7030\n",
      "training step  349\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5798\n",
      "training step  350\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0018\n",
      "training step  351\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5598\n",
      "training step  352\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6040\n",
      "training step  353\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6042\n",
      "training step  354\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4379\n",
      "training step  355\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3866\n",
      "training step  356\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0632\n",
      "training step  357\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.0124\n",
      "training step  358\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.6858\n",
      "training step  359\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9318\n",
      "training step  360\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9134\n",
      "training step  361\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9318\n",
      "training step  362\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2310\n",
      "training step  363\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6396\n",
      "training step  364\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4680\n",
      "training step  365\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4961\n",
      "training step  366\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6288\n",
      "training step  367\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6838\n",
      "training step  368\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5250\n",
      "training step  369\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7575\n",
      "training step  370\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5921\n",
      "training step  371\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3706\n",
      "training step  372\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6414\n",
      "training step  373\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5774\n",
      "training step  374\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5297\n",
      "training step  375\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1653\n",
      "training step  376\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8329\n",
      "training step  377\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7378\n",
      "training step  378\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3278\n",
      "training step  379\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8707\n",
      "training step  380\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5284\n",
      "training step  381\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6018\n",
      "training step  382\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6878\n",
      "training step  383\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6251\n",
      "training step  384\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4946\n",
      "training step  385\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6464\n",
      "training step  386\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4259\n",
      "training step  387\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7429\n",
      "training step  388\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7087\n",
      "training step  389\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3592\n",
      "training step  390\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8667\n",
      "training step  391\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2748\n",
      "training step  392\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0861\n",
      "training step  393\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4676\n",
      "training step  394\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2068\n",
      "training step  395\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3466\n",
      "training step  396\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7769\n",
      "training step  397\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3734\n",
      "training step  398\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2131\n",
      "training step  399\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9746\n",
      "training step  400\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1229\n",
      "training step  401\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5018\n",
      "training step  402\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5721\n",
      "training step  403\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7352\n",
      "training step  404\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5325\n",
      "training step  405\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3194\n",
      "training step  406\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6250\n",
      "training step  407\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5709\n",
      "training step  408\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0108\n",
      "training step  409\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4457\n",
      "training step  410\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6477\n",
      "training step  411\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5021\n",
      "training step  412\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4000\n",
      "training step  413\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6870\n",
      "training step  414\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3614\n",
      "training step  415\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3362\n",
      "training step  416\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7878\n",
      "training step  417\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3926\n",
      "training step  418\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8059\n",
      "training step  419\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6556\n",
      "training step  420\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7483\n",
      "training step  421\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7540\n",
      "training step  422\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0246\n",
      "training step  423\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5967\n",
      "training step  424\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4657\n",
      "training step  425\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0887\n",
      "training step  426\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5929\n",
      "training step  427\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5797\n",
      "training step  428\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6172\n",
      "training step  429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4194\n",
      "training step  430\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3982\n",
      "training step  431\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2534\n",
      "training step  432\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7290\n",
      "training step  433\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training step  434\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4028\n",
      "training step  435\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8364\n",
      "training step  436\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3529\n",
      "training step  437\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0145\n",
      "training step  438\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1095\n",
      "training step  439\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3733\n",
      "training step  440\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4223\n",
      "training step  441\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1782\n",
      "training step  442\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1669\n",
      "training step  443\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3363\n",
      "training step  444\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8256\n",
      "training step  445\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0655\n",
      "training step  446\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5840\n",
      "training step  447\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4919\n",
      "training step  448\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8822\n",
      "training step  449\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5011\n",
      "training step  450\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8356\n",
      "training step  451\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0951\n",
      "training step  452\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2253\n",
      "training step  453\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5588\n",
      "training step  454\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6067\n",
      "training step  455\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5972\n",
      "training step  456\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4346\n",
      "training step  457\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4811\n",
      "training step  458\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4320\n",
      "training step  459\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4319\n",
      "training step  460\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5778\n",
      "training step  461\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6989\n",
      "training step  462\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4160\n",
      "training step  463\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3574\n",
      "training step  464\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3717\n",
      "training step  465\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7435\n",
      "training step  466\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2633\n",
      "training step  467\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2703\n",
      "training step  468\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9514\n",
      "training step  469\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7926\n",
      "training step  470\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2604\n",
      "training step  471\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.8501\n",
      "training step  472\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2802\n",
      "training step  473\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5075\n",
      "training step  474\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4537\n",
      "training step  475\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7871\n",
      "training step  476\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3071\n",
      "training step  477\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5634\n",
      "training step  478\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9203\n",
      "training step  479\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8730\n",
      "training step  480\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2368\n",
      "training step  481\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2990\n",
      "training step  482\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1827\n",
      "training step  483\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9638\n",
      "training step  484\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0686\n",
      "training step  485\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4056\n",
      "training step  486\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7884\n",
      "training step  487\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0691\n",
      "training step  488\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1196\n",
      "training step  489\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4465\n",
      "training step  490\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5924\n",
      "training step  491\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2340\n",
      "training step  492\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3651\n",
      "training step  493\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2603\n",
      "training step  494\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3660\n",
      "training step  495\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2402\n",
      "training step  496\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8210\n",
      "training step  497\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0450\n",
      "training step  498\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5198\n",
      "training step  499\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8687\n",
      "training step  500\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0891\n",
      "training step  501\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4827\n",
      "training step  502\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3476\n",
      "training step  503\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4165\n",
      "training step  504\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9443\n",
      "training step  505\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4216\n",
      "training step  506\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7932\n",
      "training step  507\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6421\n",
      "training step  508\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3483\n",
      "training step  509\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6535\n",
      "training step  510\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4263\n",
      "training step  511\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4263\n",
      "training step  512\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3718\n",
      "training step  513\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8610\n",
      "training step  514\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2290\n",
      "training step  515\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1901\n",
      "training step  516\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9063\n",
      "training step  517\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3140\n",
      "training step  518\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8773\n",
      "training step  519\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6407\n",
      "training step  520\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7253\n",
      "training step  521\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6397\n",
      "training step  522\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6161\n",
      "training step  523\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5851\n",
      "training step  524\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3698\n",
      "training step  525\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4023\n",
      "training step  526\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2269\n",
      "training step  527\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1447\n",
      "training step  528\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5165\n",
      "training step  529\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7666\n",
      "training step  530\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7771\n",
      "training step  531\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5184\n",
      "training step  532\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6587\n",
      "training step  533\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9461\n",
      "training step  534\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6913\n",
      "training step  535\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5874\n",
      "training step  536\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2282\n",
      "training step  537\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4147\n",
      "training step  538\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5606\n",
      "training step  539\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4735\n",
      "training step  540\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7769\n",
      "training step  541\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3481\n",
      "training step  542\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7898\n",
      "training step  543\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8385\n",
      "training step  544\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2770\n",
      "training step  545\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8171\n",
      "training step  546\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6397\n",
      "training step  547\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4060\n",
      "training step  548\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5578\n",
      "training step  549\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5921\n",
      "training step  550\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1675\n",
      "training step  551\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6891\n",
      "training step  552\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6132\n",
      "training step  553\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5514\n",
      "training step  554\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5179\n",
      "training step  555\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2621\n",
      "training step  556\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3342\n",
      "training step  557\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5732\n",
      "training step  558\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2456\n",
      "training step  559\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9515\n",
      "training step  560\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4683\n",
      "training step  561\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1081\n",
      "training step  562\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5612\n",
      "training step  563\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6778\n",
      "training step  564\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2728\n",
      "training step  565\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3982\n",
      "training step  566\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2669\n",
      "training step  567\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0932\n",
      "training step  568\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4900\n",
      "training step  569\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7767\n",
      "training step  570\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0948\n",
      "training step  571\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9171\n",
      "training step  572\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.5305\n",
      "training step  573\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1277\n",
      "training step  574\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0815\n",
      "training step  575\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0332\n",
      "training step  576\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3536\n",
      "training step  577\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.7076\n",
      "training step  578\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0781\n",
      "training step  579\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.4904\n",
      "training step  580\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4939\n",
      "training step  581\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0728\n",
      "training step  582\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5662\n",
      "training step  583\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3713\n",
      "training step  584\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0707\n",
      "training step  585\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9286\n",
      "training step  586\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.3340\n",
      "training step  587\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3284\n",
      "training step  588\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6001\n",
      "training step  589\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3189\n",
      "training step  590\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4184\n",
      "training step  591\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5203\n",
      "training step  592\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7018\n",
      "training step  593\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5740\n",
      "training step  594\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4779\n",
      "training step  595\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8226\n",
      "training step  596\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5891\n",
      "training step  597\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1107\n",
      "training step  598\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5062\n",
      "training step  599\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5074\n",
      "training step  600\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.4580\n",
      "training step  601\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5899\n",
      "training step  602\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4949\n",
      "training step  603\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9856\n",
      "training step  604\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4684\n",
      "training step  605\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5077\n",
      "training step  606\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training step  607\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6785\n",
      "training step  608\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6149\n",
      "training step  609\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0667\n",
      "training step  610\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1651\n",
      "training step  611\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5382\n",
      "training step  612\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0139\n",
      "training step  613\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8507\n",
      "training step  614\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6055\n",
      "training step  615\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3028\n",
      "training step  616\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7587\n",
      "training step  617\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8450\n",
      "training step  618\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0727\n",
      "training step  619\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1472\n",
      "training step  620\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4692\n",
      "training step  621\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6968\n",
      "training step  622\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7747\n",
      "training step  623\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3403\n",
      "training step  624\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0896\n",
      "training step  625\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5444\n",
      "training step  626\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5903\n",
      "training step  627\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6166\n",
      "training step  628\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0999\n",
      "training step  629\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6983\n",
      "training step  630\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0769\n",
      "training step  631\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4759\n",
      "training step  632\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8452\n",
      "training step  633\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9325\n",
      "training step  634\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6462\n",
      "training step  635\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6892\n",
      "training step  636\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3132\n",
      "training step  637\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0338\n",
      "training step  638\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4012\n",
      "training step  639\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6747\n",
      "training step  640\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3142\n",
      "training step  641\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7814\n",
      "training step  642\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6107\n",
      "training step  643\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3595\n",
      "training step  644\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4302\n",
      "training step  645\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6215\n",
      "training step  646\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5665\n",
      "training step  647\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3758\n",
      "training step  648\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5655\n",
      "training step  649\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0876\n",
      "training step  650\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6684\n",
      "training step  651\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5718\n",
      "training step  652\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5648\n",
      "training step  653\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4534\n",
      "training step  654\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2083\n",
      "training step  655\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9252\n",
      "training step  656\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5203\n",
      "training step  657\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4818\n",
      "training step  658\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2430\n",
      "training step  659\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8553\n",
      "training step  660\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7135\n",
      "training step  661\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9735\n",
      "training step  662\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5515\n",
      "training step  663\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2458\n",
      "training step  664\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5618\n",
      "training step  665\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7522\n",
      "training step  666\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3463\n",
      "training step  667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5872\n",
      "training step  668\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3543\n",
      "training step  669\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3374\n",
      "training step  670\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6901\n",
      "training step  671\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5801\n",
      "training step  672\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2470\n",
      "training step  673\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1905\n",
      "training step  674\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5559\n",
      "training step  675\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1405\n",
      "training step  676\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1152\n",
      "training step  677\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2475\n",
      "training step  678\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0543\n",
      "training step  679\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3691\n",
      "training step  680\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0995\n",
      "training step  681\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7333\n",
      "training step  682\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0921\n",
      "training step  683\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2077\n",
      "training step  684\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4652\n",
      "training step  685\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1803\n",
      "training step  686\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6821\n",
      "training step  687\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0164\n",
      "training step  688\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2301\n",
      "training step  689\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5191\n",
      "training step  690\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1289\n",
      "training step  691\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4400\n",
      "training step  692\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4614\n",
      "training step  693\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3929\n",
      "training step  694\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0898\n",
      "training step  695\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3329\n",
      "training step  696\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4831\n",
      "training step  697\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4524\n",
      "training step  698\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3062\n",
      "training step  699\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4322\n",
      "training step  700\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1016\n",
      "training step  701\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1890\n",
      "training step  702\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1451\n",
      "training step  703\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4558\n",
      "training step  704\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7682\n",
      "training step  705\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7212\n",
      "training step  706\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7614\n",
      "training step  707\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0223\n",
      "training step  708\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2690\n",
      "training step  709\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3781\n",
      "training step  710\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4261\n",
      "training step  711\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6220\n",
      "training step  712\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2341\n",
      "training step  713\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6577\n",
      "training step  714\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2650\n",
      "training step  715\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9283\n",
      "training step  716\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1314\n",
      "training step  717\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3092\n",
      "training step  718\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4157\n",
      "training step  719\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1782\n",
      "training step  720\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.4858\n",
      "training step  721\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1107\n",
      "training step  722\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2013\n",
      "training step  723\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4584\n",
      "training step  724\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3182\n",
      "training step  725\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1377\n",
      "training step  726\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2236\n",
      "training step  727\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5531\n",
      "training step  728\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6399\n",
      "training step  729\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5990\n",
      "training step  730\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7824\n",
      "training step  731\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4577\n",
      "training step  732\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6839\n",
      "training step  733\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4767\n",
      "training step  734\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6725\n",
      "training step  735\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1752\n",
      "training step  736\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2179\n",
      "training step  737\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0776\n",
      "training step  738\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3988\n",
      "training step  739\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0176\n",
      "training step  740\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7876\n",
      "training step  741\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0563\n",
      "training step  742\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1602\n",
      "training step  743\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3035\n",
      "training step  744\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3103\n",
      "training step  745\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5611\n",
      "training step  746\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2731\n",
      "training step  747\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1928\n",
      "training step  748\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.9413\n",
      "training step  749\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2093\n",
      "training step  750\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4647\n",
      "training step  751\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2749\n",
      "training step  752\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5257\n",
      "training step  753\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7623\n",
      "training step  754\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6361\n",
      "training step  755\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1296\n",
      "training step  756\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3487\n",
      "training step  757\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6583\n",
      "training step  758\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1197\n",
      "training step  759\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1801\n",
      "training step  760\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5663\n",
      "training step  761\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4005\n",
      "training step  762\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4697\n",
      "training step  763\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.5452\n",
      "training step  764\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1143\n",
      "training step  765\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0713\n",
      "training step  766\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2617\n",
      "training step  767\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3139\n",
      "training step  768\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6367\n",
      "training step  769\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9033\n",
      "training step  770\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1369\n",
      "training step  771\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8753\n",
      "training step  772\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5633\n",
      "training step  773\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2389\n",
      "training step  774\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3057\n",
      "training step  775\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3074\n",
      "training step  776\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6612\n",
      "training step  777\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8506\n",
      "training step  778\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5593\n",
      "training step  779\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training step  780\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1183\n",
      "training step  781\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4636\n",
      "training step  782\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8238\n",
      "training step  783\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1391\n",
      "training step  784\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1395\n",
      "training step  785\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6413\n",
      "training step  786\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0379\n",
      "training step  787\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1197\n",
      "training step  788\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2577\n",
      "training step  789\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1778\n",
      "training step  790\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4128\n",
      "training step  791\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2144\n",
      "training step  792\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1286\n",
      "training step  793\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1045\n",
      "training step  794\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1249\n",
      "training step  795\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.3955\n",
      "training step  796\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5501\n",
      "training step  797\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1710\n",
      "training step  798\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1773\n",
      "training step  799\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6946\n",
      "training step  800\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1027\n",
      "training step  801\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7065\n",
      "training step  802\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1034\n",
      "training step  803\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0843\n",
      "training step  804\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3849\n",
      "training step  805\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0692\n",
      "training step  806\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4832\n",
      "training step  807\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6003\n",
      "training step  808\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0668\n",
      "training step  809\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5517\n",
      "training step  810\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.3356\n",
      "training step  811\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9624\n",
      "training step  812\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2544\n",
      "training step  813\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4432\n",
      "training step  814\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5506\n",
      "training step  815\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5605\n",
      "training step  816\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7214\n",
      "training step  817\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5218\n",
      "training step  818\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0748\n",
      "training step  819\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1098\n",
      "training step  820\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5343\n",
      "training step  821\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5042\n",
      "training step  822\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9417\n",
      "training step  823\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3505\n",
      "training step  824\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1783\n",
      "training step  825\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2941\n",
      "training step  826\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8807\n",
      "training step  827\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.3769\n",
      "training step  828\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5870\n",
      "training step  829\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2986\n",
      "training step  830\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7793\n",
      "training step  831\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8638\n",
      "training step  832\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0113\n",
      "training step  833\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8419\n",
      "training step  834\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2571\n",
      "training step  835\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5705\n",
      "training step  836\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2273\n",
      "training step  837\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2369\n",
      "training step  838\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1580\n",
      "training step  839\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3268\n",
      "training step  840\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0690\n",
      "training step  841\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5802\n",
      "training step  842\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9543\n",
      "training step  843\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1695\n",
      "training step  844\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1693\n",
      "training step  845\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1776\n",
      "training step  846\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0275\n",
      "training step  847\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3366\n",
      "training step  848\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2564\n",
      "training step  849\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5235\n",
      "training step  850\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5792\n",
      "training step  851\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3514\n",
      "training step  852\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2075\n",
      "training step  853\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5011\n",
      "training step  854\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4380\n",
      "training step  855\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1956\n",
      "training step  856\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3144\n",
      "training step  857\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7897\n",
      "training step  858\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0780\n",
      "training step  859\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6503\n",
      "training step  860\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1318\n",
      "training step  861\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0367\n",
      "training step  862\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2099\n",
      "training step  863\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0464\n",
      "training step  864\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5333\n",
      "training step  865\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5568\n",
      "training step  866\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1501\n",
      "training step  867\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4589\n",
      "training step  868\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0548\n",
      "training step  869\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0768\n",
      "training step  870\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.4269\n",
      "training step  871\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0101\n",
      "training step  872\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5616\n",
      "training step  873\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0295\n",
      "training step  874\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7181\n",
      "training step  875\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2992\n",
      "training step  876\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2331\n",
      "training step  877\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1546\n",
      "training step  878\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0124\n",
      "training step  879\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1947\n",
      "training step  880\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1834\n",
      "training step  881\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9418\n",
      "training step  882\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1251\n",
      "training step  883\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3765\n",
      "training step  884\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.7459\n",
      "training step  885\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.4417\n",
      "training step  886\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7656\n",
      "training step  887\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0477\n",
      "training step  888\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0759\n",
      "training step  889\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0800\n",
      "training step  890\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2525\n",
      "training step  891\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1189\n",
      "training step  892\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.3104\n",
      "training step  893\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6340\n",
      "training step  894\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0385\n",
      "training step  895\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5510\n",
      "training step  896\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4876\n",
      "training step  897\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7226\n",
      "training step  898\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1112\n",
      "training step  899\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0758\n",
      "training step  900\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2600\n",
      "training step  901\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2655\n",
      "training step  902\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4880\n",
      "training step  903\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9216\n",
      "training step  904\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1554\n",
      "training step  905\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1360\n",
      "training step  906\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6389\n",
      "training step  907\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6274\n",
      "training step  908\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3330\n",
      "training step  909\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0795\n",
      "training step  910\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5879\n",
      "training step  911\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3027\n",
      "training step  912\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6477\n",
      "training step  913\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0847\n",
      "training step  914\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0866\n",
      "training step  915\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5179\n",
      "training step  916\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6427\n",
      "training step  917\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1956\n",
      "training step  918\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9117\n",
      "training step  919\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0329\n",
      "training step  920\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8564\n",
      "training step  921\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9670\n",
      "training step  922\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0544\n",
      "training step  923\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0389\n",
      "training step  924\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0767\n",
      "training step  925\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0224\n",
      "training step  926\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2528\n",
      "training step  927\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1975\n",
      "training step  928\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0067\n",
      "training step  929\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4976\n",
      "training step  930\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0096\n",
      "training step  931\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0146\n",
      "training step  932\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0147\n",
      "training step  933\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3038\n",
      "training step  934\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7203\n",
      "training step  935\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4490\n",
      "training step  936\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2512\n",
      "training step  937\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3589\n",
      "training step  938\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9262\n",
      "training step  939\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3095\n",
      "training step  940\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4698\n",
      "training step  941\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2525\n",
      "training step  942\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6463\n",
      "training step  943\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4796\n",
      "training step  944\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0712\n",
      "training step  945\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1618\n",
      "training step  946\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3913\n",
      "training step  947\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2908\n",
      "training step  948\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7713\n",
      "training step  949\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.3063\n",
      "training step  950\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6103\n",
      "training step  951\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5881\n",
      "training step  952\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training step  953\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.4910\n",
      "training step  954\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0148\n",
      "training step  955\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5304\n",
      "training step  956\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0699\n",
      "training step  957\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1684\n",
      "training step  958\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4342\n",
      "training step  959\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0603\n",
      "training step  960\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3404\n",
      "training step  961\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4337\n",
      "training step  962\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3966\n",
      "training step  963\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0809\n",
      "training step  964\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2026\n",
      "training step  965\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0363\n",
      "training step  966\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6383\n",
      "training step  967\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7927\n",
      "training step  968\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0643\n",
      "training step  969\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5345\n",
      "training step  970\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5313\n",
      "training step  971\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1652\n",
      "training step  972\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1389\n",
      "training step  973\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2824\n",
      "training step  974\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3573\n",
      "training step  975\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0563\n",
      "training step  976\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1934\n",
      "training step  977\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6611\n",
      "training step  978\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0425\n",
      "training step  979\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1919\n",
      "training step  980\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2565\n",
      "training step  981\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8864\n",
      "training step  982\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1499\n",
      "training step  983\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0717\n",
      "training step  984\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7327\n",
      "training step  985\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0620\n",
      "training step  986\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0970\n",
      "training step  987\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4714\n",
      "training step  988\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.3233\n",
      "training step  989\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9267\n",
      "training step  990\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0155\n",
      "training step  991\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4075\n",
      "training step  992\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6571\n",
      "training step  993\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3353\n",
      "training step  994\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1397\n",
      "training step  995\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3711\n",
      "training step  996\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0957\n",
      "training step  997\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6410\n",
      "training step  998\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7666\n",
      "training step  999\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5966\n"
     ]
    }
   ],
   "source": [
    "nr_train_steps = 1000\n",
    "\n",
    "height      = THE_INPUT_IMG_SHAPE[0]\n",
    "width       = THE_INPUT_IMG_SHAPE[1]\n",
    "nr_channels = THE_INPUT_IMG_SHAPE[2]\n",
    "\n",
    "X = np.zeros( (1,height,width,nr_channels)     )\n",
    "Y = np.zeros( (1,my_image_provider.nr_classes) )\n",
    "\n",
    "for train_step in range(0,nr_train_steps):\n",
    "    \n",
    "    print(\"training step \", train_step)\n",
    "    \n",
    "    # 1. get the next random image from the dataset\n",
    "    image, class_id, class_name, teacher_vec = \\\n",
    "        my_image_provider.get_random_image()\n",
    "        \n",
    "    # 2. put the image into a 4D array\n",
    "    #    note: Keras expects a 4D array as input for\n",
    "    #          the training function fit()\n",
    "    X[0,:,:,:] = image\n",
    "    \n",
    "    # 3. the teacher value array expected by Keras\n",
    "    #    is a 2D array\n",
    "    Y[0,:] = teacher_vec\n",
    "    \n",
    "        \n",
    "    # 3. train the model using this image    \n",
    "    my_cnn.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
