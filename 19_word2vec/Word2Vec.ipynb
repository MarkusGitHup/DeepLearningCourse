{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "This Jupyter notebook is originally from [Kavita Ganesan](https://github.com/kavgan/nlp-text-mining-working-examples/tree/master/word2vec) and was adapted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with Word2Vec in Gensim and making it work!\n",
    "\n",
    "The idea behind Word2Vec is pretty simple. We are making and assumption that you can tell the meaning of a word by the company it keeps. This is analogous to the saying *show me your friends, and I'll tell who you are*. So if you have two words that have very similar neighbors (i.e. the usage context is about the same), then these words are probably quite similar in meaning or are at least highly related. For example, the words `shocked`,`appalled` and `astonished` are typically used in a similar context. \n",
    "\n",
    "In this tutorial, you will learn how to use the Gensim implementation of Word2Vec and actually get it to work! I have heard a lot of complaints about poor performance etc, but its really a combination of two things, (1) your input data and (2) your parameter settings. Note that the training algorithms in this package were ported from the [original Word2Vec implementation by Google](https://arxiv.org/pdf/1301.3781.pdf) and extended with additional functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and logging\n",
    "\n",
    "First, we start with our imports and get logging established:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Juergen Brauer\\AppData\\Local\\conda\\conda\\envs\\env_mss\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# imports needed and set up logging\n",
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset \n",
    "Next, is our dataset. The secret to getting Word2Vec really working for you is to have lots and lots of text data. In this case I am going to use data from the [OpinRank](http://kavita-ganesan.com/entity-ranking-data/) dataset. This dataset has full user reviews of cars and hotels. I have specifically concatenated all of the hotel reviews into one big file which is about 97MB compressed and 229MB uncompressed. We will use the compressed file for this tutorial. Each line in this file represents a hotel review. You can download the OpinRank Word2Vec dataset here.\n",
    "\n",
    "To avoid confusion, while gensimâ€™s word2vec tutorial says that you need to pass it a sequence of sentences as its input, you can always pass it a whole review as a sentence (i.e. a much larger size of text), and it should not make much of a difference. \n",
    "\n",
    "Now, let's take a closer look at this data below by printing the first line. You can see that this is a pretty hefty review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Oct 12 2009 \\tNice trendy hotel location not too bad.\\tI stayed in this hotel for one night. As this is a fairly new place some of the taxi drivers did not know where it was and/or did not want to drive there. Once I have eventually arrived at the hotel, I was very pleasantly surprised with the decor of the lobby/ground floor area. It was very stylish and modern. I found the reception's staff geeting me with 'Aloha' a bit out of place, but I guess they are briefed to say that to keep up the coroporate image.As I have a Starwood Preferred Guest member, I was given a small gift upon-check in. It was only a couple of fridge magnets in a gift box, but nevertheless a nice gesture.My room was nice and roomy, there are tea and coffee facilities in each room and you get two complimentary bottles of water plus some toiletries by 'bliss'.The location is not great. It is at the last metro stop and you then need to take a taxi, but if you are not planning on going to see the historic sites in Beijing, then you will be ok.I chose to have some breakfast in the hotel, which was really tasty and there was a good selection of dishes. There are a couple of computers to use in the communal area, as well as a pool table. There is also a small swimming pool and a gym area.I would definitely stay in this hotel again, but only if I did not plan to travel to central Beijing, as it can take a long time. The location is ok if you plan to do a lot of shopping, as there is a big shopping centre just few minutes away from the hotel and there are plenty of eating options around, including restaurants that serve a dog meat!\\t\\r\\n\"\n",
      "b'Sep 25 2009 \\tGreat Budget Hotel!\\tStayed two nights at Aloft on the most recent trip to China. The hotel was very modern and clean. The room was spotless and a comfortable king sized bed (as far as soft beds go in China). The staff was very punctual and went out of the way to help my every need, including going to a store across the street to purchase a China Mobile SIM card for me. The buffet breakfast was okay, nothing to write home about. The 42\\x94 LCD screen had movies on demand for 20RMB and had a good selection of western channels including HBO, CNN, BBC, Star World etc\\x85 The gym was small, had a selection of basic weights and one cable machine, there was however 6 new TechnoGym cardio machines with built in LCD TVs which were very good. The location is a bit out of the way to the central areas of Beijing, but it is better suited for my needs as I need to be in the Haidian district. Being SPG Platinum there were no upgrades to a better room, because Aloft has a policy of not doing any upgrades. The Sheraton next door is a much nicer hotel in my opinion (where I am writing this from now, with an upgraded room) but as far as bang for the buck, Aloft is a great place!\\t\\r\\n'\n",
      "b\"Aug 4 2009 \\tExcellent value - location not a big problem.\\tWe stayed at the Aloft Beijing Haidian for 5 nights from July 17 - 22nd. There are lots of reviews that talk about the location being a problem but we knew this ahead of time and found that it really wasn't an issue. The longest we spent in a taxi was about 30 minutes. We never paid more than 50rmb for a taxi ride (which is about $8 Cdn) and that was to the Forbidden City. Given there are 4 in our family, it was no big deal at all.As for the rooms, they were clean, the beds comfortable, the wireless internet connection reliable and it was one of the few hotels we found in Beijing that would accomodate 2 adults and 2 children. We paid about $70 Cdn. per night. That's an amazing price.It's not meant to be a 5 star hotel so you can't go in expecting that. We found the reception staff generally very helpful and friendly. They aren't the fastest in the world but it wasn't unreasonable at all.The hotel manager made an effort to speak with us a few times and was extremely helpful and welcoming. Their breakfast buffet was quite good and reasonably priced. There are a number of good restaurants in the Four Points Sheraton next door so there were lots of options. There is a massive mall about a block away that has other dining options as well.The only issue we ran into was a few taxi drivers refusing to take us to the Summer Palace because I guess they felt it wasn't far enough. The minimum rate is 10rmb. But the staff at the Four Points Sheraton (which is on a busier road than the side street for Aloft) were quite helpful in finding taxis for us if they weren't already there.We would definitely stay there again and recommend it to our friends for its excellent value.\\t\\r\\n\"\n",
      "b\"Jul 17 2009 \\tStylish clean reasonable value poor location\\tI am glad to be the first person to post photos of this hotel. Key points about this property:- its a starwood property, much cheaper than the rest, and it shows in the pretty incompetent reception staff, the hard bed, the cheap blinds that don't quite block out the sun in the mornings, and common areas which seem to be degrading very quickly (given how new this hotel is).- on the up side, it is very clean and tidy, and if that is important to you, this hotel presents good value. It is also quite stylishly designed and looks much more modern than anything else in this price range.- location is poor for the typical tourist. Yes, very poor. It is not really close to much cheaper the summer palace, and other places will require a taxi, which will probably result in hours of being frustratingly stuck in traffic. Taxi drivers also dont know where it is. If coming from the East, (i.e. city centre or airport) then it is the right turn AFTER the Four Points hotel, which is adjacent. If you are trying to scam an SPG platinum status in the cheapest way possible, perhaps stay in these two adjacent SPG hotels for 25 nights and check out every night.On the whole I thought it was decent value, but dollar for dollar, I would personally prefer to pay double for the Westin in Chaoyang or Financial Street. I have stayed at both at approx. double the price, for real 5 star service. But if thats not in your budget and you want the assurance of an international hotel chain, this hotel is perfect for you.Lastly, this hotel may also be appropriate for university students in Haidian, but don't pick this unless you know what you are doing. Taxis to BLCU or Tsing Hua may be exceptionally frustrating because of constant traffic around Zhong guan cun street. Tell taxi drivers to go on the four ring road, even though it might be a bit more expensive.There is free internet, but it does not seem to work with iPhones.\\t\\r\\n\"\n"
     ]
    }
   ],
   "source": [
    "data_file=\"reviews_data.txt.gz\"\n",
    "\n",
    "nr_reviews_to_show = 3\n",
    "\n",
    "with gzip.open ('reviews_data.txt.gz', 'rb') as f:\n",
    "    for i,line in enumerate (f):\n",
    "        print(line)\n",
    "        if i==nr_reviews_to_show:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read files into a list\n",
    "Now that we've had a sneak peak of our dataset, we can read it into a list so that we can pass this on to the Word2Vec model. Notice in the code below, that I am directly reading the \n",
    "compressed file. I'm also doing a mild pre-processing of the reviews using `gensim.utils.simple_preprocess (line)`. This does some basic pre-processing such as tokenization, lowercasing, etc and returns back a list of tokens (words). Documentation of this pre-processing method can be found on the official [Gensim documentation site](https://radimrehurek.com/gensim/utils.html). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-13 19:00:39,220 : INFO : reading file reviews_data.txt.gz...this may take a while\n",
      "2019-01-13 19:00:39,222 : INFO : read 0 reviews\n",
      "2019-01-13 19:00:41,367 : INFO : read 10000 reviews\n",
      "2019-01-13 19:00:43,511 : INFO : read 20000 reviews\n",
      "2019-01-13 19:00:46,004 : INFO : read 30000 reviews\n",
      "2019-01-13 19:00:48,348 : INFO : read 40000 reviews\n",
      "2019-01-13 19:00:50,891 : INFO : read 50000 reviews\n",
      "2019-01-13 19:00:53,523 : INFO : read 60000 reviews\n",
      "2019-01-13 19:00:55,617 : INFO : read 70000 reviews\n",
      "2019-01-13 19:00:57,503 : INFO : read 80000 reviews\n",
      "2019-01-13 19:00:59,485 : INFO : read 90000 reviews\n",
      "2019-01-13 19:01:01,423 : INFO : read 100000 reviews\n",
      "2019-01-13 19:01:03,375 : INFO : read 110000 reviews\n",
      "2019-01-13 19:01:05,327 : INFO : read 120000 reviews\n",
      "2019-01-13 19:01:07,310 : INFO : read 130000 reviews\n",
      "2019-01-13 19:01:09,429 : INFO : read 140000 reviews\n",
      "2019-01-13 19:01:11,731 : INFO : read 150000 reviews\n",
      "2019-01-13 19:01:13,713 : INFO : read 160000 reviews\n",
      "2019-01-13 19:01:15,670 : INFO : read 170000 reviews\n",
      "2019-01-13 19:01:17,734 : INFO : read 180000 reviews\n",
      "2019-01-13 19:01:19,715 : INFO : read 190000 reviews\n",
      "2019-01-13 19:01:21,900 : INFO : read 200000 reviews\n",
      "2019-01-13 19:01:23,988 : INFO : read 210000 reviews\n",
      "2019-01-13 19:01:26,122 : INFO : read 220000 reviews\n",
      "2019-01-13 19:01:28,042 : INFO : read 230000 reviews\n",
      "2019-01-13 19:01:30,658 : INFO : read 240000 reviews\n",
      "2019-01-13 19:01:32,678 : INFO : read 250000 reviews\n",
      "2019-01-13 19:01:33,752 : INFO : Done reading data file\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def read_input(input_file):\n",
    "    \"\"\"This method reads the input file which is in gzip format\"\"\"\n",
    "    \n",
    "    logging.info(\"reading file {0}...this may take a while\".format(input_file))\n",
    "    \n",
    "    with gzip.open (input_file, 'rb') as f:\n",
    "        for i, line in enumerate (f): \n",
    "\n",
    "            if (i%10000==0):\n",
    "                logging.info (\"read {0} reviews\".format (i))\n",
    "            # do some pre-processing and return a list of words for each review text\n",
    "            yield gensim.utils.simple_preprocess (line)\n",
    "\n",
    "# read the tokenized reviews into a list\n",
    "# each review item becomes a serries of words\n",
    "# so this becomes a list of lists\n",
    "documents = list (read_input (data_file))\n",
    "logging.info (\"Done reading data file\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "Nr of reviews= 255404\n",
      "<class 'list'>\n",
      "Length of first review= 293\n",
      "First review as list:\n",
      "['oct', 'nice', 'trendy', 'hotel', 'location', 'not', 'too', 'bad', 'stayed', 'in', 'this', 'hotel', 'for', 'one', 'night', 'as', 'this', 'is', 'fairly', 'new', 'place', 'some', 'of', 'the', 'taxi', 'drivers', 'did', 'not', 'know', 'where', 'it', 'was', 'and', 'or', 'did', 'not', 'want', 'to', 'drive', 'there', 'once', 'have', 'eventually', 'arrived', 'at', 'the', 'hotel', 'was', 'very', 'pleasantly', 'surprised', 'with', 'the', 'decor', 'of', 'the', 'lobby', 'ground', 'floor', 'area', 'it', 'was', 'very', 'stylish', 'and', 'modern', 'found', 'the', 'reception', 'staff', 'geeting', 'me', 'with', 'aloha', 'bit', 'out', 'of', 'place', 'but', 'guess', 'they', 'are', 'briefed', 'to', 'say', 'that', 'to', 'keep', 'up', 'the', 'coroporate', 'image', 'as', 'have', 'starwood', 'preferred', 'guest', 'member', 'was', 'given', 'small', 'gift', 'upon', 'check', 'in', 'it', 'was', 'only', 'couple', 'of', 'fridge', 'magnets', 'in', 'gift', 'box', 'but', 'nevertheless', 'nice', 'gesture', 'my', 'room', 'was', 'nice', 'and', 'roomy', 'there', 'are', 'tea', 'and', 'coffee', 'facilities', 'in', 'each', 'room', 'and', 'you', 'get', 'two', 'complimentary', 'bottles', 'of', 'water', 'plus', 'some', 'toiletries', 'by', 'bliss', 'the', 'location', 'is', 'not', 'great', 'it', 'is', 'at', 'the', 'last', 'metro', 'stop', 'and', 'you', 'then', 'need', 'to', 'take', 'taxi', 'but', 'if', 'you', 'are', 'not', 'planning', 'on', 'going', 'to', 'see', 'the', 'historic', 'sites', 'in', 'beijing', 'then', 'you', 'will', 'be', 'ok', 'chose', 'to', 'have', 'some', 'breakfast', 'in', 'the', 'hotel', 'which', 'was', 'really', 'tasty', 'and', 'there', 'was', 'good', 'selection', 'of', 'dishes', 'there', 'are', 'couple', 'of', 'computers', 'to', 'use', 'in', 'the', 'communal', 'area', 'as', 'well', 'as', 'pool', 'table', 'there', 'is', 'also', 'small', 'swimming', 'pool', 'and', 'gym', 'area', 'would', 'definitely', 'stay', 'in', 'this', 'hotel', 'again', 'but', 'only', 'if', 'did', 'not', 'plan', 'to', 'travel', 'to', 'central', 'beijing', 'as', 'it', 'can', 'take', 'long', 'time', 'the', 'location', 'is', 'ok', 'if', 'you', 'plan', 'to', 'do', 'lot', 'of', 'shopping', 'as', 'there', 'is', 'big', 'shopping', 'centre', 'just', 'few', 'minutes', 'away', 'from', 'the', 'hotel', 'and', 'there', 'are', 'plenty', 'of', 'eating', 'options', 'around', 'including', 'restaurants', 'that', 'serve', 'dog', 'meat']\n"
     ]
    }
   ],
   "source": [
    "print(type(documents))\n",
    "print(\"Nr of reviews=\",len(documents))\n",
    "\n",
    "print(type(documents[0]))\n",
    "print(\"Length of first review=\",len(documents[0]))\n",
    "\n",
    "print(\"First review as list:\")\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Word2Vec model\n",
    "\n",
    "Training the model is fairly straightforward. You just instantiate Word2Vec and pass the reviews that we read in the previous step (the `documents`). So, we are essentially passing on a list of lists. Where each list within the main list contains a set of tokens from a user review. Word2Vec uses all these tokens to internally create a vocabulary. And by vocabulary, I mean a set of unique words.\n",
    "\n",
    "After building the vocabulary, we just need to call `train(...)` to start training the Word2Vec model. Training on the [OpinRank](http://kavita-ganesan.com/entity-ranking-data/) dataset takes about 10 minutes so please be patient while running your code on this dataset.\n",
    "\n",
    "Behind the scenes we are actually training a simple neural network with a single hidden layer. But, we are actually not going to use the neural network after training. Instead, the goal is to learn the weights of the hidden layer. These weights are essentially the word vectors that weâ€™re trying to learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-13 19:03:32,845 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2019-01-13 19:03:32,847 : INFO : collecting all words and their counts\n",
      "2019-01-13 19:03:32,848 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-01-13 19:03:33,126 : INFO : PROGRESS: at sentence #10000, processed 1655714 words, keeping 25777 word types\n",
      "2019-01-13 19:03:33,405 : INFO : PROGRESS: at sentence #20000, processed 3317863 words, keeping 35016 word types\n",
      "2019-01-13 19:03:33,723 : INFO : PROGRESS: at sentence #30000, processed 5264072 words, keeping 47518 word types\n",
      "2019-01-13 19:03:34,017 : INFO : PROGRESS: at sentence #40000, processed 7081746 words, keeping 56675 word types\n",
      "2019-01-13 19:03:34,343 : INFO : PROGRESS: at sentence #50000, processed 9089491 words, keeping 63744 word types\n",
      "2019-01-13 19:03:34,655 : INFO : PROGRESS: at sentence #60000, processed 11013723 words, keeping 76780 word types\n",
      "2019-01-13 19:03:34,924 : INFO : PROGRESS: at sentence #70000, processed 12637525 words, keeping 83193 word types\n",
      "2019-01-13 19:03:35,172 : INFO : PROGRESS: at sentence #80000, processed 14099751 words, keeping 88453 word types\n",
      "2019-01-13 19:03:35,441 : INFO : PROGRESS: at sentence #90000, processed 15662149 words, keeping 93351 word types\n",
      "2019-01-13 19:03:35,687 : INFO : PROGRESS: at sentence #100000, processed 17164487 words, keeping 97880 word types\n",
      "2019-01-13 19:03:35,956 : INFO : PROGRESS: at sentence #110000, processed 18652292 words, keeping 102126 word types\n",
      "2019-01-13 19:03:36,239 : INFO : PROGRESS: at sentence #120000, processed 20152529 words, keeping 105917 word types\n",
      "2019-01-13 19:03:36,522 : INFO : PROGRESS: at sentence #130000, processed 21684330 words, keeping 110098 word types\n",
      "2019-01-13 19:03:36,820 : INFO : PROGRESS: at sentence #140000, processed 23330206 words, keeping 114102 word types\n",
      "2019-01-13 19:03:37,097 : INFO : PROGRESS: at sentence #150000, processed 24838754 words, keeping 118168 word types\n",
      "2019-01-13 19:03:37,398 : INFO : PROGRESS: at sentence #160000, processed 26390910 words, keeping 118664 word types\n",
      "2019-01-13 19:03:37,655 : INFO : PROGRESS: at sentence #170000, processed 27913916 words, keeping 123349 word types\n",
      "2019-01-13 19:03:37,950 : INFO : PROGRESS: at sentence #180000, processed 29535612 words, keeping 126741 word types\n",
      "2019-01-13 19:03:38,245 : INFO : PROGRESS: at sentence #190000, processed 31096459 words, keeping 129840 word types\n",
      "2019-01-13 19:03:38,530 : INFO : PROGRESS: at sentence #200000, processed 32805271 words, keeping 133248 word types\n",
      "2019-01-13 19:03:38,801 : INFO : PROGRESS: at sentence #210000, processed 34434198 words, keeping 136357 word types\n",
      "2019-01-13 19:03:39,072 : INFO : PROGRESS: at sentence #220000, processed 36083482 words, keeping 139411 word types\n",
      "2019-01-13 19:03:39,317 : INFO : PROGRESS: at sentence #230000, processed 37571762 words, keeping 142392 word types\n",
      "2019-01-13 19:03:39,575 : INFO : PROGRESS: at sentence #240000, processed 39138190 words, keeping 145225 word types\n",
      "2019-01-13 19:03:39,852 : INFO : PROGRESS: at sentence #250000, processed 40695049 words, keeping 147959 word types\n",
      "2019-01-13 19:03:40,024 : INFO : collected 150052 word types from a corpus of 41519355 raw words and 255404 sentences\n",
      "2019-01-13 19:03:40,026 : INFO : Loading a fresh vocabulary\n",
      "2019-01-13 19:03:40,241 : INFO : min_count=2 retains 70538 unique words (47% of original 150052, drops 79514)\n",
      "2019-01-13 19:03:40,242 : INFO : min_count=2 leaves 41439841 word corpus (99% of original 41519355, drops 79514)\n",
      "2019-01-13 19:03:40,444 : INFO : deleting the raw counts dictionary of 150052 items\n",
      "2019-01-13 19:03:40,454 : INFO : sample=0.001 downsamples 55 most-common words\n",
      "2019-01-13 19:03:40,455 : INFO : downsampling leaves estimated 30349257 word corpus (73.2% of prior 41439841)\n",
      "2019-01-13 19:03:40,455 : INFO : estimated required memory for 70538 words and 150 dimensions: 119914600 bytes\n",
      "2019-01-13 19:03:40,749 : INFO : resetting layer weights\n",
      "2019-01-13 19:03:41,547 : INFO : training model with 10 workers on 70538 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-01-13 19:03:42,554 : INFO : PROGRESS: at 0.94% examples, 1448611 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:03:43,567 : INFO : PROGRESS: at 1.86% examples, 1441304 words/s, in_qsize 16, out_qsize 3\n",
      "2019-01-13 19:03:44,572 : INFO : PROGRESS: at 2.65% examples, 1445127 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:03:45,573 : INFO : PROGRESS: at 3.46% examples, 1440654 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:03:46,587 : INFO : PROGRESS: at 4.15% examples, 1412557 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:03:47,593 : INFO : PROGRESS: at 4.93% examples, 1406414 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:03:48,607 : INFO : PROGRESS: at 5.87% examples, 1385449 words/s, in_qsize 16, out_qsize 3\n",
      "2019-01-13 19:03:49,623 : INFO : PROGRESS: at 6.74% examples, 1363884 words/s, in_qsize 15, out_qsize 4\n",
      "2019-01-13 19:03:50,627 : INFO : PROGRESS: at 7.59% examples, 1346546 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:03:51,628 : INFO : PROGRESS: at 8.52% examples, 1340839 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:03:52,643 : INFO : PROGRESS: at 9.52% examples, 1344792 words/s, in_qsize 16, out_qsize 3\n",
      "2019-01-13 19:03:53,654 : INFO : PROGRESS: at 10.40% examples, 1337701 words/s, in_qsize 16, out_qsize 3\n",
      "2019-01-13 19:03:54,660 : INFO : PROGRESS: at 11.33% examples, 1339206 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:03:55,662 : INFO : PROGRESS: at 12.28% examples, 1342119 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:03:56,663 : INFO : PROGRESS: at 13.28% examples, 1347492 words/s, in_qsize 15, out_qsize 4\n",
      "2019-01-13 19:03:57,679 : INFO : PROGRESS: at 14.17% examples, 1346962 words/s, in_qsize 15, out_qsize 4\n",
      "2019-01-13 19:03:58,687 : INFO : PROGRESS: at 15.15% examples, 1351800 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:03:59,691 : INFO : PROGRESS: at 15.97% examples, 1348003 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:04:00,699 : INFO : PROGRESS: at 16.70% examples, 1334960 words/s, in_qsize 20, out_qsize 0\n",
      "2019-01-13 19:04:01,703 : INFO : PROGRESS: at 17.54% examples, 1330258 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:04:02,704 : INFO : PROGRESS: at 18.42% examples, 1326008 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:04:03,704 : INFO : PROGRESS: at 19.33% examples, 1326727 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:04:04,712 : INFO : PROGRESS: at 20.10% examples, 1317182 words/s, in_qsize 20, out_qsize 1\n",
      "2019-01-13 19:04:05,720 : INFO : PROGRESS: at 21.02% examples, 1320697 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:04:06,729 : INFO : PROGRESS: at 21.88% examples, 1321965 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:04:07,732 : INFO : PROGRESS: at 22.59% examples, 1322626 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:04:08,734 : INFO : PROGRESS: at 23.42% examples, 1326881 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:04:09,741 : INFO : PROGRESS: at 24.16% examples, 1329666 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:04:10,741 : INFO : PROGRESS: at 24.98% examples, 1333037 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:04:11,754 : INFO : PROGRESS: at 26.02% examples, 1335896 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:04:12,756 : INFO : PROGRESS: at 27.02% examples, 1338358 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:04:13,756 : INFO : PROGRESS: at 28.02% examples, 1340431 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:04:14,774 : INFO : PROGRESS: at 29.00% examples, 1339720 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:04:15,780 : INFO : PROGRESS: at 29.96% examples, 1340776 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:04:16,788 : INFO : PROGRESS: at 30.90% examples, 1342491 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:04:17,801 : INFO : PROGRESS: at 31.90% examples, 1344223 words/s, in_qsize 20, out_qsize 1\n",
      "2019-01-13 19:04:18,819 : INFO : PROGRESS: at 32.89% examples, 1344594 words/s, in_qsize 16, out_qsize 3\n",
      "2019-01-13 19:04:19,837 : INFO : PROGRESS: at 33.83% examples, 1345813 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:04:20,841 : INFO : PROGRESS: at 34.77% examples, 1346994 words/s, in_qsize 16, out_qsize 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-13 19:04:21,857 : INFO : PROGRESS: at 35.65% examples, 1347868 words/s, in_qsize 20, out_qsize 2\n",
      "2019-01-13 19:04:22,862 : INFO : PROGRESS: at 36.60% examples, 1349421 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:04:23,865 : INFO : PROGRESS: at 37.53% examples, 1350248 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:04:24,883 : INFO : PROGRESS: at 38.48% examples, 1349722 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:04:25,883 : INFO : PROGRESS: at 39.43% examples, 1350815 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:04:26,890 : INFO : PROGRESS: at 40.36% examples, 1351383 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:04:27,898 : INFO : PROGRESS: at 41.27% examples, 1352143 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:04:28,905 : INFO : PROGRESS: at 42.10% examples, 1352937 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:04:29,914 : INFO : PROGRESS: at 42.88% examples, 1353523 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:04:30,919 : INFO : PROGRESS: at 43.54% examples, 1350102 words/s, in_qsize 20, out_qsize 3\n",
      "2019-01-13 19:04:31,930 : INFO : PROGRESS: at 44.33% examples, 1350429 words/s, in_qsize 20, out_qsize 0\n",
      "2019-01-13 19:04:32,935 : INFO : PROGRESS: at 45.10% examples, 1351029 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:04:33,939 : INFO : PROGRESS: at 46.10% examples, 1351458 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:04:34,942 : INFO : PROGRESS: at 47.06% examples, 1351751 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:04:35,960 : INFO : PROGRESS: at 48.05% examples, 1351904 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:04:36,975 : INFO : PROGRESS: at 49.06% examples, 1351961 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:04:37,976 : INFO : PROGRESS: at 49.99% examples, 1351860 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:04:38,984 : INFO : PROGRESS: at 50.90% examples, 1352075 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:04:39,997 : INFO : PROGRESS: at 51.80% examples, 1350529 words/s, in_qsize 20, out_qsize 1\n",
      "2019-01-13 19:04:41,004 : INFO : PROGRESS: at 52.76% examples, 1350676 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:04:42,014 : INFO : PROGRESS: at 53.68% examples, 1350315 words/s, in_qsize 16, out_qsize 3\n",
      "2019-01-13 19:04:43,024 : INFO : PROGRESS: at 54.55% examples, 1349591 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:04:44,026 : INFO : PROGRESS: at 55.45% examples, 1350072 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:04:45,035 : INFO : PROGRESS: at 56.34% examples, 1350262 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:04:46,055 : INFO : PROGRESS: at 57.24% examples, 1350188 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:04:47,057 : INFO : PROGRESS: at 58.20% examples, 1349958 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:04:48,067 : INFO : PROGRESS: at 59.11% examples, 1349540 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:04:49,070 : INFO : PROGRESS: at 60.03% examples, 1349232 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:04:50,073 : INFO : PROGRESS: at 60.92% examples, 1349585 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:04:51,080 : INFO : PROGRESS: at 61.79% examples, 1349681 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:04:52,086 : INFO : PROGRESS: at 62.51% examples, 1349639 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:04:53,089 : INFO : PROGRESS: at 63.31% examples, 1349881 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:04:54,089 : INFO : PROGRESS: at 64.01% examples, 1349364 words/s, in_qsize 20, out_qsize 1\n",
      "2019-01-13 19:04:55,093 : INFO : PROGRESS: at 64.77% examples, 1349492 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:04:56,104 : INFO : PROGRESS: at 65.71% examples, 1349396 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:04:57,107 : INFO : PROGRESS: at 66.68% examples, 1349421 words/s, in_qsize 16, out_qsize 3\n",
      "2019-01-13 19:04:58,115 : INFO : PROGRESS: at 67.58% examples, 1348713 words/s, in_qsize 16, out_qsize 3\n",
      "2019-01-13 19:04:59,125 : INFO : PROGRESS: at 68.57% examples, 1348784 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:05:00,146 : INFO : PROGRESS: at 69.53% examples, 1348403 words/s, in_qsize 15, out_qsize 4\n",
      "2019-01-13 19:05:01,164 : INFO : PROGRESS: at 70.48% examples, 1348431 words/s, in_qsize 16, out_qsize 3\n",
      "2019-01-13 19:05:02,180 : INFO : PROGRESS: at 71.42% examples, 1348574 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:05:03,182 : INFO : PROGRESS: at 72.34% examples, 1348415 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:05:04,195 : INFO : PROGRESS: at 73.29% examples, 1348349 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:05:05,202 : INFO : PROGRESS: at 74.18% examples, 1348198 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:05:06,205 : INFO : PROGRESS: at 75.10% examples, 1348345 words/s, in_qsize 20, out_qsize 0\n",
      "2019-01-13 19:05:07,204 : INFO : PROGRESS: at 75.94% examples, 1347960 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:05:08,208 : INFO : PROGRESS: at 76.82% examples, 1347812 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:05:09,216 : INFO : PROGRESS: at 77.76% examples, 1347848 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:05:10,217 : INFO : PROGRESS: at 78.67% examples, 1347618 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:05:11,225 : INFO : PROGRESS: at 79.62% examples, 1347757 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:05:12,245 : INFO : PROGRESS: at 80.52% examples, 1347638 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:05:13,256 : INFO : PROGRESS: at 81.42% examples, 1347620 words/s, in_qsize 16, out_qsize 3\n",
      "2019-01-13 19:05:14,258 : INFO : PROGRESS: at 82.20% examples, 1347753 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:05:15,278 : INFO : PROGRESS: at 82.93% examples, 1346979 words/s, in_qsize 20, out_qsize 2\n",
      "2019-01-13 19:05:16,281 : INFO : PROGRESS: at 83.68% examples, 1346945 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:05:17,285 : INFO : PROGRESS: at 84.42% examples, 1346577 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:05:18,293 : INFO : PROGRESS: at 85.20% examples, 1346467 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:05:19,300 : INFO : PROGRESS: at 86.09% examples, 1345212 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:05:20,307 : INFO : PROGRESS: at 86.99% examples, 1344516 words/s, in_qsize 16, out_qsize 3\n",
      "2019-01-13 19:05:21,321 : INFO : PROGRESS: at 87.72% examples, 1341086 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:05:22,328 : INFO : PROGRESS: at 88.68% examples, 1340840 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:05:23,328 : INFO : PROGRESS: at 89.58% examples, 1340274 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:05:24,336 : INFO : PROGRESS: at 90.48% examples, 1339807 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:05:25,352 : INFO : PROGRESS: at 91.39% examples, 1339486 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:05:26,359 : INFO : PROGRESS: at 92.29% examples, 1339195 words/s, in_qsize 15, out_qsize 4\n",
      "2019-01-13 19:05:27,378 : INFO : PROGRESS: at 93.21% examples, 1338572 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:05:28,377 : INFO : PROGRESS: at 94.08% examples, 1338346 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:05:29,382 : INFO : PROGRESS: at 94.96% examples, 1337780 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:05:30,388 : INFO : PROGRESS: at 95.66% examples, 1335793 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:05:31,391 : INFO : PROGRESS: at 96.54% examples, 1335727 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:05:32,406 : INFO : PROGRESS: at 97.42% examples, 1335365 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:05:33,417 : INFO : PROGRESS: at 98.37% examples, 1335306 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:05:34,418 : INFO : PROGRESS: at 99.26% examples, 1335100 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:05:35,194 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-01-13 19:05:35,200 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-01-13 19:05:35,202 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-01-13 19:05:35,213 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-01-13 19:05:35,214 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-01-13 19:05:35,216 : INFO : worker thread finished; awaiting finish of 4 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-13 19:05:35,219 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-13 19:05:35,221 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-13 19:05:35,225 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-13 19:05:35,229 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-13 19:05:35,230 : INFO : training on 207596775 raw words (151750357 effective words) took 113.7s, 1334911 effective words/s\n",
      "2019-01-13 19:05:35,232 : INFO : training model with 10 workers on 70538 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-01-13 19:05:36,256 : INFO : PROGRESS: at 0.42% examples, 1263692 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:05:37,262 : INFO : PROGRESS: at 0.84% examples, 1289570 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:05:38,268 : INFO : PROGRESS: at 1.20% examples, 1299789 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:05:39,275 : INFO : PROGRESS: at 1.59% examples, 1311688 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:05:40,281 : INFO : PROGRESS: at 1.96% examples, 1317676 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:05:41,291 : INFO : PROGRESS: at 2.33% examples, 1318265 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:05:42,294 : INFO : PROGRESS: at 2.77% examples, 1317768 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:05:43,294 : INFO : PROGRESS: at 3.24% examples, 1318806 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:05:44,301 : INFO : PROGRESS: at 3.67% examples, 1307761 words/s, in_qsize 16, out_qsize 3\n",
      "2019-01-13 19:05:45,303 : INFO : PROGRESS: at 4.13% examples, 1305807 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:05:46,317 : INFO : PROGRESS: at 4.58% examples, 1298763 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:05:47,329 : INFO : PROGRESS: at 4.96% examples, 1281237 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:05:48,339 : INFO : PROGRESS: at 5.37% examples, 1275197 words/s, in_qsize 15, out_qsize 4\n",
      "2019-01-13 19:05:49,346 : INFO : PROGRESS: at 5.74% examples, 1260009 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:05:50,367 : INFO : PROGRESS: at 6.15% examples, 1253397 words/s, in_qsize 20, out_qsize 2\n",
      "2019-01-13 19:05:51,368 : INFO : PROGRESS: at 6.52% examples, 1238724 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:05:52,378 : INFO : PROGRESS: at 6.95% examples, 1241494 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:05:53,388 : INFO : PROGRESS: at 7.39% examples, 1243531 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:05:54,391 : INFO : PROGRESS: at 7.76% examples, 1240396 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:05:55,399 : INFO : PROGRESS: at 8.14% examples, 1236956 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:05:56,406 : INFO : PROGRESS: at 8.57% examples, 1240277 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:05:57,418 : INFO : PROGRESS: at 9.04% examples, 1242910 words/s, in_qsize 20, out_qsize 1\n",
      "2019-01-13 19:05:58,421 : INFO : PROGRESS: at 9.49% examples, 1246128 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:05:59,436 : INFO : PROGRESS: at 9.95% examples, 1248715 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:06:00,448 : INFO : PROGRESS: at 10.39% examples, 1251518 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:06:01,457 : INFO : PROGRESS: at 10.79% examples, 1251384 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:06:02,475 : INFO : PROGRESS: at 11.14% examples, 1250474 words/s, in_qsize 20, out_qsize 3\n",
      "2019-01-13 19:06:03,480 : INFO : PROGRESS: at 11.50% examples, 1251069 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:06:04,498 : INFO : PROGRESS: at 11.84% examples, 1248668 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:06:05,500 : INFO : PROGRESS: at 12.19% examples, 1248709 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:06:06,514 : INFO : PROGRESS: at 12.54% examples, 1247989 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:06:07,522 : INFO : PROGRESS: at 12.99% examples, 1247921 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:06:08,529 : INFO : PROGRESS: at 13.45% examples, 1248774 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:06:09,535 : INFO : PROGRESS: at 13.86% examples, 1246406 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:06:10,537 : INFO : PROGRESS: at 14.32% examples, 1246742 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:06:11,537 : INFO : PROGRESS: at 14.75% examples, 1246143 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:06:12,567 : INFO : PROGRESS: at 15.15% examples, 1242633 words/s, in_qsize 15, out_qsize 4\n",
      "2019-01-13 19:06:13,568 : INFO : PROGRESS: at 15.55% examples, 1240680 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:06:14,575 : INFO : PROGRESS: at 15.93% examples, 1237204 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:06:15,576 : INFO : PROGRESS: at 16.32% examples, 1234174 words/s, in_qsize 20, out_qsize 1\n",
      "2019-01-13 19:06:16,591 : INFO : PROGRESS: at 16.71% examples, 1231675 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:06:17,617 : INFO : PROGRESS: at 17.07% examples, 1227535 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:06:18,619 : INFO : PROGRESS: at 17.44% examples, 1224104 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:06:19,619 : INFO : PROGRESS: at 17.80% examples, 1222297 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:06:20,624 : INFO : PROGRESS: at 18.18% examples, 1220786 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:06:21,639 : INFO : PROGRESS: at 18.56% examples, 1219507 words/s, in_qsize 20, out_qsize 0\n",
      "2019-01-13 19:06:22,650 : INFO : PROGRESS: at 18.98% examples, 1217835 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:06:23,668 : INFO : PROGRESS: at 19.38% examples, 1216374 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:06:24,670 : INFO : PROGRESS: at 19.78% examples, 1215473 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:06:25,672 : INFO : PROGRESS: at 20.20% examples, 1216108 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:06:26,689 : INFO : PROGRESS: at 20.63% examples, 1217561 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:06:27,689 : INFO : PROGRESS: at 20.98% examples, 1216889 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:06:28,693 : INFO : PROGRESS: at 21.34% examples, 1217993 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:06:29,693 : INFO : PROGRESS: at 21.71% examples, 1219830 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:06:30,694 : INFO : PROGRESS: at 22.05% examples, 1220980 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:06:31,705 : INFO : PROGRESS: at 22.42% examples, 1222252 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:06:32,712 : INFO : PROGRESS: at 22.88% examples, 1223668 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:06:33,725 : INFO : PROGRESS: at 23.34% examples, 1224311 words/s, in_qsize 16, out_qsize 3\n",
      "2019-01-13 19:06:34,726 : INFO : PROGRESS: at 23.79% examples, 1225902 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:06:35,743 : INFO : PROGRESS: at 24.26% examples, 1226515 words/s, in_qsize 16, out_qsize 3\n",
      "2019-01-13 19:06:36,752 : INFO : PROGRESS: at 24.73% examples, 1227609 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:06:37,762 : INFO : PROGRESS: at 25.19% examples, 1229068 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:06:38,769 : INFO : PROGRESS: at 25.63% examples, 1230140 words/s, in_qsize 16, out_qsize 3\n",
      "2019-01-13 19:06:39,779 : INFO : PROGRESS: at 26.05% examples, 1229732 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:06:40,783 : INFO : PROGRESS: at 26.52% examples, 1230714 words/s, in_qsize 20, out_qsize 2\n",
      "2019-01-13 19:06:41,787 : INFO : PROGRESS: at 26.95% examples, 1231892 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:06:42,790 : INFO : PROGRESS: at 27.39% examples, 1233036 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:06:43,790 : INFO : PROGRESS: at 27.79% examples, 1233615 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:06:44,799 : INFO : PROGRESS: at 28.19% examples, 1233013 words/s, in_qsize 20, out_qsize 1\n",
      "2019-01-13 19:06:45,806 : INFO : PROGRESS: at 28.59% examples, 1232925 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:06:46,811 : INFO : PROGRESS: at 29.02% examples, 1232265 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:06:47,814 : INFO : PROGRESS: at 29.43% examples, 1231752 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:06:48,817 : INFO : PROGRESS: at 29.86% examples, 1232106 words/s, in_qsize 18, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-13 19:06:49,820 : INFO : PROGRESS: at 30.30% examples, 1233230 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:06:50,839 : INFO : PROGRESS: at 30.72% examples, 1233878 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:06:51,864 : INFO : PROGRESS: at 31.10% examples, 1234673 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:06:52,872 : INFO : PROGRESS: at 31.47% examples, 1235520 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:06:53,890 : INFO : PROGRESS: at 31.84% examples, 1236482 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:06:54,894 : INFO : PROGRESS: at 32.21% examples, 1237341 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:06:55,896 : INFO : PROGRESS: at 32.58% examples, 1238033 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:06:56,899 : INFO : PROGRESS: at 33.05% examples, 1238646 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:06:57,900 : INFO : PROGRESS: at 33.50% examples, 1239284 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:06:58,902 : INFO : PROGRESS: at 33.96% examples, 1240041 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:06:59,914 : INFO : PROGRESS: at 34.44% examples, 1240368 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:07:00,915 : INFO : PROGRESS: at 34.89% examples, 1241013 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:07:01,920 : INFO : PROGRESS: at 35.31% examples, 1241395 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:07:02,932 : INFO : PROGRESS: at 35.77% examples, 1241814 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:07:03,942 : INFO : PROGRESS: at 36.21% examples, 1242375 words/s, in_qsize 16, out_qsize 3\n",
      "2019-01-13 19:07:04,945 : INFO : PROGRESS: at 36.66% examples, 1242825 words/s, in_qsize 16, out_qsize 3\n",
      "2019-01-13 19:07:05,955 : INFO : PROGRESS: at 37.10% examples, 1243512 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:07:06,957 : INFO : PROGRESS: at 37.54% examples, 1244073 words/s, in_qsize 20, out_qsize 0\n",
      "2019-01-13 19:07:07,969 : INFO : PROGRESS: at 37.96% examples, 1244610 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:07:08,973 : INFO : PROGRESS: at 38.38% examples, 1245109 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:07:09,982 : INFO : PROGRESS: at 38.82% examples, 1245353 words/s, in_qsize 20, out_qsize 1\n",
      "2019-01-13 19:07:10,984 : INFO : PROGRESS: at 39.28% examples, 1245983 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:07:11,997 : INFO : PROGRESS: at 39.72% examples, 1246288 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:07:12,999 : INFO : PROGRESS: at 40.15% examples, 1246721 words/s, in_qsize 16, out_qsize 3\n",
      "2019-01-13 19:07:14,013 : INFO : PROGRESS: at 40.50% examples, 1244721 words/s, in_qsize 20, out_qsize 0\n",
      "2019-01-13 19:07:15,022 : INFO : PROGRESS: at 40.81% examples, 1241871 words/s, in_qsize 20, out_qsize 2\n",
      "2019-01-13 19:07:16,025 : INFO : PROGRESS: at 41.10% examples, 1239900 words/s, in_qsize 16, out_qsize 3\n",
      "2019-01-13 19:07:17,038 : INFO : PROGRESS: at 41.39% examples, 1237721 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:07:18,050 : INFO : PROGRESS: at 41.69% examples, 1235575 words/s, in_qsize 20, out_qsize 2\n",
      "2019-01-13 19:07:19,071 : INFO : PROGRESS: at 41.94% examples, 1232747 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:07:20,087 : INFO : PROGRESS: at 42.23% examples, 1230237 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:07:21,094 : INFO : PROGRESS: at 42.48% examples, 1227572 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:07:22,096 : INFO : PROGRESS: at 42.86% examples, 1225453 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:07:23,124 : INFO : PROGRESS: at 43.20% examples, 1222671 words/s, in_qsize 16, out_qsize 3\n",
      "2019-01-13 19:07:24,131 : INFO : PROGRESS: at 43.53% examples, 1220106 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:07:25,164 : INFO : PROGRESS: at 43.88% examples, 1217624 words/s, in_qsize 15, out_qsize 4\n",
      "2019-01-13 19:07:26,168 : INFO : PROGRESS: at 44.24% examples, 1215492 words/s, in_qsize 20, out_qsize 0\n",
      "2019-01-13 19:07:27,172 : INFO : PROGRESS: at 44.62% examples, 1213948 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:07:28,175 : INFO : PROGRESS: at 44.97% examples, 1212144 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:07:29,184 : INFO : PROGRESS: at 45.28% examples, 1209744 words/s, in_qsize 20, out_qsize 2\n",
      "2019-01-13 19:07:30,185 : INFO : PROGRESS: at 45.63% examples, 1208007 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:07:31,185 : INFO : PROGRESS: at 46.03% examples, 1207508 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:07:32,187 : INFO : PROGRESS: at 46.44% examples, 1206996 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:07:33,187 : INFO : PROGRESS: at 46.82% examples, 1206310 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:07:34,195 : INFO : PROGRESS: at 47.21% examples, 1206144 words/s, in_qsize 20, out_qsize 1\n",
      "2019-01-13 19:07:35,203 : INFO : PROGRESS: at 47.63% examples, 1206641 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:07:36,213 : INFO : PROGRESS: at 48.04% examples, 1206976 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:07:37,223 : INFO : PROGRESS: at 48.44% examples, 1207192 words/s, in_qsize 20, out_qsize 2\n",
      "2019-01-13 19:07:38,232 : INFO : PROGRESS: at 48.90% examples, 1207771 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:07:39,238 : INFO : PROGRESS: at 49.33% examples, 1208339 words/s, in_qsize 16, out_qsize 3\n",
      "2019-01-13 19:07:40,242 : INFO : PROGRESS: at 49.78% examples, 1208889 words/s, in_qsize 16, out_qsize 3\n",
      "2019-01-13 19:07:41,250 : INFO : PROGRESS: at 50.21% examples, 1209477 words/s, in_qsize 16, out_qsize 3\n",
      "2019-01-13 19:07:42,265 : INFO : PROGRESS: at 50.63% examples, 1209966 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:07:43,273 : INFO : PROGRESS: at 50.98% examples, 1209329 words/s, in_qsize 19, out_qsize 2\n",
      "2019-01-13 19:07:44,296 : INFO : PROGRESS: at 51.28% examples, 1208662 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:07:45,301 : INFO : PROGRESS: at 51.62% examples, 1208190 words/s, in_qsize 20, out_qsize 1\n",
      "2019-01-13 19:07:46,312 : INFO : PROGRESS: at 51.93% examples, 1207696 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:07:47,313 : INFO : PROGRESS: at 52.24% examples, 1206681 words/s, in_qsize 16, out_qsize 3\n",
      "2019-01-13 19:07:48,318 : INFO : PROGRESS: at 52.56% examples, 1206001 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:07:49,347 : INFO : PROGRESS: at 52.97% examples, 1205142 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:07:50,356 : INFO : PROGRESS: at 53.37% examples, 1204419 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:07:51,358 : INFO : PROGRESS: at 53.77% examples, 1203989 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:07:52,366 : INFO : PROGRESS: at 54.19% examples, 1203549 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:07:53,369 : INFO : PROGRESS: at 54.61% examples, 1203311 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:07:54,373 : INFO : PROGRESS: at 55.05% examples, 1203733 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:07:55,381 : INFO : PROGRESS: at 55.48% examples, 1204432 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:07:56,388 : INFO : PROGRESS: at 55.92% examples, 1204791 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:07:57,410 : INFO : PROGRESS: at 56.40% examples, 1205456 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:07:58,412 : INFO : PROGRESS: at 56.84% examples, 1206196 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:07:59,430 : INFO : PROGRESS: at 57.27% examples, 1206721 words/s, in_qsize 20, out_qsize 2\n",
      "2019-01-13 19:08:00,437 : INFO : PROGRESS: at 57.70% examples, 1207336 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:08:01,450 : INFO : PROGRESS: at 58.12% examples, 1207976 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:08:02,459 : INFO : PROGRESS: at 58.55% examples, 1208573 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:08:03,476 : INFO : PROGRESS: at 59.01% examples, 1209011 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:08:04,482 : INFO : PROGRESS: at 59.46% examples, 1209560 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:08:05,493 : INFO : PROGRESS: at 59.90% examples, 1210013 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:08:06,494 : INFO : PROGRESS: at 60.32% examples, 1210493 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:08:07,510 : INFO : PROGRESS: at 60.74% examples, 1210798 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:08:08,515 : INFO : PROGRESS: at 61.07% examples, 1210433 words/s, in_qsize 19, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-13 19:08:09,545 : INFO : PROGRESS: at 61.31% examples, 1207988 words/s, in_qsize 20, out_qsize 0\n",
      "2019-01-13 19:08:10,554 : INFO : PROGRESS: at 61.60% examples, 1206599 words/s, in_qsize 20, out_qsize 1\n",
      "2019-01-13 19:08:11,561 : INFO : PROGRESS: at 61.92% examples, 1206409 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:08:12,565 : INFO : PROGRESS: at 62.26% examples, 1206505 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:08:13,586 : INFO : PROGRESS: at 62.63% examples, 1206451 words/s, in_qsize 15, out_qsize 4\n",
      "2019-01-13 19:08:14,597 : INFO : PROGRESS: at 63.09% examples, 1206731 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:08:15,602 : INFO : PROGRESS: at 63.54% examples, 1207175 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:08:16,619 : INFO : PROGRESS: at 63.96% examples, 1206976 words/s, in_qsize 16, out_qsize 3\n",
      "2019-01-13 19:08:17,620 : INFO : PROGRESS: at 64.42% examples, 1207305 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:08:18,636 : INFO : PROGRESS: at 64.86% examples, 1207384 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:08:19,641 : INFO : PROGRESS: at 65.27% examples, 1207395 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:08:20,659 : INFO : PROGRESS: at 65.67% examples, 1207042 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:08:21,672 : INFO : PROGRESS: at 66.09% examples, 1206908 words/s, in_qsize 16, out_qsize 3\n",
      "2019-01-13 19:08:22,685 : INFO : PROGRESS: at 66.52% examples, 1206894 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:08:23,690 : INFO : PROGRESS: at 66.95% examples, 1207357 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:08:24,705 : INFO : PROGRESS: at 67.39% examples, 1207747 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:08:25,720 : INFO : PROGRESS: at 67.77% examples, 1207694 words/s, in_qsize 20, out_qsize 1\n",
      "2019-01-13 19:08:26,741 : INFO : PROGRESS: at 68.11% examples, 1206543 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:08:27,753 : INFO : PROGRESS: at 68.48% examples, 1206098 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:08:28,762 : INFO : PROGRESS: at 68.86% examples, 1205319 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:08:29,782 : INFO : PROGRESS: at 69.28% examples, 1205267 words/s, in_qsize 20, out_qsize 2\n",
      "2019-01-13 19:08:30,792 : INFO : PROGRESS: at 69.70% examples, 1205280 words/s, in_qsize 20, out_qsize 2\n",
      "2019-01-13 19:08:31,798 : INFO : PROGRESS: at 70.13% examples, 1205638 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:08:32,804 : INFO : PROGRESS: at 70.55% examples, 1205998 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:08:33,808 : INFO : PROGRESS: at 70.94% examples, 1206179 words/s, in_qsize 20, out_qsize 3\n",
      "2019-01-13 19:08:34,840 : INFO : PROGRESS: at 71.25% examples, 1205927 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:08:35,874 : INFO : PROGRESS: at 71.59% examples, 1205332 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:08:36,878 : INFO : PROGRESS: at 71.92% examples, 1205392 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:08:37,884 : INFO : PROGRESS: at 72.28% examples, 1205660 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:08:38,886 : INFO : PROGRESS: at 72.67% examples, 1206029 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:08:39,888 : INFO : PROGRESS: at 73.11% examples, 1206057 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:08:40,890 : INFO : PROGRESS: at 73.48% examples, 1205257 words/s, in_qsize 16, out_qsize 3\n",
      "2019-01-13 19:08:41,891 : INFO : PROGRESS: at 73.91% examples, 1205317 words/s, in_qsize 16, out_qsize 3\n",
      "2019-01-13 19:08:42,901 : INFO : PROGRESS: at 74.35% examples, 1205434 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:08:43,903 : INFO : PROGRESS: at 74.78% examples, 1205373 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:08:44,905 : INFO : PROGRESS: at 75.18% examples, 1205109 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:08:45,912 : INFO : PROGRESS: at 75.57% examples, 1204765 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:08:46,933 : INFO : PROGRESS: at 75.97% examples, 1204415 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:08:47,936 : INFO : PROGRESS: at 76.37% examples, 1204010 words/s, in_qsize 20, out_qsize 0\n",
      "2019-01-13 19:08:48,945 : INFO : PROGRESS: at 76.76% examples, 1203621 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:08:49,958 : INFO : PROGRESS: at 77.05% examples, 1202024 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:08:50,966 : INFO : PROGRESS: at 77.31% examples, 1199664 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:08:51,973 : INFO : PROGRESS: at 77.66% examples, 1199081 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:08:52,985 : INFO : PROGRESS: at 78.02% examples, 1198595 words/s, in_qsize 20, out_qsize 0\n",
      "2019-01-13 19:08:53,996 : INFO : PROGRESS: at 78.37% examples, 1197821 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:08:55,019 : INFO : PROGRESS: at 78.76% examples, 1197378 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:08:56,025 : INFO : PROGRESS: at 79.17% examples, 1197204 words/s, in_qsize 20, out_qsize 0\n",
      "2019-01-13 19:08:57,051 : INFO : PROGRESS: at 79.56% examples, 1196740 words/s, in_qsize 20, out_qsize 2\n",
      "2019-01-13 19:08:58,073 : INFO : PROGRESS: at 79.96% examples, 1196458 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:08:59,092 : INFO : PROGRESS: at 80.33% examples, 1196000 words/s, in_qsize 20, out_qsize 3\n",
      "2019-01-13 19:09:00,099 : INFO : PROGRESS: at 80.70% examples, 1195679 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:09:01,102 : INFO : PROGRESS: at 81.02% examples, 1195246 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:09:02,113 : INFO : PROGRESS: at 81.34% examples, 1195006 words/s, in_qsize 20, out_qsize 0\n",
      "2019-01-13 19:09:03,119 : INFO : PROGRESS: at 81.68% examples, 1194902 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:09:04,135 : INFO : PROGRESS: at 81.99% examples, 1194669 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:09:05,145 : INFO : PROGRESS: at 82.32% examples, 1194555 words/s, in_qsize 20, out_qsize 0\n",
      "2019-01-13 19:09:06,157 : INFO : PROGRESS: at 82.70% examples, 1194431 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:09:07,181 : INFO : PROGRESS: at 83.05% examples, 1193217 words/s, in_qsize 16, out_qsize 3\n",
      "2019-01-13 19:09:08,184 : INFO : PROGRESS: at 83.38% examples, 1192065 words/s, in_qsize 20, out_qsize 3\n",
      "2019-01-13 19:09:09,192 : INFO : PROGRESS: at 83.75% examples, 1191381 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:09:10,201 : INFO : PROGRESS: at 84.13% examples, 1190547 words/s, in_qsize 20, out_qsize 0\n",
      "2019-01-13 19:09:11,213 : INFO : PROGRESS: at 84.52% examples, 1189972 words/s, in_qsize 20, out_qsize 0\n",
      "2019-01-13 19:09:12,215 : INFO : PROGRESS: at 84.87% examples, 1189128 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:09:13,227 : INFO : PROGRESS: at 85.20% examples, 1188130 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:09:14,228 : INFO : PROGRESS: at 85.56% examples, 1187447 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:09:15,232 : INFO : PROGRESS: at 85.87% examples, 1186196 words/s, in_qsize 20, out_qsize 1\n",
      "2019-01-13 19:09:16,253 : INFO : PROGRESS: at 86.23% examples, 1185440 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:09:17,263 : INFO : PROGRESS: at 86.59% examples, 1184653 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:09:18,264 : INFO : PROGRESS: at 86.93% examples, 1183748 words/s, in_qsize 16, out_qsize 3\n",
      "2019-01-13 19:09:19,264 : INFO : PROGRESS: at 87.26% examples, 1183064 words/s, in_qsize 20, out_qsize 3\n",
      "2019-01-13 19:09:20,266 : INFO : PROGRESS: at 87.62% examples, 1182567 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:09:21,288 : INFO : PROGRESS: at 87.98% examples, 1182155 words/s, in_qsize 20, out_qsize 1\n",
      "2019-01-13 19:09:22,297 : INFO : PROGRESS: at 88.36% examples, 1181946 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:09:23,305 : INFO : PROGRESS: at 88.71% examples, 1181357 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:09:24,309 : INFO : PROGRESS: at 89.08% examples, 1180787 words/s, in_qsize 16, out_qsize 3\n",
      "2019-01-13 19:09:25,330 : INFO : PROGRESS: at 89.48% examples, 1180627 words/s, in_qsize 20, out_qsize 3\n",
      "2019-01-13 19:09:26,346 : INFO : PROGRESS: at 89.88% examples, 1180456 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:09:27,358 : INFO : PROGRESS: at 90.26% examples, 1180316 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:09:28,370 : INFO : PROGRESS: at 90.64% examples, 1180138 words/s, in_qsize 18, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-13 19:09:29,385 : INFO : PROGRESS: at 90.90% examples, 1178514 words/s, in_qsize 20, out_qsize 2\n",
      "2019-01-13 19:09:30,399 : INFO : PROGRESS: at 91.13% examples, 1177093 words/s, in_qsize 20, out_qsize 0\n",
      "2019-01-13 19:09:31,410 : INFO : PROGRESS: at 91.44% examples, 1176747 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:09:32,421 : INFO : PROGRESS: at 91.76% examples, 1176600 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:09:33,440 : INFO : PROGRESS: at 92.07% examples, 1176529 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:09:34,450 : INFO : PROGRESS: at 92.41% examples, 1176453 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:09:35,464 : INFO : PROGRESS: at 92.82% examples, 1176380 words/s, in_qsize 20, out_qsize 0\n",
      "2019-01-13 19:09:36,472 : INFO : PROGRESS: at 93.21% examples, 1175919 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:09:37,484 : INFO : PROGRESS: at 93.50% examples, 1174530 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:09:38,485 : INFO : PROGRESS: at 93.84% examples, 1173690 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:09:39,505 : INFO : PROGRESS: at 94.20% examples, 1172779 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:09:40,513 : INFO : PROGRESS: at 94.55% examples, 1171839 words/s, in_qsize 20, out_qsize 0\n",
      "2019-01-13 19:09:41,520 : INFO : PROGRESS: at 94.84% examples, 1170556 words/s, in_qsize 20, out_qsize 1\n",
      "2019-01-13 19:09:42,525 : INFO : PROGRESS: at 95.12% examples, 1169084 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:09:43,541 : INFO : PROGRESS: at 95.44% examples, 1168244 words/s, in_qsize 20, out_qsize 1\n",
      "2019-01-13 19:09:44,561 : INFO : PROGRESS: at 95.76% examples, 1167063 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:09:45,586 : INFO : PROGRESS: at 96.09% examples, 1166155 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:09:46,601 : INFO : PROGRESS: at 96.45% examples, 1165328 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:09:47,625 : INFO : PROGRESS: at 96.69% examples, 1163580 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:09:48,626 : INFO : PROGRESS: at 96.97% examples, 1162222 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:09:49,643 : INFO : PROGRESS: at 97.29% examples, 1161348 words/s, in_qsize 20, out_qsize 1\n",
      "2019-01-13 19:09:50,658 : INFO : PROGRESS: at 97.60% examples, 1160429 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:09:51,673 : INFO : PROGRESS: at 97.91% examples, 1159618 words/s, in_qsize 19, out_qsize 0\n",
      "2019-01-13 19:09:52,681 : INFO : PROGRESS: at 98.21% examples, 1158684 words/s, in_qsize 16, out_qsize 3\n",
      "2019-01-13 19:09:53,707 : INFO : PROGRESS: at 98.54% examples, 1157980 words/s, in_qsize 19, out_qsize 1\n",
      "2019-01-13 19:09:54,724 : INFO : PROGRESS: at 98.91% examples, 1157449 words/s, in_qsize 18, out_qsize 1\n",
      "2019-01-13 19:09:55,748 : INFO : PROGRESS: at 99.20% examples, 1156088 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:09:56,753 : INFO : PROGRESS: at 99.56% examples, 1155678 words/s, in_qsize 16, out_qsize 3\n",
      "2019-01-13 19:09:57,753 : INFO : PROGRESS: at 99.91% examples, 1155200 words/s, in_qsize 17, out_qsize 2\n",
      "2019-01-13 19:09:57,910 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-01-13 19:09:57,917 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-01-13 19:09:57,925 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-01-13 19:09:57,927 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-01-13 19:09:57,929 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-01-13 19:09:57,938 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-01-13 19:09:57,947 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-13 19:09:57,948 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-13 19:09:57,950 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-13 19:09:57,951 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-13 19:09:57,952 : INFO : training on 415193550 raw words (303498017 effective words) took 262.7s, 1155246 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "303498017"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec (documents, size=150, window=10, min_count=2, workers=10)\n",
    "model.train(documents,total_examples=len(documents),epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's look at some output \n",
    "This first example shows a simple case of looking up words similar to the word `dirty`. All we need to do here is to call the `most_similar` function and provide the word `dirty` as the positive example. This returns the top 10 similar words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-13 19:10:38,982 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('filthy', 0.8666520118713379),\n",
       " ('stained', 0.7880571484565735),\n",
       " ('unclean', 0.7774798274040222),\n",
       " ('grubby', 0.7669922113418579),\n",
       " ('dusty', 0.7632491588592529),\n",
       " ('smelly', 0.748940110206604),\n",
       " ('dingy', 0.7375096678733826),\n",
       " ('gross', 0.727153480052948),\n",
       " ('disgusting', 0.7210978269577026),\n",
       " ('mouldy', 0.7186775803565979)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "w1 = \"dirty\"\n",
    "model.wv.most_similar (positive=w1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good, right? Let's look at a few more. Let's look at similarity for `polite`, `france` and `shocked`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('courteous', 0.9224385619163513),\n",
       " ('friendly', 0.8283950090408325),\n",
       " ('cordial', 0.8119661808013916),\n",
       " ('professional', 0.789492130279541),\n",
       " ('attentive', 0.7806856632232666),\n",
       " ('curteous', 0.7668532729148865)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look up top 6 words similar to 'polite'\n",
    "w1 = [\"polite\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('canada', 0.6461841464042664),\n",
       " ('spain', 0.6417020559310913),\n",
       " ('germany', 0.6397117376327515),\n",
       " ('lisbon', 0.6203882098197937),\n",
       " ('barcelona', 0.6153843402862549),\n",
       " ('mexico', 0.610442042350769)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look up top 6 words similar to 'france'\n",
    "w1 = [\"france\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('horrified', 0.7834050059318542),\n",
       " ('amazed', 0.78287273645401),\n",
       " ('astonished', 0.7745355367660522),\n",
       " ('dismayed', 0.7672185301780701),\n",
       " ('stunned', 0.7611684799194336),\n",
       " ('appalled', 0.7562318444252014)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look up top 6 words similar to 'shocked'\n",
    "w1 = [\"shocked\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's, nice. You can even specify several positive examples to get things that are related in the provided context and provide negative examples to say what should not be considered as related. In the example below we are asking for all items that *relate to bed* only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('duvet', 0.7317785024642944),\n",
       " ('matress', 0.7093439102172852),\n",
       " ('mattress', 0.7077168226242065),\n",
       " ('blanket', 0.7019851207733154),\n",
       " ('quilt', 0.7019296884536743),\n",
       " ('pillowcase', 0.6634088754653931),\n",
       " ('foam', 0.6375822424888611),\n",
       " ('pillows', 0.6329945921897888),\n",
       " ('sheets', 0.6327593922615051),\n",
       " ('comforter', 0.6164166331291199)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get everything related to stuff on the bed\n",
    "w1 = [\"bed\",'sheet','pillow']\n",
    "w2 = ['couch']\n",
    "model.wv.most_similar (positive=w1,negative=w2,topn=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity between two words in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even use the Word2Vec model to return the similarity between two words that are present in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7489401437293669"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity between two different words\n",
    "model.wv.similarity(w1=\"dirty\",w2=\"smelly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity between two identical words\n",
    "model.wv.similarity(w1=\"dirty\",w2=\"dirty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2795017074758513"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity between two unrelated words\n",
    "model.wv.similarity(w1=\"dirty\",w2=\"clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, the above three snippets computes the cosine similarity between the two specified words using word vectors of each. From the scores, it makes sense that `dirty` is highly similar to `smelly` but `dirty` is dissimilar to `clean`. If you do a similarity between two identical words, the score will be 1.0 as the range of the cosine similarity score will always be between [0.0-1.0]. You can read more about cosine similarity scoring [here](https://en.wikipedia.org/wiki/Cosine_similarity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the odd one out\n",
    "You can even use Word2Vec to find odd items given a list of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'france'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which one is the odd one out in this list?\n",
    "model.wv.doesnt_match([\"cat\",\"dog\",\"france\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shower'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which one is the odd one out in this list?\n",
    "model.wv.doesnt_match([\"bed\",\"pillow\",\"duvet\",\"shower\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding some of the parameters\n",
    "To train the model earlier, we had to set some parameters. Now, let's try to understand what some of them mean. For reference, this is the command that we used to train the model.\n",
    "\n",
    "```\n",
    "model = gensim.models.Word2Vec (documents, size=150, window=10, min_count=2, workers=10)\n",
    "```\n",
    "\n",
    "### `size`\n",
    "The size of the dense vector to represent each token or word. If you have very limited data, then size should be a much smaller value. If you have lots of data, its good to experiment with various sizes. A value of 100-150 has worked well for me. \n",
    "\n",
    "### `window`\n",
    "The maximum distance between the target word and its neighboring word. If your neighbor's position is greater than the maximum window width to the left and the right, then, some neighbors are not considered as being related to the target word. In theory, a smaller window should give you terms that are more related. If you have lots of data, then the window size should not matter too much, as long as its a decent sized window. \n",
    "\n",
    "### `min_count`\n",
    "Minimium frequency count of words. The model would ignore words that do not statisfy the `min_count`. Extremely infrequent words are usually unimportant, so its best to get rid of those. Unless your dataset is really tiny, this does not really affect the model.\n",
    "\n",
    "### `workers`\n",
    "How many threads to use behind the scenes?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When should you use Word2Vec?\n",
    "\n",
    "There are many application scenarios for Word2Vec. Imagine if you need to build a sentiment lexicon. Training a Word2Vec model on large amounts of user reviews helps you achieve that. You have a lexicon for not just sentiment, but for most words in the vocabulary. \n",
    "\n",
    "Beyond, raw unstructured text data, you could also use Word2Vec for more structured data. For example, if you had tags for a million stackoverflow questions and answers, you could find tags that are related to a given tag and recommend the related ones for exploration. You can do this by treating each set of co-occuring tags as a \"sentence\" and train a Word2Vec model on this data. Granted, you still need a large number of examples to make it work. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
