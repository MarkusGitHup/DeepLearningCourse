{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP (Sentiment analysis) with LSTM\n",
    "\n",
    "This notebook is originally from Francois Chollet's notebook accompanying his book \"Deep Learning with Python\".\n",
    "\n",
    "I used it as a starting point for own experiments with word embeddings and modified it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. Word embeddings in general\n",
    "2. Learning word embeddings with the Embedding layer\n",
    "3. Using pre-trained word embeddings\n",
    "4. Training a LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Word embeddings in general\n",
    "\n",
    "This notebook contains the second code sample found in Chapter 6, Section 1 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Another popular and powerful way to associate a vector with a word is the use of dense \"word vectors\", also called \"word embeddings\". \n",
    "While the vectors obtained through one-hot encoding are binary, sparse (mostly made of zeros) and very high-dimensional (same dimensionality as the \n",
    "number of words in the vocabulary), \"word embeddings\" are low-dimensional floating point vectors \n",
    "(i.e. \"dense\" vectors, as opposed to sparse vectors). \n",
    "Unlike word vectors obtained via one-hot encoding, word embeddings are learned from data. \n",
    "It is common to see word embeddings that are 256-dimensional, 512-dimensional, or 1024-dimensional when dealing with very large vocabularies. \n",
    "On the other hand, one-hot encoding words generally leads to vectors that are 20,000-dimensional or higher (capturing a vocabulary of 20,000 \n",
    "token in this case). So, word embeddings pack more information into far fewer dimensions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![word embeddings vs. one hot encoding](https://s3.amazonaws.com/book.keras.io/img/ch6/word_embeddings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to obtain word embeddings:\n",
    "\n",
    "* Learn word embeddings jointly with the main task you care about (e.g. document classification or sentiment prediction). \n",
    "In this setup, you would start with random word vectors, then learn your word vectors in the same way that you learn the weights of a neural network.\n",
    "* Load into your model word embeddings that were pre-computed using a different machine learning task than the one you are trying to solve. \n",
    "These are called \"pre-trained word embeddings\". \n",
    "\n",
    "Let's take a look at both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Learning word embeddings with the `Embedding` layer\n",
    "\n",
    "\n",
    "The simplest way to associate a dense vector to a word would be to pick the vector at random. The problem with this approach is that the \n",
    "resulting embedding space would have no structure: for instance, the words \"accurate\" and \"exact\" may end up with completely different \n",
    "embeddings, even though they are interchangeable in most sentences. It would be very difficult for a deep neural network to make sense of \n",
    "such a noisy, unstructured embedding space. \n",
    "\n",
    "To get a bit more abstract: the geometric relationships between word vectors should reflect the semantic relationships between these words. \n",
    "Word embeddings are meant to map human language into a geometric space. For instance, in a reasonable embedding space, we would expect \n",
    "synonyms to be embedded into similar word vectors, and in general we would expect the geometric distance (e.g. L2 distance) between any two \n",
    "word vectors to relate to the semantic distance of the associated words (words meaning very different things would be embedded to points \n",
    "far away from each other, while related words would be closer). Even beyond mere distance, we may want specific __directions__ in the \n",
    "embedding space to be meaningful. \n",
    "\n",
    "[...]\n",
    "\n",
    "\n",
    "In real-world word embedding spaces, common examples of meaningful geometric transformations are \"gender vectors\" and \"plural vector\". For \n",
    "instance, by adding a \"female vector\" to the vector \"king\", one obtain the vector \"queen\". By adding a \"plural vector\", one obtain \"kings\". \n",
    "Word embedding spaces typically feature thousands of such interpretable and potentially useful vectors.\n",
    "\n",
    "Is there some \"ideal\" word embedding space that would perfectly map human language and could be used for any natural language processing \n",
    "task? Possibly, but in any case, we have yet to compute anything of the sort. Also, there isn't such a thing as \"human language\", there are \n",
    "many different languages and they are not isomorphic, as a language is the reflection of a specific culture and a specific context. But more \n",
    "pragmatically, what makes a good word embedding space depends heavily on your task: the perfect word embedding space for an \n",
    "English-language movie review sentiment analysis model may look very different from the perfect embedding space for an English-language \n",
    "legal document classification model, because the importance of certain semantic relationships varies from task to task.\n",
    "\n",
    "It is thus reasonable to __learn__ a new embedding space with every new task. Thankfully, backpropagation makes this really easy, and Keras makes it \n",
    "even easier. It's just about learning the weights of a layer: the `Embedding` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "# The Embedding layer takes at least two arguments:\n",
    "# the number of possible tokens, here 1000 (1 + maximum word index),\n",
    "# and the dimensionality of the embeddings, here 64.\n",
    "embedding_layer = Embedding(1000, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The `Embedding` layer is best understood as a dictionary mapping integer indices (which stand for specific words) to dense vectors. It takes \n",
    "as input integers, it looks up these integers into an internal dictionary, and it returns the associated vectors. It's effectively a dictionary lookup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The `Embedding` layer takes as input a 2D tensor of integers, of shape `(samples, sequence_length)`, where each entry is a sequence of \n",
    "integers. It can embed sequences of variable lengths, so for instance we could feed into our embedding layer above batches that could have \n",
    "shapes `(32, 10)` (batch of 32 sequences of length 10) or `(64, 15)` (batch of 64 sequences of length 15). All sequences in a batch must \n",
    "have the same length, though (since we need to pack them into a single tensor), so sequences that are shorter than others should be padded \n",
    "with zeros, and sequences that are longer should be truncated.\n",
    "\n",
    "This layer returns a 3D floating point tensor, of shape `(samples, sequence_length, embedding_dimensionality)`. Such a 3D tensor can then \n",
    "be processed by a RNN layer or a 1D convolution layer (both will be introduced in the next sections).\n",
    "\n",
    "When you instantiate an `Embedding` layer, its weights (its internal dictionary of token vectors) are initially random, just like with any \n",
    "other layer. During training, these word vectors will be gradually adjusted via backpropagation, structuring the space into something that the \n",
    "downstream model can exploit. Once fully trained, your embedding space will show a lot of structure -- a kind of structure specialized for \n",
    "the specific problem you were training your model for.\n",
    "\n",
    "Let's apply this idea to the IMDB movie review sentiment prediction task that you are already familiar with. Let's quickly prepare \n",
    "the data. We will restrict the movie reviews to the top 10,000 most common words (like we did the first time we worked with this dataset), \n",
    "and cut the reviews after only 20 words. Our network will simply learn 8-dimensional embeddings for each of the 10,000 words, turn the \n",
    "input integer sequences (2D integer tensor) into embedded sequences (3D float tensor), flatten the tensor to 2D, and train a single `Dense` \n",
    "layer on top for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "# Number of words to consider as features\n",
    "max_features = 10000\n",
    "# Cut texts after this number of words \n",
    "# (among top max_features most common words)\n",
    "maxlen = 20\n",
    "\n",
    "# Load the data as lists of integers.\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# This turns our lists of integers\n",
    "# into a 2D integer tensor of shape `(samples, maxlen)`\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train= (25000, 20)\n",
      "shape of y_train= (25000,)\n",
      "shape of x_test= (25000, 20)\n",
      "shape of y_test= (25000,)\n",
      "\n",
      "Example sequence # 0\n",
      "\tInput: [  65   16   38 1334   88   12   16  283    5   16 4472  113  103   32\n",
      "   15   16 5345   19  178   32]\n",
      "\tOutput: 1\n",
      "\n",
      "Example sequence # 1\n",
      "\tInput: [  23    4 1690   15   16    4 1355    5   28    6   52  154  462   33\n",
      "   89   78  285   16  145   95]\n",
      "\tOutput: 0\n",
      "\n",
      "Example sequence # 2\n",
      "\tInput: [1352   13  191   79  638   89    2   14    9    8  106  607  624   35\n",
      "  534    6  227    7  129  113]\n",
      "\tOutput: 0\n",
      "\n",
      "Example sequence # 3\n",
      "\tInput: [   7 2804    5    4  559  154  888    7  726   50   26   49 7008   15\n",
      "  566   30  579   21   64 2574]\n",
      "\tOutput: 1\n",
      "\n",
      "Example sequence # 4\n",
      "\tInput: [  15  595   13  784   25 3171   18  165  170  143   19   14    5 7224\n",
      "    6  226  251    7   61  113]\n",
      "\tOutput: 0\n",
      "\n",
      "Example sequence # 5\n",
      "\tInput: [  10   10 1361  173    4  749    2   16 3804    8    4  226   65   12\n",
      "   43  127   24    2   10   10]\n",
      "\tOutput: 0\n",
      "\n",
      "Example sequence # 6\n",
      "\tInput: [  99   76   23    2    7  419  665   40   91   85  108    7    4 2084\n",
      "    5 4773   81   55   52 1901]\n",
      "\tOutput: 1\n",
      "\n",
      "Example sequence # 7\n",
      "\tInput: [ 277 1730   37   25   92  202    6 8848   44   25   28    6   22   15\n",
      "  122   24 4171   72   33   32]\n",
      "\tOutput: 0\n",
      "\n",
      "Example sequence # 8\n",
      "\tInput: [  12  639   21   13   80  140    5  135   15   14    9   31    7    4\n",
      "  118 3672   13   28  126  110]\n",
      "\tOutput: 1\n",
      "\n",
      "Example sequence # 9\n",
      "\tInput: [  78  807    9  375    8 1167    8  794   76    7    4   58    5    4\n",
      "  816    9  243    7   43   50]\n",
      "\tOutput: 0\n",
      "\n",
      "Example sequence # 10\n",
      "\tInput: [4442  444    2    9    6  371   87  189   22    5   31    7    4  118\n",
      "    7    4 2068  545 1178  829]\n",
      "\tOutput: 1\n",
      "\n",
      "Example sequence # 11\n",
      "\tInput: [  31  155   36  100  763  379   20  103  351 5308   13  202   12 2241\n",
      "    5    6  320   46    7  457]\n",
      "\tOutput: 0\n",
      "\n",
      "Example sequence # 12\n",
      "\tInput: [ 534    5    6  320    8  516    5   42   25  181    8  130   56  547\n",
      " 3571    5 1471  851   14 2286]\n",
      "\tOutput: 0\n",
      "\n",
      "Example sequence # 13\n",
      "\tInput: [  16    6    2   17    8   63   31   16  433   51    9  170   23   11\n",
      " 1898  134  504 1195 1195 1195]\n",
      "\tOutput: 0\n",
      "\n",
      "Example sequence # 14\n",
      "\tInput: [   4 4017 2809   10   10  719    2   70 2885    4 2552    2 4430  175\n",
      " 6640   11    4    2  543 1609]\n",
      "\tOutput: 0\n",
      "[1 0 0 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of x_train=\", x_train.shape)\n",
    "print(\"shape of y_train=\", y_train.shape)\n",
    "\n",
    "print(\"shape of x_test=\", x_test.shape)\n",
    "print(\"shape of y_test=\", y_test.shape)\n",
    "\n",
    "NR_EXAMPLES_TO_SHOW = 15\n",
    "for i in range(0,NR_EXAMPLES_TO_SHOW):\n",
    "    print()\n",
    "    print(\"Example sequence #\", i)\n",
    "    print(\"\\tInput:\",  x_train[i])\n",
    "    print(\"\\tOutput:\", y_train[i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 20, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 1s 57us/step - loss: 0.6759 - acc: 0.6043 - val_loss: 0.6398 - val_acc: 0.6810\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 1s 45us/step - loss: 0.5657 - acc: 0.7428 - val_loss: 0.5467 - val_acc: 0.7206\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 1s 45us/step - loss: 0.4752 - acc: 0.7808 - val_loss: 0.5113 - val_acc: 0.7384\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 1s 45us/step - loss: 0.4263 - acc: 0.8079 - val_loss: 0.5008 - val_acc: 0.7454\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 1s 46us/step - loss: 0.3930 - acc: 0.8257 - val_loss: 0.4981 - val_acc: 0.7540\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 1s 47us/step - loss: 0.3668 - acc: 0.8395 - val_loss: 0.5013 - val_acc: 0.7534\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 1s 45us/step - loss: 0.3435 - acc: 0.8534 - val_loss: 0.5051 - val_acc: 0.7518\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 1s 44us/step - loss: 0.3223 - acc: 0.8658 - val_loss: 0.5132 - val_acc: 0.7486\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 1s 44us/step - loss: 0.3022 - acc: 0.8765 - val_loss: 0.5213 - val_acc: 0.7492\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 1s 46us/step - loss: 0.2839 - acc: 0.8860 - val_loss: 0.5302 - val_acc: 0.7466\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "# We specify the maximum input length to our Embedding layer\n",
    "# so we can later flatten the embedded inputs\n",
    "model.add(Embedding(10000, 8, input_length=maxlen))\n",
    "# After the Embedding layer, \n",
    "# our activations have shape `(samples, maxlen, 8)`.\n",
    "\n",
    "# We flatten the 3D tensor of embeddings \n",
    "# into a 2D tensor of shape `(samples, maxlen * 8)`\n",
    "model.add(Flatten())\n",
    "\n",
    "# We add the classifier on top\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get to a validation accuracy of ~76%, which is pretty good considering that we only look at the first 20 words in every review. But \n",
    "note that merely flattening the embedded sequences and training a single `Dense` layer on top leads to a model that treats each word in the \n",
    "input sequence separately, without considering inter-word relationships and structure sentence (e.g. it would likely treat both _\"this movie \n",
    "is shit\"_ and _\"this movie is the shit\"_ as being negative \"reviews\"). It would be much better to add recurrent layers or 1D convolutional \n",
    "layers on top of the embedded sequences to learn features that take into account each sequence as a whole. That's what we will focus on in \n",
    "the next few sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using pre-trained word embeddings\n",
    "\n",
    "\n",
    "Sometimes, you have so little training data available that could never use your data alone to learn an appropriate task-specific embedding \n",
    "of your vocabulary. What to do then?\n",
    "\n",
    "Instead of learning word embeddings jointly with the problem you want to solve, you could be loading embedding vectors from a pre-computed \n",
    "embedding space known to be highly structured and to exhibit useful properties -- that captures generic aspects of language structure. The \n",
    "rationale behind using pre-trained word embeddings in natural language processing is very much the same as for using pre-trained convnets \n",
    "in image classification: we don't have enough data available to learn truly powerful features on our own, but we expect the features that \n",
    "we need to be fairly generic, i.e. common visual features or semantic features. In this case it makes sense to reuse features learned on a \n",
    "different problem.\n",
    "\n",
    "Such word embeddings are generally computed using word occurrence statistics (observations about what words co-occur in sentences or \n",
    "documents), using a variety of techniques, some involving neural networks, others not. The idea of a dense, low-dimensional embedding space \n",
    "for words, computed in an unsupervised way, was initially explored by Bengio et al. in the early 2000s, but it only started really taking \n",
    "off in research and industry applications after the release of one of the most famous and successful word embedding scheme: the Word2Vec \n",
    "algorithm, developed by Mikolov at Google in 2013. Word2Vec dimensions capture specific semantic properties, e.g. gender.\n",
    "\n",
    "There are various pre-computed databases of word embeddings that can download and start using in a Keras `Embedding` layer. Word2Vec is one \n",
    "of them. Another popular one is called \"GloVe\", developed by Stanford researchers in 2014. It stands for \"Global Vectors for Word \n",
    "Representation\", and it is an embedding technique based on factorizing a matrix of word co-occurrence statistics. Its developers have made \n",
    "available pre-computed embeddings for millions of English tokens, obtained from Wikipedia data or from Common Crawl data.\n",
    "\n",
    "Let's take a look at how you can get started using GloVe embeddings in a Keras model. The same method will of course be valid for Word2Vec \n",
    "embeddings or any other word embedding database that you can download. We will also use this example to refresh the text tokenization \n",
    "techniques we introduced a few paragraphs ago: we will start from raw text, and work our way up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together: from raw text to word embeddings\n",
    "\n",
    "\n",
    "We will be using a model similar to the one we just went over -- embedding sentences in sequences of vectors, flattening them and training a \n",
    "`Dense` layer on top. But we will do it using pre-trained word embeddings, and instead of using the pre-tokenized IMDB data packaged in \n",
    "Keras, we will start from scratch, by downloading the original text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the IMDB data as raw text\n",
    "\n",
    "\n",
    "First, head to `http://ai.stanford.edu/~amaas/data/sentiment/` and download the raw IMDB dataset (if the URL isn't working anymore, just \n",
    "Google \"IMDB dataset\"). Uncompress it.\n",
    "\n",
    "Now let's collect the individual training reviews into a list of strings, one string per review, and let's also collect the review labels \n",
    "(positive / negative) into a `labels` list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "imdb_dir = 'large_movie_review_dataset/'\n",
    "train_dir = os.path.join(imdb_dir, 'train')\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname), encoding=\"utf8\")\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of labels= 25000\n",
      "len of texts= 25000\n",
      "\n",
      "Example sequence # 0\n",
      "\tLabel: 0\n",
      "\tText: Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\n",
      "\n",
      "Example sequence # 1\n",
      "\tLabel: 0\n",
      "\tText: Airport '77 starts as a brand new luxury 747 plane is loaded up with valuable paintings & such belonging to rich businessman Philip Stevens (James Stewart) who is flying them & a bunch of VIP's to his estate in preparation of it being opened to the public as a museum, also on board is Stevens daughter Julie (Kathleen Quinlan) & her son. The luxury jetliner takes off as planned but mid-air the plane is hi-jacked by the co-pilot Chambers (Robert Foxworth) & his two accomplice's Banker (Monte Markham) & Wilson (Michael Pataki) who knock the passengers & crew out with sleeping gas, they plan to steal the valuable cargo & land on a disused plane strip on an isolated island but while making his descent Chambers almost hits an oil rig in the Ocean & loses control of the plane sending it crashing into the sea where it sinks to the bottom right bang in the middle of the Bermuda Triangle. With air in short supply, water leaking in & having flown over 200 miles off course the problems mount for the survivor's as they await help with time fast running out...<br /><br />Also known under the slightly different tile Airport 1977 this second sequel to the smash-hit disaster thriller Airport (1970) was directed by Jerry Jameson & while once again like it's predecessors I can't say Airport '77 is any sort of forgotten classic it is entertaining although not necessarily for the right reasons. Out of the three Airport films I have seen so far I actually liked this one the best, just. It has my favourite plot of the three with a nice mid-air hi-jacking & then the crashing (didn't he see the oil rig?) & sinking of the 747 (maybe the makers were trying to cross the original Airport with another popular disaster flick of the period The Poseidon Adventure (1972)) & submerged is where it stays until the end with a stark dilemma facing those trapped inside, either suffocate when the air runs out or drown as the 747 floods or if any of the doors are opened & it's a decent idea that could have made for a great little disaster flick but bad unsympathetic character's, dull dialogue, lethargic set-pieces & a real lack of danger or suspense or tension means this is a missed opportunity. While the rather sluggish plot keeps one entertained for 108 odd minutes not that much happens after the plane sinks & there's not as much urgency as I thought there should have been. Even when the Navy become involved things don't pick up that much with a few shots of huge ships & helicopters flying about but there's just something lacking here. George Kennedy as the jinxed airline worker Joe Patroni is back but only gets a couple of scenes & barely even says anything preferring to just look worried in the background.<br /><br />The home video & theatrical version of Airport '77 run 108 minutes while the US TV versions add an extra hour of footage including a new opening credits sequence, many more scenes with George Kennedy as Patroni, flashbacks to flesh out character's, longer rescue scenes & the discovery or another couple of dead bodies including the navigator. While I would like to see this extra footage I am not sure I could sit through a near three hour cut of Airport '77. As expected the film has dated badly with horrible fashions & interior design choices, I will say no more other than the toy plane model effects aren't great either. Along with the other two Airport sequels this takes pride of place in the Razzie Award's Hall of Shame although I can think of lots of worse films than this so I reckon that's a little harsh. The action scenes are a little dull unfortunately, the pace is slow & not much excitement or tension is generated which is a shame as I reckon this could have been a pretty good film if made properly.<br /><br />The production values are alright if nothing spectacular. The acting isn't great, two time Oscar winner Jack Lemmon has said since it was a mistake to star in this, one time Oscar winner James Stewart looks old & frail, also one time Oscar winner Lee Grant looks drunk while Sir Christopher Lee is given little to do & there are plenty of other familiar faces to look out for too.<br /><br />Airport '77 is the most disaster orientated of the three Airport films so far & I liked the ideas behind it even if they were a bit silly, the production & bland direction doesn't help though & a film about a sunken plane just shouldn't be this boring or lethargic. Followed by The Concorde ... Airport '79 (1979).\n",
      "\n",
      "Example sequence # 2\n",
      "\tLabel: 0\n",
      "\tText: This film lacked something I couldn't put my finger on at first: charisma on the part of the leading actress. This inevitably translated to lack of chemistry when she shared the screen with her leading man. Even the romantic scenes came across as being merely the actors at play. It could very well have been the director who miscalculated what he needed from the actors. I just don't know.<br /><br />But could it have been the screenplay? Just exactly who was the chef in love with? He seemed more enamored of his culinary skills and restaurant, and ultimately of himself and his youthful exploits, than of anybody or anything else. He never convinced me he was in love with the princess.<br /><br />I was disappointed in this movie. But, don't forget it was nominated for an Oscar, so judge for yourself.\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"len of labels=\",len(labels))\n",
    "print(\"len of texts=\",len(texts))\n",
    "\n",
    "NR_EXAMPLES_TO_SHOW = 3\n",
    "for i in range(0,NR_EXAMPLES_TO_SHOW):\n",
    "    print()\n",
    "    print(\"Example sequence #\", i)\n",
    "    print(\"\\tLabel:\",  labels[i])\n",
    "    print(\"\\tText:\", texts[i])\n",
    "    \n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the data\n",
    "\n",
    "\n",
    "Let's vectorize the texts we collected, and prepare a training and validation split.\n",
    "We will merely be using the concepts we introduced earlier in this section.\n",
    "\n",
    "Because pre-trained word embeddings are meant to be particularly useful on problems where little training data is available (otherwise, \n",
    "task-specific embeddings are likely to outperform them), we will add the following twist: we restrict the training data to its first 200 \n",
    "samples. So we will be learning to classify movie reviews after looking at just 200 examples...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of sequences is 25000\n",
      "Example review as text: Once again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the terrific sea rescue sequences, of which there are very few I just did not care about any of the characters. Most of us have ghosts in the closet, and Costner's character are realized early on, and then forgotten until much later, by which time I did not care. The character we should really care about is a very cocky, overconfident Ashton Kutcher. The problem is he comes off as kid who thinks he's better than anyone else around him and shows no signs of a cluttered closet. His only obstacle appears to be winning over Costner. Finally when we are well past the half way point of this stinker, Costner tells us all about Kutcher's ghosts. We are told why Kutcher is driven to be the best with no prior inkling or foreshadowing. No magic here, it was all I could do to keep from turning it off an hour in.\n",
      "Example review as integer sequence: [282, 171, 443, 6416, 45, 3585, 41, 3, 15, 17, 222, 1109, 72, 1803, 1171, 36, 1, 1353, 1366, 2127, 861, 4, 63, 46, 23, 52, 170, 9, 39, 115, 22, 460, 42, 99, 4, 1, 101, 87, 4, 180, 25, 3542, 8, 1, 4446, 2, 108, 23, 1660, 409, 20, 2, 91, 1508, 366, 71, 307, 31, 63, 56, 9, 115, 22, 460, 1, 108, 73, 143, 62, 460, 42, 6, 3, 52, 8448, 7753, 1, 459, 6, 28, 286, 121, 14, 551, 35, 1204, 237, 125, 72, 245, 331, 182, 85, 2, 269, 54, 3586, 4, 3, 4446, 24, 64, 739, 5, 27, 1920, 123, 6416, 428, 50, 73, 23, 69, 523, 1, 289, 95, 224, 4, 11, 3648, 6416, 741, 180, 30, 42, 3542, 73, 23, 562, 134, 7753, 6, 2108, 5, 27, 1, 118, 16, 54, 2538, 38, 54, 1490, 135, 10, 13, 30, 9, 98, 78, 5, 387, 36, 1629, 10, 121, 32, 522, 8]\n",
      "Found 87393 unique tokens.\n",
      "\n",
      "Shape of data tensor: (25000, 100)\n",
      "Shape of label tensor: (25000,)\n",
      "\n",
      "Shape of x_train is (5000, 100)\n",
      "Shape of y_train is (5000,)\n",
      "\n",
      "Shape of x_val is (10000, 100)\n",
      "Shape of y_val is (10000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 100  # We will cut reviews after 100 words\n",
    "training_samples = 5000  # We will be training on 200 samples\n",
    "validation_samples = 10000  # We will be validating on 10000 samples\n",
    "max_words = 10000  # We will only consider the top 10,000 words in the dataset\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "print(\"Len of sequences is\", len(sequences))\n",
    "print(\"Example review as text:\", texts[0])\n",
    "print(\"Example review as integer sequence:\", sequences[0])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "\n",
    "print()\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# Split the data into a training set and a validation set\n",
    "# But first, shuffle the data, since we started from data\n",
    "# where sample are ordered (all negative first, then all positive).\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]\n",
    "\n",
    "print()\n",
    "print(\"Shape of x_train is\", x_train.shape)\n",
    "print(\"Shape of y_train is\", y_train.shape)\n",
    "\n",
    "print()\n",
    "print(\"Shape of x_val is\", x_val.shape)\n",
    "print(\"Shape of y_val is\", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the GloVe word embeddings\n",
    "\n",
    "\n",
    "Head to `https://nlp.stanford.edu/projects/glove/` (where you can learn more about the GloVe algorithm), and download the pre-computed \n",
    "embeddings from 2014 English Wikipedia. It's a 822MB zip file named `glove.6B.zip`, containing 100-dimensional embedding vectors for \n",
    "400,000 words (or non-word tokens). Un-zip it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the embeddings\n",
    "\n",
    "\n",
    "Let's parse the un-zipped file (it's a `txt` file) to build an index mapping words (as strings) to their vector representation (as number \n",
    "vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dir = 'glove'\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "Word brockett --> [ 0.33578   -0.68282   -0.77117    0.17338   -0.084162  -0.34996\n",
      "  0.20898    0.3376     0.15405    0.46283   -0.30397    0.12714\n",
      "  0.054559  -0.35231    0.095936  -0.16224   -0.21077    0.18083\n",
      " -0.55152    0.37061   -0.46865   -0.20636   -0.11348   -0.34453\n",
      "  0.10206    0.15897    0.077552  -0.14138   -0.16021    0.17821\n",
      " -0.32088   -0.20289    0.34438    0.15887    0.39497    0.95127\n",
      " -0.24971   -0.050084   0.03273    0.13508   -0.1225     0.49871\n",
      " -0.40989   -0.052672  -0.41463   -0.12683   -0.43082   -0.066442\n",
      "  0.26695    0.66747    0.11949   -0.16577   -0.069714  -0.053098\n",
      " -0.26931    0.22682    0.43827    0.25237   -1.0767    -0.71395\n",
      "  0.26846   -0.13479    0.8962    -0.26139   -0.39995   -0.71377\n",
      " -0.2004    -0.38557   -0.55952    0.56991   -0.064482   0.34024\n",
      "  0.035591   0.13796    0.20172   -0.081919  -0.079089  -0.12896\n",
      "  0.25096    0.58596   -0.461     -0.18261   -0.14723    0.099315\n",
      "  0.77502   -0.13878    0.0020861 -0.30491   -0.022962   0.16503\n",
      " -0.014771   0.11395    0.48078   -0.24806    0.049897  -0.10147\n",
      "  0.1689     0.020555   0.02053    0.5776   ]\n",
      "Word stenbock --> [ 1.3893e-01 -3.4624e-01 -6.7919e-01  2.9203e-01  1.5694e-01  2.2583e-01\n",
      " -5.8498e-04  3.9217e-02  2.7407e-02  2.4882e-02  2.7927e-01  3.0573e-01\n",
      " -8.3416e-01 -3.3693e-01 -7.3900e-02  2.4945e-01 -5.5659e-01  4.2483e-02\n",
      " -2.5909e-01  6.2308e-02 -2.4986e-01 -7.1793e-01  1.0747e-01  4.0689e-01\n",
      "  1.2775e-01 -1.1274e-01 -9.3406e-03  1.1475e-01  4.1617e-01 -2.3704e-01\n",
      "  4.5733e-01 -5.6622e-01  3.0977e-01  1.7619e-01 -1.8320e-01  3.4612e-02\n",
      "  2.3312e-01  3.0340e-01 -2.0558e-01  9.0395e-02 -1.5169e-01  4.7176e-01\n",
      "  3.4634e-01  2.8917e-02 -1.3162e-01  2.0498e-02 -2.2306e-01  2.9687e-01\n",
      "  6.8260e-01  4.8766e-01  3.8487e-01 -8.9623e-02  3.9244e-01  4.4832e-02\n",
      " -4.1658e-02  5.9350e-01 -8.9383e-03  2.0716e-01 -8.4213e-01  2.0553e-01\n",
      "  1.5039e-01 -7.1784e-01 -1.4292e-01  7.8652e-01 -2.4094e-01  1.6388e-01\n",
      " -7.0130e-01 -1.1207e-01 -2.8797e-02 -1.5104e-02  3.4850e-01 -2.0476e-01\n",
      "  3.3443e-01  6.2036e-01  3.7743e-01  4.0044e-01 -2.0993e-01 -1.9281e-01\n",
      "  7.0671e-01  3.8411e-01 -5.8323e-01  1.5663e-01  1.5541e-01 -1.7249e-01\n",
      "  4.4976e-02  1.5678e-01 -2.3564e-01 -2.1090e-01 -2.0483e-01 -4.3650e-01\n",
      "  5.6723e-02 -2.3089e-01  7.0190e-01 -1.1732e-01  3.4536e-01 -6.6770e-01\n",
      "  1.3653e-01  1.1767e-01  3.6946e-02 -4.4128e-01]\n",
      "Word c-119 --> [-3.5343e-01 -1.9285e-01 -7.2146e-01  6.8383e-01  1.1735e-01  1.6926e-01\n",
      " -3.4901e-01 -6.7863e-01 -4.9607e-01  3.1370e-01  5.1765e-01 -1.1271e+00\n",
      "  1.0218e-01  2.9921e-01 -1.3914e-01  6.4770e-01  6.0425e-01 -2.4223e-01\n",
      "  1.9017e+00 -7.4874e-01  7.7451e-03  1.6401e-01  8.8372e-02 -6.8814e-01\n",
      " -3.4451e-02 -5.5404e-02 -7.2655e-01  9.3596e-01  5.4919e-01  1.4667e-01\n",
      "  3.0645e-01 -2.7041e-01 -5.5396e-01 -4.3719e-02  4.3852e-03  5.8719e-01\n",
      "  5.7257e-01  1.0782e-01  1.2081e-01 -2.2627e-01 -1.8725e-01  1.1220e+00\n",
      "  3.8006e-01  3.8456e-01  1.3449e-03  6.6319e-01  4.0795e-02  8.4852e-01\n",
      " -1.9214e-01  3.5682e-01 -4.3054e-01  1.0800e-02 -4.4968e-01  2.2906e-01\n",
      "  6.5066e-01  1.1299e+00  9.7727e-01  1.6750e-01  8.4764e-01  8.9909e-01\n",
      "  1.8556e-01 -2.6664e-01 -2.8447e-01  4.4375e-01 -6.4290e-01 -9.2757e-02\n",
      " -1.5541e-01 -2.2293e-02  9.6560e-01  5.9928e-01 -5.8521e-01 -9.2261e-01\n",
      "  4.0968e-02  3.8957e-04 -4.7642e-01  7.8666e-01  5.6717e-01 -8.2637e-01\n",
      "  6.0190e-01 -2.1018e-01 -3.5798e-01 -2.1138e-01  1.5655e-01  5.6193e-01\n",
      "  2.3896e-01 -2.7992e-01  6.2134e-01 -1.1486e+00 -4.7355e-01  4.3887e-01\n",
      " -1.0805e+00 -7.0327e-01  2.9255e-01 -3.3267e-01 -1.9466e-01 -3.7309e-01\n",
      " -1.5395e-01 -1.8017e-01 -6.1435e-01 -7.8548e-01]\n",
      "Word unconscious --> [ 0.59176   -0.37149    0.34817    0.36196   -0.30959    1.2103\n",
      "  0.37649    0.086353   0.52277   -0.49503    0.0098354  0.84468\n",
      "  0.3727    -0.25524    0.87239    0.020887  -0.78319    0.91647\n",
      " -0.11403   -0.19903    0.33449    0.079179   0.43958    0.33486\n",
      " -0.018914  -0.070152   0.12994   -0.27041    0.31071    0.89456\n",
      "  0.37941    0.28675   -0.77084    0.46857   -0.62516   -0.37953\n",
      " -0.98836    0.22181    0.41647   -0.060335  -0.74081   -0.086375\n",
      "  0.38382   -0.11951   -0.21602    0.68747   -0.73346    1.1553\n",
      " -0.43605    0.030556  -0.19372   -0.26626   -0.0051489  1.0013\n",
      "  0.30309   -0.91281   -0.04339   -0.68496    0.39376    0.51492\n",
      "  0.87829    0.64289   -0.12933    0.36217   -0.18758   -0.14993\n",
      "  0.7968    -0.6044    -0.80943   -0.33105   -0.67002   -0.27739\n",
      "  1.0679     0.079929   0.4759     0.29735    0.088412  -0.071899\n",
      " -0.11244   -0.42528   -0.1481     1.0214    -0.44826    0.35305\n",
      " -0.85209   -0.56295   -0.43356    0.70786   -0.54218   -0.19921\n",
      "  0.29364   -0.20869    0.21358    1.1039     0.32585    0.12437\n",
      "  0.2686    -0.8078    -0.047229  -0.42765  ]\n"
     ]
    }
   ],
   "source": [
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "for i,key in enumerate(embeddings_index):\n",
    "    value = embeddings_index[key]\n",
    "    print(\"Word\", key, \"-->\", value)\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now let's build an embedding matrix that we will be able to load into an `Embedding` layer. It must be a matrix of shape `(max_words, \n",
    "embedding_dim)`, where each entry `i` contains the `embedding_dim`-dimensional vector for the word of index `i` in our reference word index \n",
    "(built during tokenization). Note that the index `0` is not supposed to stand for any word or token -- it's a placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding_matrix is (10000, 100)\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if i < max_words:\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "print(\"Shape of embedding_matrix is\", embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a model\n",
    "\n",
    "We will be using the same model architecture as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                320032    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,320,065\n",
      "Trainable params: 1,320,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the GloVe embeddings in the model\n",
    "\n",
    "\n",
    "The `Embedding` layer has a single weight matrix: a 2D float matrix where each entry `i` is the word vector meant to be associated with \n",
    "index `i`. Simple enough. Let's just load the GloVe matrix we prepared into our `Embedding` layer, the first layer in our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Additionally, we freeze the embedding layer (we set its `trainable` attribute to `False`), following the same rationale as what you are \n",
    "already familiar with in the context of pre-trained convnet features: when parts of a model are pre-trained (like our `Embedding` layer), \n",
    "and parts are randomly initialized (like our classifier), the pre-trained parts should not be updated during training to avoid forgetting \n",
    "what they already know. The large gradient update triggered by the randomly initialized layers would be very disruptive to the already \n",
    "learned features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate\n",
    "\n",
    "Let's compile our model and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train is (5000, 100)\n",
      "Shape of y_train is (5000,)\n",
      "\n",
      "Shape of x_val is (10000, 100)\n",
      "Shape of y_val is (10000,)\n",
      "Train on 5000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "5000/5000 [==============================] - 2s 355us/step - loss: 0.7280 - acc: 0.5054 - val_loss: 0.6933 - val_acc: 0.4933\n",
      "Epoch 2/10\n",
      "5000/5000 [==============================] - 2s 333us/step - loss: 0.6945 - acc: 0.5088 - val_loss: 0.6936 - val_acc: 0.4930\n",
      "Epoch 3/10\n",
      "5000/5000 [==============================] - 2s 328us/step - loss: 0.7016 - acc: 0.5320 - val_loss: 0.7689 - val_acc: 0.4935\n",
      "Epoch 4/10\n",
      "5000/5000 [==============================] - 2s 340us/step - loss: 0.6722 - acc: 0.5842 - val_loss: 0.7375 - val_acc: 0.4937\n",
      "Epoch 5/10\n",
      "5000/5000 [==============================] - 2s 349us/step - loss: 0.6141 - acc: 0.6648 - val_loss: 1.1038 - val_acc: 0.5045\n",
      "Epoch 6/10\n",
      "5000/5000 [==============================] - 2s 328us/step - loss: 0.5353 - acc: 0.7430 - val_loss: 0.7771 - val_acc: 0.5033\n",
      "Epoch 7/10\n",
      "5000/5000 [==============================] - 2s 327us/step - loss: 0.4596 - acc: 0.7824 - val_loss: 1.1106 - val_acc: 0.5029\n",
      "Epoch 8/10\n",
      "5000/5000 [==============================] - 2s 333us/step - loss: 0.3694 - acc: 0.8460 - val_loss: 0.9129 - val_acc: 0.4953\n",
      "Epoch 9/10\n",
      "5000/5000 [==============================] - 2s 330us/step - loss: 0.2989 - acc: 0.8838 - val_loss: 0.9733 - val_acc: 0.4973\n",
      "Epoch 10/10\n",
      "5000/5000 [==============================] - 2s 331us/step - loss: 0.2355 - acc: 0.9112 - val_loss: 1.1110 - val_acc: 0.4972\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of x_train is\", x_train.shape)\n",
    "print(\"Shape of y_train is\", y_train.shape)\n",
    "\n",
    "print()\n",
    "print(\"Shape of x_val is\", x_val.shape)\n",
    "print(\"Shape of y_val is\", y_val.shape)\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))\n",
    "model.save_weights('pre_trained_glove_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot its performance over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x217ae0d1ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8VOX1+PHPYRFkESyiggjBnV0gohaRRau4laKgbAoo\nAvoTN6Ral2pdWhc2UQyorfoVlKKtIopF64a7LCKKiFJZDIsEhLALIef3x5lAEkgyCTNzZ27O+/XK\nKzN37tx7MknOPPPc5zmPqCrOOefCpULQATjnnIs9T+7OORdCntydcy6EPLk751wIeXJ3zrkQ8uTu\nnHMh5Mnd7ZeIVBSRLSLSMJb7BklEjhORmI/9FZGzRWRZvvuLRaRDNPuW4VxPi8jtZX1+Mce9X0Se\njfVxXXAqBR2Aiw0R2ZLvbjXgV2B35P4QVZ1cmuOp6m6gRqz3LQ9U9cRYHEdEBgH9VLVTvmMPisWx\nXfh5cg8JVd2TXCMtw0Gq+t+i9heRSqqak4jYnHOJ590y5UTkY/c/ReRFEdkM9BOR00XkMxHZKCKr\nRWSciFSO7F9JRFRE0iL3J0Uef1NENovIpyLSuLT7Rh4/T0S+F5FsEXlMRD4WkQFFxB1NjENEZImI\nbBCRcfmeW1FExojIehH5EehazOtzh4hMKbRtvIiMjtweJCKLIj/P/yKt6qKOlSkinSK3q4nI85HY\nFgJtC+17p4j8GDnuQhH5fWR7C+BxoEOky2tdvtf2nnzPHxr52deLyKsiUi+a16YkItI9Es9GEXlX\nRE7M99jtIrJKRDaJyHf5ftbTRGReZPvPIvJItOdzcaCq/hWyL2AZcHahbfcDO4GLsDf1g4FTgFOx\nT3DHAN8D10X2rwQokBa5PwlYB6QDlYF/ApPKsO/hwGagW+Sxm4FdwIAifpZoYpwG1ALSgF/yfnbg\nOmAh0ACoA8yyP/n9nucYYAtQPd+x1wLpkfsXRfYRoAuwHWgZeexsYFm+Y2UCnSK3RwLvA4cCjYBv\nC+17KVAv8jvpE4nhiMhjg4D3C8U5CbgncvucSIwnA1WBJ4B3o3lt9vPz3w88G7ndJBJHl8jv6HZg\nceR2M2A5cGRk38bAMZHbs4Hekds1gVOD/l8oz1/eci9fPlLV6aqaq6rbVXW2qn6uqjmq+iPwJNCx\nmOe/rKpzVHUXMBlLKqXd90JgvqpOizw2Bnsj2K8oY/ybqmar6jIskead61JgjKpmqup64MFizvMj\n8A32pgPwO2CDqs6JPD5dVX9U8y7wDrDfi6aFXArcr6obVHU51hrPf96pqro68jt5AXtjTo/iuAB9\ngadVdb6q7gBuAzqKSIN8+xT12hSnF/Caqr4b+R09iL1BnArkYG8kzSJde0sjrx3Ym/TxIlJHVTer\n6udR/hwuDjy5ly8/5b8jIieJyBsiskZENgH3AocV8/w1+W5vo/iLqEXtWz9/HKqqWEt3v6KMMapz\nYS3O4rwA9I7c7hO5nxfHhSLyuYj8IiIbsVZzca9VnnrFxSAiA0Tkq0j3x0bgpCiPC/bz7Tmeqm4C\nNgBH5dunNL+zoo6bi/2OjlLVxcBw7PewNtLNd2Rk14FAU2CxiHwhIudH+XO4OPDkXr4UHgY4EWut\nHqeqhwB/xrod4mk11k0CgIgIBZNRYQcS42rg6Hz3SxqqORU4W0SOwlrwL0RiPBh4Gfgb1mVSG3gr\nyjjWFBWDiBwDZADXAHUix/0u33FLGra5CuvqyTteTaz7Z2UUcZXmuBWw39lKAFWdpKrtsS6Zitjr\ngqouVtVeWNfbKOBfIlL1AGNxZeTJvXyrCWQDW0WkCTAkAed8HWgjIheJSCXgBqBunGKcCtwoIkeJ\nSB3g1uJ2VtU1wEfAs8BiVf0h8lAV4CAgC9gtIhcCZ5UihttFpLbYPIDr8j1WA0vgWdj73NVYyz3P\nz0CDvAvI+/EicJWItBSRKliS/VBVi/wkVIqYfy8inSLnHoFdJ/lcRJqISOfI+bZHvnKxH+ByETks\n0tLPjvxsuQcYiysjT+7l23CgP/aPOxG78BlXqvozcBkwGlgPHAt8iY3Lj3WMGVjf+NfYxb6Xo3jO\nC9gF0j1dMqq6EbgJeAW7KNkDe5OKxt3YJ4hlwJvA/+U77gLgMeCLyD4nAvn7qd8GfgB+FpH83St5\nz/8P1j3ySuT5DbF++AOiqgux1zwDe+PpCvw+0v9eBXgYu06yBvukcEfkqecDi8RGY40ELlPVnQca\njysbsS5P54IhIhWxboAeqvph0PE4FxbecncJJyJdI90UVYC7sFEWXwQclnOh4sndBeEM4EfsI/+5\nQHdVLapbxjlXBt4t45xzIeQtd+ecC6HACocddthhmpaWFtTpnXMuJc2dO3edqhY3fBgIMLmnpaUx\nZ86coE7vnHMpSURKmmkNeLeMc86Fkid355wLIU/uzjkXQkm1EtOuXbvIzMxkx44dQYfiolC1alUa\nNGhA5cpFlT5xzgUlqZJ7ZmYmNWvWJC0tDSsW6JKVqrJ+/XoyMzNp3LhxyU9wziVUUnXL7Nixgzp1\n6nhiTwEiQp06dfxTlnNJKqmSO+CJPYX478q55JV0yd05FxvbtsHbb4NXGEkeqnDvvfDVV/E/lyf3\nfNavX8/JJ5/MySefzJFHHslRRx215/7OndGVpR44cCCLFy8udp/x48czefLkWITMGWecwfz582Ny\nLBcut94K55wDTz8ddCQOYPduGDIE7r4bpkxJwAmDWpm7bdu2Wti33367z7biTJqk2qiRqoh9nzSp\nVE8v1t13362PPPLIPttzc3N19+7dsTvRAWrfvr1++eWXgZ2/tL+zeNq6VbVjR9U33ww6kuCtX69a\nrZpq5cqqVaqoBvgn4lR1xw7VSy5RBdXbb1fNzS37sYA5GkWOTdmW++TJMHgwLF9uH3WWL7f7MWoQ\nF7BkyRKaNm1K3759adasGatXr2bw4MGkp6fTrFkz7r333j375rWkc3JyqF27NrfddhutWrXi9NNP\nZ+3atQDceeedjB07ds/+t912G+3atePEE0/kk08+AWDr1q1ccsklNG3alB49epCenl5iC33SpEm0\naNGC5s2bc/vttwOQk5PD5Zdfvmf7uHHjABgzZgxNmzalZcuW9OvXL+avWRCmTIEPPoDbb/euiIkT\nrVvmP/+Bww6Dnj0hOzvoqMqnzZvh/PPhX/+C0aPhgQcgIZeronkHiMfXgbbcGzWyd8HCX40aRX2I\nYuVvuf/www8qIjp79uw9j69fv15VVXft2qVnnHGGLly4UFX3tqR37dqlgM6YMUNVVW+66Sb929/+\npqqqd9xxh44ZM2bP/n/84x9VVXXatGl67rnnqqrq3/72N7322mtVVXX+/PlaoUKF/bbQ8873008/\naaNGjTQrK0t37typZ555pk6fPl0/++wz7dq16579N2zYoKqqRx55pP76668FtpVFMrXc09NVDzrI\n/g7efjvoaILz66+q9eqp/u53dv/DD1UrVlTt0ePAWoyu9Nautb/LihVVn3suNsck7C33FStKt/1A\nHXvssaSnp++5/+KLL9KmTRvatGnDokWL+Pbbb/d5zsEHH8x5550HQNu2bVm2bNl+j33xxRfvs89H\nH31Er169AGjVqhXNmjUrNr7PP/+cLl26cNhhh1G5cmX69OnDrFmzOO6441i8eDHXX389M2fOpFat\nWgA0a9aMfv36MXny5FBMQpozx77++lc48kh45JGgIwrOiy/C6tUwfLjdP+MMe11efhkefzzY2MqT\nFSugQwf45ht45RW44orEnj9lk3vDhqXbfqCqV6++5/YPP/zAo48+yrvvvsuCBQvo2rXrfsd7H3TQ\nQXtuV6xYkZycnP0eu0qVKiXuU1Z16tRhwYIFdOjQgfHjxzNkyBAAZs6cydChQ5k9ezbt2rVj9+7d\nMT1vok2YANWqwaBBcMMN8NZbiRmRkGxUYdQoaN7cLqbmueUWuPBCS/hf+IKGcfftt/Db38KaNfa3\neNFFiY8hZZP7Aw/YP3N+1arZ9njbtGkTNWvW5JBDDmH16tXMnDkz5udo3749U6dOBeDrr7/e7yeD\n/E499VTee+891q9fT05ODlOmTKFjx45kZWWhqvTs2ZN7772XefPmsXv3bjIzM+nSpQsPP/ww69at\nY9u2bTH/GRJl40Z44QXo0wdq1YKhQ6FGjfLZen/nHfj6a7j55oL9uhUqwHPPQb16cOml8MsvwcUY\ndp9/bi32nBy7BtShQzBxJFX5gdLo29e+33GHffxp2NASe972eGrTpg1NmzblpJNOolGjRrRv3z7m\n5xg2bBhXXHEFTZs23fOV16WyPw0aNOC+++6jU6dOqCoXXXQRF1xwAfPmzeOqq65CVRERHnroIXJy\ncujTpw+bN28mNzeXW265hZo1a8b8Z0iU55+H7dstqQPUrg1XXw3jxll3RLw+zSWjUaOsW6pPn30f\n+81vYOpUSzb9+8O0aZb0Xey8/TZ07w5HHGEt9mOPDTCYaDrm4/EVi6GQYbZr1y7dvn27qqp+//33\nmpaWprt27Qo4qn0F/TvLzVVt0kT1lFMKbl++XLVSJdWbbgomriB8841dTL7//uL3e/RR2+/hhxMT\nV3nxz3/a0NOWLVVXrYrfeQj7BdWw27JlC+3bt6dVq1ZccsklTJw4kUqVUvaDVtx8+CEsWgTXXFNw\ne8OG0KsXPPkkbNgQTGyJNno0HHzw3k8wRRk2DC65BP70J/j448TEFnYZGfb3duqp1hVTr17QEaVw\nn3vY1a5dm7lz5/LVV1+xYMECzsl/dcztkZFh3TCXXbbvY7fcAlu32sXWsFuzBiZNgoEDoU6d4vcV\ngb//HdLS7HXLykpIiKGkCvfdB9dea2PZZ860v8dk4Mndpayff7aJIf3773txHaBVKzj3XOt7D3vx\nyvHjYdcuuOmm6PavVQteegnWrYN+/SA3N77xhVFuLtx4I/z5z3D55TbccX9/h0Hx5O5S1jPPWEIr\nrhtixIi9rdqw2rbNPsF06wbHHRf981q3tje+t96yC88uert22bj1ceMswT/7LCTbdBFP7i4l7d5t\nU+w7d4aTTip6vy5dLImNHBne1ulzz8H69XsnLZXG1VfbCLO774Z33419bGG0bZu9kU6ebCP0Ro9O\nzlFHSRiScyWbOROWLSv54qEI/PGPsHgxvP56QkJLqNxcGDMG2rWDsozIFbFrEiecYMMnV6+OfYxh\nsmED/O53VrNn4kSrY5Ssyxp4cs+nc+fO+0xIGjt2LNcUHopRSI0aNQBYtWoVPXr02O8+nTp1Ys6c\nOcUeZ+zYsQUmE51//vls3LgxmtCLdc899zBy5MgDPk4ymTDBxhL/4Q8l79ujh108fPjhuIeVcNOn\nww8/WKu9rEmmRg0rTbBpE/TubZNv3L5WrYIzz7QyF1OnWqHCZObJPZ/evXszpVCh5SlTptC7d++o\nnl+/fn1efvnlMp+/cHKfMWMGtZPl0nsSWbEC3njDSg3kq/BQpEqV7ELjxx9DpOhmaIwaBY0aQaQ8\nUZk1a2b99h98APfcE5PQQmXJEvtktHSp/e0V0YZLKp7c8+nRowdvvPHGnoU5li1bxqpVq+jQoQNb\ntmzhrLPOok2bNrRo0YJp06bt8/xly5bRvHlzALZv306vXr1o0qQJ3bt3Z/v27Xv2u+aaa/aUC777\n7rsBGDduHKtWraJz58507twZgLS0NNatWwfA6NGjad68Oc2bN99TLnjZsmU0adKEq6++mmbNmnHO\nOecUOM/+zJ8/n9NOO42WLVvSvXt3NkQGgY8bN25PCeC8gmUffPDBnsVKWrduzebNm8v82sbSk0/a\nELSrr47+OVddZTM0w1SSYPZsG+d/4432Bnag+ve31+mBB+DNNw/8eGHx5ZeW2Ddvhvfeg7PPDjqi\nKEUz0ykeXyXNUL3hBlt4IZZfN9xQ8uyvCy64QF999VVVtbK7w4cPV1WbMZqdna2qqllZWXrsscdq\nbqR+avXq1VVVdenSpdqsWTNVVR01apQOHDhQVVW/+uorrVix4p6SwXnlgnNycrRjx4761Vdfqaru\nKdmbJ+/+nDlztHnz5rplyxbdvHmzNm3aVOfNm6dLly7VihUr7ikF3LNnT33++ef3+Znyly9u0aKF\nvv/++6qqetddd+kNkRelXr16umPHDlXdWwL4wgsv1I8++khVVTdv3rzfGbKJnqG6c6fqEUeoXnhh\n6Z975522sMvixbGPKwiXXaZaq5bqpk2xO+a2bTbDsk4d1RUrYnfcVPX++6qHHKLaoIHqokVBR2Pw\nGaplk79rJn+XjKpy++2307JlS84++2xWrlzJzz//XORxZs2atWcRjJYtW9KyZcs9j02dOpU2bdrQ\nunVrFi5cWGJRsI8++oju3btTvXp1atSowcUXX8yHH34IQOPGjTn55JOB4ssKA2RnZ7Nx40Y6duwI\nQP/+/Zk1a9aeGPv27cukSZP2zIRt3749N998M+PGjWPjxo1JMUP21VdtfHsJl0H267rrrBtn1KjY\nx5Voy5dbP/ngwRDLskAHH2zj33/91SY47doVu2Onmtdes3kS9etbd15xo7KSUfD/rUWI9DwkXLdu\n3bjpppuYN28e27Zto23btgBMnjyZrKws5s6dS+XKlUlLS9tvmd+SLF26lJEjRzJ79mwOPfRQBgwY\nUKbj5MkrFwxWMrikbpmivPHGG8yaNYvp06fzwAMP8PXXX3PbbbdxwQUXMGPGDNq3b8/MmTM5KeC/\n8IwM62M+99zSP/eII2DAABuTfO+9dj9VPfqoXUC9/vrYH/uEE2zd1V69rERByK7FR+XZZ+2aTps2\nMGOGrWaVarzlXkiNGjXo3LkzV155ZYELqdnZ2Rx++OFUrlyZ9957j+XLlxd7nDPPPJMXXngBgG++\n+YYFCxYAVi64evXq1KpVi59//pk383Vu1qxZc7/92h06dODVV19l27ZtbN26lVdeeYUOZagjWqtW\nLQ499NA9rf7nn3+ejh07kpuby08//UTnzp156KGHyM7OZsuWLfzvf/+jRYsW3HrrrZxyyil89913\npT5nLH33nfV5DhkCFSuW7RjDh8POnfDYY7GNLZGysy35XnYZNGgQn3NcdplNqR81yqpHlicjR1oZ\nh86drYRyKiZ2iKLlLiL/AC4E1qpq8/08LsCjwPnANmCAqs6LdaCJ1Lt3b7p3715g5Ezfvn256KKL\naNGiBenp6SW2YK+55hoGDhxIkyZNaNKkyZ5PAK1ataJ169acdNJJHH300QXKBQ8ePJiuXbtSv359\n3nvvvT3b27Rpw4ABA2jXrh0AgwYNonXr1sV2wRTlueeeY+jQoWzbto1jjjmGZ555ht27d9OvXz+y\ns7NRVa6//npq167NXXfdxXvvvUeFChVo1qzZnlWlgjJxos0CvPLKsh/j+ONt+OQTT8Btt9kwwFTz\n1FN2ce/mm+N7ntGjrTZ5//4wbx4cc0x8zxc0Vfuk8tBDNhpm0iTI98E49ZTUKQ+cCbQBvini8fOB\nNwEBTgM+j6az30v+hkOifmdbt6rWrm0XEQ/Up59ayduxYw/8WIm2c6dd3OvUKTHn+9//7KJt27aq\nkevtobRrl+pVV9nfxZAhqjk5QUdUNGJ1QVVVZwHFrdvSDfi/yHk/A2qLSBIUvHRhMnWqrbhUlgup\nhZ12mq0rOmZM6k3YeeklyMwsW6mBsjjmGOt/njs3cedMtB07bHWqv/8d7rzTruuUtdsvmcSiz/0o\n4Kd89zMj2/YhIoNFZI6IzMnyOqOuFDIyoEkTmyEYCyNG2IiTl16KzfESIW991BNPtPKyifKHP1gX\n0Pjx8M9/Ju68ibBpk72Wr7xigzjuuy95ywmUVkIvqKrqk6qarqrpdevWLWqfRIbkDkCiflfz5tmi\nzkOHxu4f78ILbWjbww9b0kwFH3xgr8XNNye+UNWDD8Lpp9vEse+/T+y542XtWrtoOmuWLdV4ww1B\nRxRbsfgTWQkcne9+g8i2UqtatSrr16/3BJ8CVJX169dTtWrVuJ9rwgQbf33FFbE7ZoUKtpjH/Pk2\nIiIVjB4Ndeta7fBEq1zZWu0HHQQ9e9qatals+XLrmvv2WxsNFJmSEiqxGOf+GnCdiEwBTgWyVbVM\nteUaNGhAZmYm3mWTGqpWrUqDeI3Fi8jOttKqvXvHfoWbfv2sj/WRR5J/SvnixVYk7O677Y0uCEcf\nbS3c88+38fVPPRVMHAdq4UKbJ7F1qy1ofcYZQUcUH9EMhXwR6AQcJiKZwN1AZQBVnQDMwEbMLMGG\nQg4sazCVK1emcePGZX26C6Hnn7f62bG4kFpYlSr2UfxPf7IWfGSib1IaM8bivfbaYOM47zwrc/vX\nv0KHDrH9NJUIn31mb05Vqlg3V76J46EjQXWBpKena0klcF35pgotWlhLdfbs+Jxj40ZrkXbrlryr\nNWVl2YLfl19uRdOClpNjn3Rmz7ZrIc2aBR1RyXbvtiUZBw6EI4+0FnuqjtsXkbmqml7Sfj5D1SWt\njz6yj9AlLchxIGrXtouEU6ZYP2wyysiw4XrxnrQUrUqV4MUXbQJYz56wZUvQERVtzRr7lHHccTbr\n9vjjrfRzqib20vDk7pJWRoYt5BypQBw3N95oo3CCqmdUnB07bAjiBRckV+GqevUswX/3nb35JtMY\niNxca5n36GGfyu64Axo3tgvCX3xhLffywJO7S0pr11rVw/79oXr1+J6rYUN7A3nqKVtGLZlMmmSv\nRTJOIOrSBf7yF7vg/fTTQUdjr9PDD1vhs3POgffft2sqixfb+rCXXhrd4i5h4cndJaVnnrFys0OG\nJOZ8t9xioycyMhJzvmjk5trwx9atoVOnoKPZvzvusEQ6bJhdlE40VSsm16uXFVG79VY46ih7w8nM\ntCJgJ5yQ+LiSgV9QdUknN9f6SBs2tNZXonTtaglq2TJIwPD9Es2YYd0xkyZB375BR1O0rCwbaVSt\nmq0vWqtW/M+5fj0895xdYF682K6d9O9v9e2bNo3/+YPkF1RdynrrLVurMh7DH4szYoQtBJIso2ZG\njbLW6KWXBh1J8erWtQvSS5daDfR4tRdVbVnBfv2sdT58ONSpY0l+1Sq7ZhL2xF4antxd0snIgMMP\nh+7dE3veLl1scYaRI+3TQ5Dmz7d+4uuvt9mhya5DBxuV8vLL8PjjsT32hg22OEnz5lZbaPp0G+G0\nYIGNfLniiuAmdiUzT+4uqaxYAa+/bgs1J/ril4i13vNmgwZp1CgbaliaRcCDdsstVrNn+HAblXIg\nVOHTT23lrPr1bURTjRpWuXHVKltspUWLmIQdWp7cXVJ5+mn7xx48OJjz9+gBaWlWkiAomZnWzTFo\nUOxLLsRThQrWRVKvnnUl/VJcofAiZGfb0M9WreC3v7WJRwMGwJdf2sIhV14Z/9FTYeHJ3SWNXbss\nuZ93niXYIFSqZJOFPv7YFkUOwmOPWbdQKlYp/M1vrPb+qlV2gTOa7i1Vm+06aJC10q+7zrqiJk60\n42RkJHdpiGTlyd0ljWnTYPXqxF9ILezKKy1JBdF637zZklreJ4hUdOqpdt3i9dete6komzfbaJe2\nbaFdO5sU1aePJfq5c+3TW82aiYs7bDy5u6QxYYINfwx4qVaqV7cCXdOmWf97Iv3jH9Y1kYyTlkpj\n2DC45BIryvbxxwUf+/JLm9Vav77NY9i927piVq2yiWTpJQ7yc9Hwce4uKXz/va0wdP/9NjEmaGvX\n2hvNFVckrlhXTo7VPmnQwIb8pbrsbGuV79hhdYLeecc+lcyebaNbLrvMkvupp4Zn9aNE8HHuLqVM\nnGj93VddFXQk5vDD7ULe//2fjX1PhFdesQlUqd5qz1Orli1juG6d1XYZNMhmAT/6KKxcabOQTzvN\nE3u8eHJ3gdu+3f7RL744uYo6DR8OO3faBc54y1sf9bjj4KKL4n++RGnd2mryX3WVfRr55hsbu3/o\noUFHFn6e3F3gpk61iSrxLO1bFscfbxOpnngi/mVtP/nEhvrddBNUrBjfcyVaz542CuqMM7yVnkie\n3F3gJkywcrbJWBxrxAh74/n73+N7nlGjrDXbv398z+PKD0/uLlDz59vSZ0OHJmer7rTTrMU5erSN\nw4+H//0PXn3VhoD6BB0XK57cXaAyMmzkRDKvxfnHP1pZhJdeis/xx461STvXXRef47vyyZO7C8ym\nTVZ3u1ev5L7AlrcK0iOPxL7i4S+/2Nj2Pn1s2r5zseLJ3QVm0iQbGhf0jNSSVKhgRbHmz4f//je2\nx544EbZtS571UV14+CQmFwhVaNnSKj/OmZOc/e35/fqrlQNo0cLqzcfCzp17jzlzZmyO6cLPJzG5\npPbJJzbm+Zprkj+xA1SpYoW83n47dsvJvfii1dIJy6Qll1w8ubtAZGTAIYdA795BRxK9oUOtpngs\nCorlTVpq0QJ+97sDP55zhXlydwmXlWUjT664IrWG/tWubZUK//lPWL78wI713//C119bX3sqfHJx\nqceTu0u4Z5+1/uZkm5EajRtvtGQ8duyBHWfUKCu1kEqfXFxq8eTuEio310aInHkmNGsWdDSld/TR\nNnTzqads5mpZfPONXUAdNsz68p2LB0/uLqHefttmZKZiqz3PiBE2hDMjo2zPHz0aqlVL7dfAJT9P\n7i6hJkyAunWtAmSqatkSzj0Xxo2zWuWlsWaNTdwaONBWe3IuXjy5u4TJzITXXrPyr6neHTFihNV5\nf/750j3v8cetRs2NN8YnLufyeHJ3CfPUUzYEcPDgoCM5cF26QJs2dmE0mkWgYW9Xzh/+YHXbnYsn\nT+4uIXbtsuTetautypPqRKz1vngxTJ8e3XOee85qyfikJZcIntxdQkyfbrMxk72OTGn06GHlAx5+\nuOR9d++GMWNsvdDf/jbuoTnnyd0lRkaGDSM8//ygI4mdSpVsEtInn9hXcaZPhyVLrNXuk5ZcInhy\nd3H3ww82I3Pw4PAtIXfllTbqpaSSBKNGWSu/e/eEhOWcJ3cXfxMnWiv3qquCjiT2qleHa6+FadOs\n/31/vvgCPvrIRshUqpTY+Fz5FVVyF5GuIrJYRJaIyG37ebyWiEwXka9EZKGIDIx9qC4Vbd8Ozzxj\nI0TCuhipHrhCAAAPpElEQVTFsGFWunjUqP0/PmoU1KplrXznEqXE5C4iFYHxwHlAU6C3iDQttNv/\nA75V1VZAJ2CUiBwU41hdCnr5ZRshEqYLqYUdfjgMGGCjYdasKfjYsmX2GgwZAjVrBhGdK6+iabm3\nA5ao6o+quhOYAnQrtI8CNUVEgBrAL0BOTCN1KSkjA044ATp3DjqS+Bo+3IZ7PvZYwe3jxtlKTsOG\nBROXK7+iSe5HAT/lu58Z2Zbf40ATYBXwNXCDqu4ztUNEBovIHBGZk5WVVcaQXar46iv49FOroRL2\nESLHH28XS594ArZssW3Z2fD001ZorEGDYONz5U+sLqieC8wH6gMnA4+LyCGFd1LVJ1U1XVXT69at\nG6NTu2Q1YQJUrQr9+wcdSWKMGAEbN1pCB5u0tXmzr4/qghFNcl8JHJ3vfoPItvwGAv9WswRYCpwU\nmxBdKtq82RbAvuyy8lMg67TToEMHm6y0fTs8+qh1R7VuHXRkrjyKJrnPBo4XkcaRi6S9gNcK7bMC\nOAtARI4ATgR+jGWgLrVMmmTdE2G+kLo/I0bAihXWFZOZ6aUGXHBEVUveSeR8YCxQEfiHqj4gIkMB\nVHWCiNQHngXqAQI8qKqTijtmenq6zpkz5wDDd8lIFVq1sjHdc+eGv789v9xcW4Tku+/gpJNg4UK7\noOpcrIjIXFVNL2m/qKZUqOoMYEahbRPy3V4FnFPaIF04ffqprQ86cWL5SuxgiXzECJuwdfPNnthd\ncHy+XIpbs8bqijdtCpUrBx2NyciwMd19+gQdSTAGDIAjjrAKmM4FxZN7ilq5Eh58EJ580habrlrV\n6ou3awennGLfjz028S3ndevgpZdg0CCoUSOx504WFSrABRcEHYUr7zy5p5hVq/Ym9d27bbm2jh2t\nb/uLL6wrZOxY2/fQQy3J50/4RxwR3/iefRZ+/dXXB3UuaFFdUI0Hv6BaOqtXw0MPWfLetcs++t9x\nx74LX+Tk2EW8L76wr9mzrf87b7Wghg0LJvu2bWM3LT43F048EY48Ej78MDbHdM4VFNMLqi44a9ZY\nUp8wwZJ6//6W1I85Zv/7V6pkI1VatYKrr7ZtW7fCl1/uTfZffGH1TsC6bZo2LZjwW7SwQlil9c47\nVrP8L38p28/qnIsdb7knqZ9/thV+MjKsT/3yy+HOO60fPRbWrbNEn5fsv/gC8ipCVKliE2/ykn27\ndrbmZ0kjPy6+2FrsmZmpvwC2c8kq2pa7J/cks3atLfwwfrz1Xecl9XgvqKwKy5cXbN3PmQPbttnj\ntWtbss+f8POX8F25Eho1skk7Dz0U31idK8+8WybFZGXtTeo7dkDfvnDXXVaQKhFEbKWgtDS49FLb\nlpMDixYVbN0//LBtByuGlded88MPdoF3yJDExOucK15KJffJk62/ecUKuzD4wAOWBFPZunUwciQ8\n/rjVI+nd25L6iScGHZn137doYV95C01s3w7z5xe8YPvvf9tjXbsWfS3AOZdYKZPcJ0+2NTjzugmW\nL7f7kJoJfv16S+qPPWY/U15SPynJy60dfDCcfrp95fnlF7tg27x5cHE55wpKmT73tDRL6IU1amSr\n3aSKX36xZdfGjbNRLJddBn/+MzRpEnRkzrlUELo+9xUrSrc92fzyC4webUl9yxbr177rLisy5Zxz\nsZYyyb1hw/233Bs2THwspbFhg9X3fvRR2LQJeva0lrp3YTjn4illatY98ABUq1ZwW7Vqtj0ZbdwI\n99xjM0jvuw/OOQcWLICpUz2xO+fiL2WSe9++Vk+lUSMbtteokd1Ptoup2dk2QzMtzb6fdZatJfrS\nSzbqJJYmT7bzVKhg3ydPju3xnXOpK2WSO1giX7bMapgsWxZcYt9fUt20yVroaWnWYu/c2UaQ/Otf\n0LJlfGIYPNi6qvImIA0e7AneOWdSZrRMsig8JBOsjvpBB9nol27d4O67479uZlhGDznnSid0o2Xy\nbNgAP0ZWZ817X8r//lR4W3GPlWXb8OEFEztYQa+8JeXatCndz1NWqT56yDkXXymX3P/7373T45PJ\njh2JS+yQuqOHnHOJkXLJvX17eO21vffzVhrKv+JQNNtKu3/e7d69rWJjYYlOqg88sG/3UDKPHnLO\nJVbKJff69e0rKKNGJUdSzbuYnAy1dsJY88e5VJdyyT1oyZRU+/YNPomGreaPc2Hho2XcAfFRO84l\nVrSjZVJqnLtLPj5qx7nk5MndHZCiLiT7qB3nguXJ3R2QVKv541x54cndHZBUqfnjXHnjo2XcAUuG\nUTvOuYK85e6ccyHkyd0550LIk7tzzoWQJ3fnnAshT+7OORdCntxdaPiyg87t5UMhXSh4ATPnCvKW\nuwuFO+7Yd4Wsbdtsu3PlUVTJXUS6ishiEVkiIrcVsU8nEZkvIgtF5IPYhulc8byAmXMFlZjcRaQi\nMB44D2gK9BaRpoX2qQ08AfxeVZsBPeMQq3NF8gJmzhUUTcu9HbBEVX9U1Z3AFKBboX36AP9W1RUA\nqro2tmE6VzwvYOZcQdEk96OAn/Ldz4xsy+8E4FAReV9E5orIFbEK0LloeAEz5wqK1WiZSkBb4Czg\nYOBTEflMVb/Pv5OIDAYGAzT0z8suxryAmXN7RdNyXwkcne9+g8i2/DKBmaq6VVXXAbOAVoUPpKpP\nqmq6qqbXrVu3rDE755wrQTTJfTZwvIg0FpGDgF7Aa4X2mQacISKVRKQacCqwKLahOueci1aJ3TKq\nmiMi1wEzgYrAP1R1oYgMjTw+QVUXich/gAVALvC0qn4Tz8Cdc84VTVQ1kBOnp6frnDlzAjm3c86l\nKhGZq6rpJe3nM1Sdcy6EPLk751wIeXJ3zrkQ8uTunHMh5MndOedCyJO7c86FkCd352LMV4RyycBX\nYnIuhnxFKJcsvOXuXAz5ilAuWXhydy6GfEUolyw8uTsXQ74ilEsWntydiyFfEcolC0/uzsWQrwjl\nkoWPlnEuxnxFKJcMvOXunHMh5MndOedCyJO7c86FkCd355wLIU/uzjkXQp7cnXMuhDy5O+dcCHly\nd865EPLk7pxzIeTJ3bkQ8gVDnJcfcC5kfMEQB95ydy50fMEQB57cnQsdXzDEgSd350LHFwxx4Mnd\nudDxBUMceHJ3LnR8wRAHPlrGuVDyBUOct9ydcy6EPLk751wIeXJ3zrkQ8uTunHMh5MndOedCyJO7\nc86FUFTJXUS6ishiEVkiIrcVs98pIpIjIj1iF6JzzrnSKjG5i0hFYDxwHtAU6C0iTYvY7yHgrVgH\n6ZxzrnSiabm3A5ao6o+quhOYAnTbz37DgH8Ba2MYn3POuTKIJrkfBfyU735mZNseInIU0B3IKO5A\nIjJYROaIyJysrKzSxuqccy5KsbqgOha4VVVzi9tJVZ9U1XRVTa9bt26MTu2cc66waGrLrASOzne/\nQWRbfunAFBEBOAw4X0RyVPXVmETpnHOuVKJJ7rOB40WkMZbUewF98u+gqo3zbovIs8Drntidcy44\nJXbLqGoOcB0wE1gETFXVhSIyVESGxjtA51zq8oW6gxNVyV9VnQHMKLRtQhH7DjjwsJxzqc4X6g6W\nz1B1zsWFL9QdLE/uzrm48IW6g+XJ3TkXF75Qd7A8uTvn4sIX6g6WJ3fnXFz4Qt3B8gWynXNx4wt1\nB8db7s45F0Ke3J1zLoQ8uTvnXAh5cnfOuRDy5O6ccyHkyd0550LIk7tzzoWQJ3fnnAshT+7OudAr\nj3XlfYaqcy7UymtdeW+5O+dCrbzWlffk7pwLtfJaV96Tu3Mu1MprXXlP7s65UCuvdeU9uTvnQq28\n1pX30TLOudArj3XlveXunHMh5MndOedCyJO7c86FkCd355wLIU/uzjkXQp7cnXMuhDy5O+dcgiSy\nOqWPc3fOuQRIdHVKb7k751wCJLo6pSd355xLgERXp/Tk7pxzCZDo6pSe3J1zLgESXZ3Sk7tzziVA\noqtT+mgZ55xLkERWp4yq5S4iXUVksYgsEZHb9vN4XxFZICJfi8gnItIq9qE655yLVonJXUQqAuOB\n84CmQG8RaVpot6VAR1VtAdwHPBnrQJ1zzkUvmpZ7O2CJqv6oqjuBKUC3/Duo6iequiFy9zOgQWzD\ndM45VxrRJPejgJ/y3c+MbCvKVcCb+3tARAaLyBwRmZOVlRV9lM4550olpqNlRKQzltxv3d/jqvqk\nqqaranrdunVjeWrnnHP5RDNaZiVwdL77DSLbChCRlsDTwHmqur6kg86dO3ediCyPNtAkdRiwLugg\nkoi/HgX567GXvxYFHcjr0SianURVi99BpBLwPXAWltRnA31UdWG+fRoC7wJXqOonZQw45YjIHFVN\nDzqOZOGvR0H+euzlr0VBiXg9Smy5q2qOiFwHzAQqAv9Q1YUiMjTy+ATgz0Ad4AkRAcjxX6RzzgUn\nqklMqjoDmFFo24R8twcBg2IbmnPOubLy8gMHxsfzF+SvR0H+euzlr0VBcX89Suxzd845l3q85e6c\ncyHkyd0550LIk3sZiMjRIvKeiHwrIgtF5IagYwqaiFQUkS9F5PWgYwmaiNQWkZdF5DsRWSQipwcd\nU5BE5KbI/8k3IvKiiFQNOqZEEpF/iMhaEfkm37bfiMjbIvJD5PuhsT6vJ/eyyQGGq2pT4DTg/+2n\nmFp5cwOwKOggksSjwH9U9SSgFeX4dRGRo4DrgXRVbY4Np+4VbFQJ9yzQtdC224B3VPV44J3I/Zjy\n5F4GqrpaVedFbm/G/nmLq7cTaiLSALgAm6FcrolILeBM4O8AqrpTVTcGG1XgKgEHRyZEVgNWBRxP\nQqnqLOCXQpu7Ac9Fbj8H/CHW5/XkfoBEJA1oDXwebCSBGgv8EcgNOpAk0BjIAp6JdFM9LSLVgw4q\nKKq6EhgJrABWA9mq+lawUSWFI1R1deT2GuCIWJ/Ak/sBEJEawL+AG1V1U9DxBEFELgTWqurcoGNJ\nEpWANkCGqrYGthKHj9ypItKX3A1706sPVBeRfsFGlVzUxqPHfEy6J/cyEpHKWGKfrKr/DjqeALUH\nfi8iy7Ba/11EZFKwIQUqE8hU1bxPci9jyb68OhtYqqpZqroL+Dfw24BjSgY/i0g9gMj3tbE+gSf3\nMhAroPN3YJGqjg46niCp6p9UtYGqpmEXyt5V1XLbMlPVNcBPInJiZNNZwLcBhhS0FcBpIlIt8n9z\nFuX4AnM+rwH9I7f7A9NifQJP7mXTHrgca6XOj3ydH3RQLmkMAyaLyALgZOCvAccTmMgnmJeBecDX\nWM4pV6UIRORF4FPgRBHJFJGrgAeB34nID9inmwdjfl4vP+Ccc+HjLXfnnAshT+7OORdCntydcy6E\nPLk751wIeXJ3zrkQ8uTunHMh5MndOedC6P8DVZcnq89nw2EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x217ae103630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The model quickly starts overfitting, unsurprisingly given the small number of training samples. Validation accuracy has high variance for \n",
    "the same reason, but seems to reach high 50s.\n",
    "\n",
    "Note that your mileage may vary: since we have so few training samples, performance is heavily dependent on which exact 200 samples we \n",
    "picked, and we picked them at random. If it worked really poorly for you, try picking a different random set of 200 samples, just for the \n",
    "sake of the exercise (in real life you don't get to pick your training data).\n",
    "\n",
    "We can also try to train the same model without loading the pre-trained word embeddings and without freezing the embedding layer. In that \n",
    "case, we would be learning a task-specific embedding of our input tokens, which is generally more powerful than pre-trained word embeddings \n",
    "when lots of data is available. However, in our case, we have only 200 training samples. Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 32)                320032    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,320,065\n",
      "Trainable params: 1,320,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "5000/5000 [==============================] - 4s 737us/step - loss: 0.6948 - acc: 0.4986 - val_loss: 0.6941 - val_acc: 0.4991\n",
      "Epoch 2/10\n",
      "5000/5000 [==============================] - 3s 698us/step - loss: 0.4190 - acc: 0.8848 - val_loss: 0.8166 - val_acc: 0.4976\n",
      "Epoch 3/10\n",
      "5000/5000 [==============================] - 4s 714us/step - loss: 0.0628 - acc: 0.9894 - val_loss: 1.0918 - val_acc: 0.5027\n",
      "Epoch 4/10\n",
      "5000/5000 [==============================] - 3s 690us/step - loss: 0.0118 - acc: 0.9986 - val_loss: 1.3010 - val_acc: 0.4992\n",
      "Epoch 5/10\n",
      "5000/5000 [==============================] - 3s 696us/step - loss: 0.0080 - acc: 0.9986 - val_loss: 1.4529 - val_acc: 0.5011\n",
      "Epoch 6/10\n",
      "5000/5000 [==============================] - 3s 699us/step - loss: 0.0041 - acc: 0.9982 - val_loss: 1.6526 - val_acc: 0.5030\n",
      "Epoch 7/10\n",
      "5000/5000 [==============================] - 3s 695us/step - loss: 0.0035 - acc: 0.9990 - val_loss: 1.7511 - val_acc: 0.5021\n",
      "Epoch 8/10\n",
      "5000/5000 [==============================] - 3s 695us/step - loss: 0.0017 - acc: 0.9988 - val_loss: 1.8910 - val_acc: 0.4978\n",
      "Epoch 9/10\n",
      "5000/5000 [==============================] - 4s 702us/step - loss: 0.0014 - acc: 0.9990 - val_loss: 2.0187 - val_acc: 0.4990\n",
      "Epoch 10/10\n",
      "5000/5000 [==============================] - 4s 709us/step - loss: 0.0014 - acc: 0.9990 - val_loss: 2.0723 - val_acc: 0.4984\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHO5mZhnkcDczy6D/BkXscGMbF76MAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x217ae66cd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRJKQwl1EJAkp3EVEkpDCXUQkCf1/KxxMkmHjy2gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x217af941898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Validation accuracy stalls in the low 50s. So in our case, pre-trained word embeddings does outperform jointly learned embeddings. If you \n",
    "increase the number of training samples, this will quickly stop being the case -- try it as an exercise.\n",
    "\n",
    "Finally, let's evaluate the model on the test data. First, we will need to tokenize the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = os.path.join(imdb_dir, 'test')\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(test_dir, label_type)\n",
    "    for fname in sorted(os.listdir(dir_name)):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname), encoding=\"utf8\")\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
    "y_test = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's load and evaluate the first model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 2s 71us/step\n",
      "['loss', 'acc']\n",
      "result= [1.183568579864502, 0.50452]\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('pre_trained_glove_model.h5')\n",
    "result = model.evaluate(x_test, y_test)\n",
    "print(model.metrics_names)\n",
    "print(\"result=\",result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get an appalling test accuracy of 54%. Working with just a handful of training samples is hard!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training a LSTM model\n",
    "\n",
    "The models used so far were just MLPs. Now let us try a LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train is (5000, 100)\n",
      "Shape of y_train is (5000,)\n",
      "\n",
      "Shape of x_val is (10000, 100)\n",
      "Shape of y_val is (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of x_train is\", x_train.shape)\n",
    "print(\"Shape of y_train is\", y_train.shape)\n",
    "\n",
    "print()\n",
    "print(\"Shape of x_val is\", x_val.shape)\n",
    "print(\"Shape of y_val is\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_words= 10000\n",
      "embedding_dim= 100\n",
      "input_length= 100\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 32)                17024     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,017,057\n",
      "Trainable params: 1,017,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "print(\"max_words=\", max_words)\n",
    "print(\"embedding_dim=\", embedding_dim)\n",
    "print(\"input_length=\", maxlen)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 10000 samples\n",
      "Epoch 1/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.6978 - acc: 0.4984 - val_loss: 0.6982 - val_acc: 0.4959\n",
      "Epoch 2/250\n",
      "5000/5000 [==============================] - 9s 2ms/step - loss: 0.6935 - acc: 0.5120 - val_loss: 0.6991 - val_acc: 0.4943\n",
      "Epoch 3/250\n",
      "5000/5000 [==============================] - 9s 2ms/step - loss: 0.6901 - acc: 0.5332 - val_loss: 0.7020 - val_acc: 0.4951\n",
      "Epoch 4/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.6870 - acc: 0.5400 - val_loss: 0.7061 - val_acc: 0.4951\n",
      "Epoch 5/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.6821 - acc: 0.5552 - val_loss: 0.6988 - val_acc: 0.4996\n",
      "Epoch 6/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.6773 - acc: 0.5680 - val_loss: 0.7005 - val_acc: 0.5018\n",
      "Epoch 7/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.6718 - acc: 0.5842 - val_loss: 0.7017 - val_acc: 0.5009\n",
      "Epoch 8/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.6646 - acc: 0.5978 - val_loss: 0.7066 - val_acc: 0.5005\n",
      "Epoch 9/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.6531 - acc: 0.6150 - val_loss: 0.7170 - val_acc: 0.4962\n",
      "Epoch 10/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.6431 - acc: 0.6372 - val_loss: 0.7151 - val_acc: 0.5007\n",
      "Epoch 11/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.6321 - acc: 0.6440 - val_loss: 0.7297 - val_acc: 0.4940\n",
      "Epoch 12/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.6196 - acc: 0.6586 - val_loss: 0.7369 - val_acc: 0.4971\n",
      "Epoch 13/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.6034 - acc: 0.6810 - val_loss: 0.7409 - val_acc: 0.5001\n",
      "Epoch 14/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.5907 - acc: 0.6914 - val_loss: 0.7580 - val_acc: 0.4987\n",
      "Epoch 15/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.5743 - acc: 0.7074 - val_loss: 0.7823 - val_acc: 0.4964\n",
      "Epoch 16/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.5532 - acc: 0.7260 - val_loss: 0.7812 - val_acc: 0.5032\n",
      "Epoch 17/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.5340 - acc: 0.7398 - val_loss: 0.8135 - val_acc: 0.4992\n",
      "Epoch 18/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.5144 - acc: 0.7566 - val_loss: 0.8382 - val_acc: 0.5031\n",
      "Epoch 19/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.4951 - acc: 0.7678 - val_loss: 0.8499 - val_acc: 0.5012\n",
      "Epoch 20/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.4725 - acc: 0.7818 - val_loss: 0.8793 - val_acc: 0.4951\n",
      "Epoch 21/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.4486 - acc: 0.7976 - val_loss: 0.8943 - val_acc: 0.5068\n",
      "Epoch 22/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.4284 - acc: 0.8106 - val_loss: 0.9375 - val_acc: 0.4979\n",
      "Epoch 23/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.4007 - acc: 0.8284 - val_loss: 0.9673 - val_acc: 0.5008\n",
      "Epoch 24/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.3803 - acc: 0.8404 - val_loss: 1.0188 - val_acc: 0.5022\n",
      "Epoch 25/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.3562 - acc: 0.8480 - val_loss: 1.0267 - val_acc: 0.5040\n",
      "Epoch 26/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.3300 - acc: 0.8660 - val_loss: 1.0944 - val_acc: 0.5069\n",
      "Epoch 27/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.3150 - acc: 0.8732 - val_loss: 1.0892 - val_acc: 0.5006\n",
      "Epoch 28/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.2940 - acc: 0.8836 - val_loss: 1.1245 - val_acc: 0.5062\n",
      "Epoch 29/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.2686 - acc: 0.8998 - val_loss: 1.1542 - val_acc: 0.5002\n",
      "Epoch 30/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.2555 - acc: 0.9050 - val_loss: 1.2098 - val_acc: 0.5010\n",
      "Epoch 31/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.2300 - acc: 0.9190 - val_loss: 1.2539 - val_acc: 0.5056\n",
      "Epoch 32/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.2108 - acc: 0.9226 - val_loss: 1.3405 - val_acc: 0.5041\n",
      "Epoch 33/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.1929 - acc: 0.9356 - val_loss: 1.3997 - val_acc: 0.5012\n",
      "Epoch 34/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.1765 - acc: 0.9444 - val_loss: 1.4767 - val_acc: 0.4985\n",
      "Epoch 35/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.1595 - acc: 0.9504 - val_loss: 1.4842 - val_acc: 0.5036\n",
      "Epoch 36/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.1464 - acc: 0.9534 - val_loss: 1.5745 - val_acc: 0.5066\n",
      "Epoch 37/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.1381 - acc: 0.9566 - val_loss: 1.6672 - val_acc: 0.4995\n",
      "Epoch 38/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.1236 - acc: 0.9654 - val_loss: 1.6265 - val_acc: 0.5021\n",
      "Epoch 39/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.1073 - acc: 0.9712 - val_loss: 1.6969 - val_acc: 0.5079\n",
      "Epoch 40/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.1016 - acc: 0.9718 - val_loss: 1.7426 - val_acc: 0.5058\n",
      "Epoch 41/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0928 - acc: 0.9776 - val_loss: 1.8128 - val_acc: 0.5060\n",
      "Epoch 42/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0858 - acc: 0.9770 - val_loss: 1.8745 - val_acc: 0.5037\n",
      "Epoch 43/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0791 - acc: 0.9798 - val_loss: 1.9318 - val_acc: 0.5066\n",
      "Epoch 44/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0750 - acc: 0.9790 - val_loss: 2.0370 - val_acc: 0.5065\n",
      "Epoch 45/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0727 - acc: 0.9820 - val_loss: 2.0525 - val_acc: 0.5046\n",
      "Epoch 46/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0657 - acc: 0.9844 - val_loss: 2.1053 - val_acc: 0.5000\n",
      "Epoch 47/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0620 - acc: 0.9856 - val_loss: 2.0744 - val_acc: 0.5030\n",
      "Epoch 48/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0602 - acc: 0.9852 - val_loss: 2.1774 - val_acc: 0.5054\n",
      "Epoch 49/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0512 - acc: 0.9898 - val_loss: 2.2290 - val_acc: 0.5033\n",
      "Epoch 50/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0514 - acc: 0.9886 - val_loss: 2.1975 - val_acc: 0.5066\n",
      "Epoch 51/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0494 - acc: 0.9868 - val_loss: 2.3004 - val_acc: 0.5006\n",
      "Epoch 52/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0426 - acc: 0.9912 - val_loss: 2.2779 - val_acc: 0.5090\n",
      "Epoch 53/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0444 - acc: 0.9904 - val_loss: 2.4254 - val_acc: 0.5065\n",
      "Epoch 54/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0376 - acc: 0.9912 - val_loss: 2.4411 - val_acc: 0.5031\n",
      "Epoch 55/250\n",
      "5000/5000 [==============================] - 12s 2ms/step - loss: 0.0421 - acc: 0.9890 - val_loss: 2.4816 - val_acc: 0.5031\n",
      "Epoch 56/250\n",
      "5000/5000 [==============================] - 12s 2ms/step - loss: 0.0352 - acc: 0.9912 - val_loss: 2.5847 - val_acc: 0.5086\n",
      "Epoch 57/250\n",
      "5000/5000 [==============================] - 12s 2ms/step - loss: 0.0371 - acc: 0.9882 - val_loss: 2.4155 - val_acc: 0.5053\n",
      "Epoch 58/250\n",
      "5000/5000 [==============================] - 12s 2ms/step - loss: 0.0379 - acc: 0.9906 - val_loss: 2.4996 - val_acc: 0.5014\n",
      "Epoch 59/250\n",
      "5000/5000 [==============================] - 12s 2ms/step - loss: 0.0340 - acc: 0.9918 - val_loss: 2.5522 - val_acc: 0.5025\n",
      "Epoch 60/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0327 - acc: 0.9924 - val_loss: 2.5873 - val_acc: 0.5078\n",
      "Epoch 61/250\n",
      "5000/5000 [==============================] - 12s 2ms/step - loss: 0.0346 - acc: 0.9904 - val_loss: 2.5123 - val_acc: 0.5048\n",
      "Epoch 62/250\n",
      "5000/5000 [==============================] - 12s 2ms/step - loss: 0.0293 - acc: 0.9940 - val_loss: 2.7183 - val_acc: 0.5020\n",
      "Epoch 63/250\n",
      "5000/5000 [==============================] - 12s 2ms/step - loss: 0.0259 - acc: 0.9946 - val_loss: 2.6596 - val_acc: 0.5077\n",
      "Epoch 64/250\n",
      "5000/5000 [==============================] - 12s 2ms/step - loss: 0.0277 - acc: 0.9922 - val_loss: 2.6975 - val_acc: 0.5056\n",
      "Epoch 65/250\n",
      "5000/5000 [==============================] - 12s 2ms/step - loss: 0.0303 - acc: 0.9910 - val_loss: 2.7350 - val_acc: 0.5018\n",
      "Epoch 66/250\n",
      "5000/5000 [==============================] - 12s 2ms/step - loss: 0.0308 - acc: 0.9918 - val_loss: 2.6880 - val_acc: 0.5063\n",
      "Epoch 67/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0240 - acc: 0.9930 - val_loss: 2.7833 - val_acc: 0.5048\n",
      "Epoch 68/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0294 - acc: 0.9924 - val_loss: 2.7333 - val_acc: 0.5128\n",
      "Epoch 69/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0286 - acc: 0.9916 - val_loss: 2.7860 - val_acc: 0.5075\n",
      "Epoch 70/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0219 - acc: 0.9944 - val_loss: 2.8478 - val_acc: 0.5045\n",
      "Epoch 71/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0293 - acc: 0.9918 - val_loss: 2.8945 - val_acc: 0.5046\n",
      "Epoch 72/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.0332 - acc: 0.9896 - val_loss: 2.7916 - val_acc: 0.5045\n",
      "Epoch 73/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0252 - acc: 0.9928 - val_loss: 2.7959 - val_acc: 0.5066\n",
      "Epoch 74/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0211 - acc: 0.9952 - val_loss: 2.8414 - val_acc: 0.5057\n",
      "Epoch 75/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0299 - acc: 0.9902 - val_loss: 2.9452 - val_acc: 0.5029\n",
      "Epoch 76/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0247 - acc: 0.9938 - val_loss: 2.9236 - val_acc: 0.5030\n",
      "Epoch 77/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.0243 - acc: 0.9932 - val_loss: 2.8869 - val_acc: 0.5069\n",
      "Epoch 78/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0193 - acc: 0.9952 - val_loss: 2.9310 - val_acc: 0.5008\n",
      "Epoch 79/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0198 - acc: 0.9948 - val_loss: 2.9471 - val_acc: 0.5076\n",
      "Epoch 80/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0185 - acc: 0.9948 - val_loss: 2.9306 - val_acc: 0.5090\n",
      "Epoch 81/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0255 - acc: 0.9928 - val_loss: 2.9846 - val_acc: 0.5061\n",
      "Epoch 82/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0205 - acc: 0.9950 - val_loss: 2.9899 - val_acc: 0.5064\n",
      "Epoch 83/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0170 - acc: 0.9954 - val_loss: 3.0794 - val_acc: 0.5038\n",
      "Epoch 84/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0268 - acc: 0.9920 - val_loss: 3.0386 - val_acc: 0.5018\n",
      "Epoch 85/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0199 - acc: 0.9936 - val_loss: 3.0919 - val_acc: 0.5055\n",
      "Epoch 86/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0166 - acc: 0.9948 - val_loss: 2.9971 - val_acc: 0.5033\n",
      "Epoch 87/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0250 - acc: 0.9926 - val_loss: 3.1271 - val_acc: 0.5036\n",
      "Epoch 88/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0133 - acc: 0.9964 - val_loss: 3.1884 - val_acc: 0.5037\n",
      "Epoch 89/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0231 - acc: 0.9928 - val_loss: 3.1761 - val_acc: 0.5048\n",
      "Epoch 90/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0213 - acc: 0.9936 - val_loss: 3.2003 - val_acc: 0.5047\n",
      "Epoch 91/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0200 - acc: 0.9936 - val_loss: 3.1814 - val_acc: 0.5041\n",
      "Epoch 92/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0206 - acc: 0.9944 - val_loss: 3.1280 - val_acc: 0.5057\n",
      "Epoch 93/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0192 - acc: 0.9930 - val_loss: 3.1344 - val_acc: 0.5031\n",
      "Epoch 94/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0221 - acc: 0.9940 - val_loss: 3.1947 - val_acc: 0.5040\n",
      "Epoch 95/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0177 - acc: 0.9944 - val_loss: 3.2794 - val_acc: 0.5047\n",
      "Epoch 96/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0178 - acc: 0.9948 - val_loss: 3.2736 - val_acc: 0.4985\n",
      "Epoch 97/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0176 - acc: 0.9946 - val_loss: 3.2135 - val_acc: 0.5061\n",
      "Epoch 98/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0174 - acc: 0.9954 - val_loss: 3.1923 - val_acc: 0.4988\n",
      "Epoch 99/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0194 - acc: 0.9938 - val_loss: 3.1791 - val_acc: 0.5060\n",
      "Epoch 100/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0162 - acc: 0.9958 - val_loss: 3.2236 - val_acc: 0.5053\n",
      "Epoch 101/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0193 - acc: 0.9934 - val_loss: 3.2560 - val_acc: 0.5002\n",
      "Epoch 102/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0141 - acc: 0.9954 - val_loss: 3.3410 - val_acc: 0.5029\n",
      "Epoch 103/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0144 - acc: 0.9960 - val_loss: 3.3392 - val_acc: 0.5009\n",
      "Epoch 104/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0181 - acc: 0.9950 - val_loss: 3.3527 - val_acc: 0.5032\n",
      "Epoch 105/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.0210 - acc: 0.9938 - val_loss: 3.3438 - val_acc: 0.5058\n",
      "Epoch 106/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0149 - acc: 0.9964 - val_loss: 3.2879 - val_acc: 0.5007\n",
      "Epoch 107/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.0155 - acc: 0.9956 - val_loss: 3.2830 - val_acc: 0.5012\n",
      "Epoch 108/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0152 - acc: 0.9940 - val_loss: 3.4128 - val_acc: 0.5073\n",
      "Epoch 109/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0171 - acc: 0.9950 - val_loss: 3.4043 - val_acc: 0.5029\n",
      "Epoch 110/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0171 - acc: 0.9942 - val_loss: 3.3135 - val_acc: 0.5033\n",
      "Epoch 111/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0151 - acc: 0.9938 - val_loss: 3.3874 - val_acc: 0.5072\n",
      "Epoch 112/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0142 - acc: 0.9956 - val_loss: 3.3851 - val_acc: 0.5039\n",
      "Epoch 113/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0145 - acc: 0.9954 - val_loss: 3.3953 - val_acc: 0.5002\n",
      "Epoch 114/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0112 - acc: 0.9970 - val_loss: 3.4858 - val_acc: 0.5008\n",
      "Epoch 115/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0124 - acc: 0.9974 - val_loss: 3.3965 - val_acc: 0.5076\n",
      "Epoch 116/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0143 - acc: 0.9960 - val_loss: 3.6034 - val_acc: 0.5039\n",
      "Epoch 117/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0213 - acc: 0.9940 - val_loss: 3.4534 - val_acc: 0.5020\n",
      "Epoch 118/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0124 - acc: 0.9966 - val_loss: 3.4773 - val_acc: 0.4999\n",
      "Epoch 119/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0175 - acc: 0.9934 - val_loss: 3.4402 - val_acc: 0.4978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0139 - acc: 0.9950 - val_loss: 3.5022 - val_acc: 0.5018\n",
      "Epoch 121/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0162 - acc: 0.9964 - val_loss: 3.4123 - val_acc: 0.5077\n",
      "Epoch 122/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0117 - acc: 0.9962 - val_loss: 3.3953 - val_acc: 0.5026\n",
      "Epoch 123/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0099 - acc: 0.9972 - val_loss: 3.4752 - val_acc: 0.5021\n",
      "Epoch 124/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0156 - acc: 0.9952 - val_loss: 3.5384 - val_acc: 0.5035\n",
      "Epoch 125/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0115 - acc: 0.9966 - val_loss: 3.5336 - val_acc: 0.5068\n",
      "Epoch 126/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0140 - acc: 0.9956 - val_loss: 3.5933 - val_acc: 0.4987\n",
      "Epoch 127/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0113 - acc: 0.9970 - val_loss: 3.5157 - val_acc: 0.5036\n",
      "Epoch 128/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0133 - acc: 0.9954 - val_loss: 3.3838 - val_acc: 0.5041\n",
      "Epoch 129/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0116 - acc: 0.9960 - val_loss: 3.5761 - val_acc: 0.5052\n",
      "Epoch 130/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0132 - acc: 0.9958 - val_loss: 3.5235 - val_acc: 0.5069\n",
      "Epoch 131/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0123 - acc: 0.9962 - val_loss: 3.5537 - val_acc: 0.5026\n",
      "Epoch 132/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0109 - acc: 0.9962 - val_loss: 3.5517 - val_acc: 0.5055\n",
      "Epoch 133/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0107 - acc: 0.9968 - val_loss: 3.6901 - val_acc: 0.4980\n",
      "Epoch 134/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0096 - acc: 0.9968 - val_loss: 3.5635 - val_acc: 0.5025\n",
      "Epoch 135/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0125 - acc: 0.9958 - val_loss: 3.5515 - val_acc: 0.5030\n",
      "Epoch 136/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0092 - acc: 0.9972 - val_loss: 3.6780 - val_acc: 0.5033\n",
      "Epoch 137/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0083 - acc: 0.9972 - val_loss: 3.6331 - val_acc: 0.5023\n",
      "Epoch 138/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0123 - acc: 0.9954 - val_loss: 3.6082 - val_acc: 0.4999\n",
      "Epoch 139/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0114 - acc: 0.9958 - val_loss: 3.6346 - val_acc: 0.5037\n",
      "Epoch 140/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0119 - acc: 0.9966 - val_loss: 3.6351 - val_acc: 0.5042\n",
      "Epoch 141/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0167 - acc: 0.9956 - val_loss: 3.5634 - val_acc: 0.5054\n",
      "Epoch 142/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0083 - acc: 0.9972 - val_loss: 3.5916 - val_acc: 0.5034\n",
      "Epoch 143/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0110 - acc: 0.9956 - val_loss: 3.5741 - val_acc: 0.5053\n",
      "Epoch 144/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0065 - acc: 0.9982 - val_loss: 3.7339 - val_acc: 0.5048\n",
      "Epoch 145/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0103 - acc: 0.9964 - val_loss: 3.6420 - val_acc: 0.5029\n",
      "Epoch 146/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0113 - acc: 0.9968 - val_loss: 3.6832 - val_acc: 0.5007\n",
      "Epoch 147/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0103 - acc: 0.9970 - val_loss: 3.5948 - val_acc: 0.5013\n",
      "Epoch 148/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0102 - acc: 0.9966 - val_loss: 3.6326 - val_acc: 0.5037\n",
      "Epoch 149/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0087 - acc: 0.9968 - val_loss: 3.6541 - val_acc: 0.5043\n",
      "Epoch 150/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0112 - acc: 0.9968 - val_loss: 3.6387 - val_acc: 0.4997\n",
      "Epoch 151/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0108 - acc: 0.9958 - val_loss: 3.6021 - val_acc: 0.5002\n",
      "Epoch 152/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0095 - acc: 0.9970 - val_loss: 3.6418 - val_acc: 0.5038\n",
      "Epoch 153/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0151 - acc: 0.9962 - val_loss: 3.6930 - val_acc: 0.5027\n",
      "Epoch 154/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0084 - acc: 0.9964 - val_loss: 3.8362 - val_acc: 0.4978\n",
      "Epoch 155/250\n",
      "5000/5000 [==============================] - 12s 2ms/step - loss: 0.0077 - acc: 0.9974 - val_loss: 3.5909 - val_acc: 0.5085\n",
      "Epoch 156/250\n",
      "5000/5000 [==============================] - 12s 2ms/step - loss: 0.0087 - acc: 0.9966 - val_loss: 3.7575 - val_acc: 0.4978\n",
      "Epoch 157/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0124 - acc: 0.9952 - val_loss: 3.7054 - val_acc: 0.5006\n",
      "Epoch 158/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0106 - acc: 0.9962 - val_loss: 3.6999 - val_acc: 0.5061\n",
      "Epoch 159/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0091 - acc: 0.9978 - val_loss: 3.8401 - val_acc: 0.5055\n",
      "Epoch 160/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0101 - acc: 0.9966 - val_loss: 3.6752 - val_acc: 0.5049\n",
      "Epoch 161/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0100 - acc: 0.9966 - val_loss: 3.7660 - val_acc: 0.5005\n",
      "Epoch 162/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0127 - acc: 0.9954 - val_loss: 3.7347 - val_acc: 0.5003\n",
      "Epoch 163/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0087 - acc: 0.9968 - val_loss: 3.7461 - val_acc: 0.5023\n",
      "Epoch 164/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0100 - acc: 0.9966 - val_loss: 3.7516 - val_acc: 0.5012\n",
      "Epoch 165/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0073 - acc: 0.9978 - val_loss: 3.8413 - val_acc: 0.5028\n",
      "Epoch 166/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0085 - acc: 0.9974 - val_loss: 3.7689 - val_acc: 0.5067\n",
      "Epoch 167/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0108 - acc: 0.9968 - val_loss: 3.7277 - val_acc: 0.5050\n",
      "Epoch 168/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0066 - acc: 0.9978 - val_loss: 3.8282 - val_acc: 0.5071\n",
      "Epoch 169/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0090 - acc: 0.9970 - val_loss: 3.8802 - val_acc: 0.5022\n",
      "Epoch 170/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0072 - acc: 0.9980 - val_loss: 3.7296 - val_acc: 0.5063\n",
      "Epoch 171/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0116 - acc: 0.9964 - val_loss: 3.8319 - val_acc: 0.5041\n",
      "Epoch 172/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0108 - acc: 0.9968 - val_loss: 3.8470 - val_acc: 0.5038\n",
      "Epoch 173/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0101 - acc: 0.9962 - val_loss: 3.7993 - val_acc: 0.5077\n",
      "Epoch 174/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.0073 - acc: 0.9978 - val_loss: 3.9445 - val_acc: 0.5072\n",
      "Epoch 175/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0126 - acc: 0.9954 - val_loss: 3.7616 - val_acc: 0.5028\n",
      "Epoch 176/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0104 - acc: 0.9962 - val_loss: 3.8610 - val_acc: 0.5067\n",
      "Epoch 177/250\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.0082 - acc: 0.9970 - val_loss: 3.8504 - val_acc: 0.5038\n",
      "Epoch 178/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0086 - acc: 0.9970 - val_loss: 3.8922 - val_acc: 0.5038\n",
      "Epoch 179/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0065 - acc: 0.9982 - val_loss: 3.8497 - val_acc: 0.5037\n",
      "Epoch 180/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0073 - acc: 0.9970 - val_loss: 3.8609 - val_acc: 0.5047\n",
      "Epoch 181/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0086 - acc: 0.9970 - val_loss: 3.8532 - val_acc: 0.5053\n",
      "Epoch 182/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0070 - acc: 0.9974 - val_loss: 3.8426 - val_acc: 0.5068\n",
      "Epoch 183/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0109 - acc: 0.9964 - val_loss: 3.8550 - val_acc: 0.5023\n",
      "Epoch 184/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0080 - acc: 0.9982 - val_loss: 3.7610 - val_acc: 0.5046\n",
      "Epoch 185/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0088 - acc: 0.9964 - val_loss: 3.8001 - val_acc: 0.5048\n",
      "Epoch 186/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0056 - acc: 0.9978 - val_loss: 3.8429 - val_acc: 0.5057\n",
      "Epoch 187/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0069 - acc: 0.9972 - val_loss: 3.9382 - val_acc: 0.5044\n",
      "Epoch 188/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0096 - acc: 0.9970 - val_loss: 3.8020 - val_acc: 0.5007\n",
      "Epoch 189/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0069 - acc: 0.9970 - val_loss: 3.8432 - val_acc: 0.5041\n",
      "Epoch 190/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0069 - acc: 0.9974 - val_loss: 3.8644 - val_acc: 0.5064\n",
      "Epoch 191/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0080 - acc: 0.9972 - val_loss: 3.9670 - val_acc: 0.5082\n",
      "Epoch 192/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0060 - acc: 0.9978 - val_loss: 4.0257 - val_acc: 0.5024\n",
      "Epoch 193/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0104 - acc: 0.9960 - val_loss: 3.8616 - val_acc: 0.5073\n",
      "Epoch 194/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0080 - acc: 0.9968 - val_loss: 3.8780 - val_acc: 0.5052\n",
      "Epoch 195/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0068 - acc: 0.9976 - val_loss: 3.8711 - val_acc: 0.5032\n",
      "Epoch 196/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0066 - acc: 0.9976 - val_loss: 4.0035 - val_acc: 0.5054\n",
      "Epoch 197/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0070 - acc: 0.9972 - val_loss: 3.9849 - val_acc: 0.5045\n",
      "Epoch 198/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0066 - acc: 0.9976 - val_loss: 4.1291 - val_acc: 0.5055\n",
      "Epoch 199/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0087 - acc: 0.9968 - val_loss: 3.9503 - val_acc: 0.5035\n",
      "Epoch 200/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0077 - acc: 0.9964 - val_loss: 3.9407 - val_acc: 0.5042\n",
      "Epoch 201/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0079 - acc: 0.9974 - val_loss: 3.9313 - val_acc: 0.5109\n",
      "Epoch 202/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0070 - acc: 0.9970 - val_loss: 4.0217 - val_acc: 0.5066\n",
      "Epoch 203/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0057 - acc: 0.9978 - val_loss: 4.1399 - val_acc: 0.5040\n",
      "Epoch 204/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0109 - acc: 0.9962 - val_loss: 4.0251 - val_acc: 0.5059\n",
      "Epoch 205/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0100 - acc: 0.9958 - val_loss: 4.0295 - val_acc: 0.4990\n",
      "Epoch 206/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0086 - acc: 0.9970 - val_loss: 3.9806 - val_acc: 0.5039\n",
      "Epoch 207/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0073 - acc: 0.9976 - val_loss: 4.0218 - val_acc: 0.5066\n",
      "Epoch 208/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0154 - acc: 0.9956 - val_loss: 3.9162 - val_acc: 0.5037\n",
      "Epoch 209/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0075 - acc: 0.9966 - val_loss: 3.9348 - val_acc: 0.5080\n",
      "Epoch 210/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0065 - acc: 0.9970 - val_loss: 4.0390 - val_acc: 0.4995\n",
      "Epoch 211/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0104 - acc: 0.9960 - val_loss: 4.0076 - val_acc: 0.5027\n",
      "Epoch 212/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0069 - acc: 0.9974 - val_loss: 4.1106 - val_acc: 0.5005\n",
      "Epoch 213/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0132 - acc: 0.9946 - val_loss: 3.9199 - val_acc: 0.5059\n",
      "Epoch 214/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0083 - acc: 0.9972 - val_loss: 3.9981 - val_acc: 0.5046\n",
      "Epoch 215/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0057 - acc: 0.9984 - val_loss: 4.1330 - val_acc: 0.5024\n",
      "Epoch 216/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0056 - acc: 0.9972 - val_loss: 4.1262 - val_acc: 0.5045\n",
      "Epoch 217/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0091 - acc: 0.9974 - val_loss: 4.0026 - val_acc: 0.5049\n",
      "Epoch 218/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0064 - acc: 0.9978 - val_loss: 4.0571 - val_acc: 0.5049\n",
      "Epoch 219/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0064 - acc: 0.9978 - val_loss: 4.0422 - val_acc: 0.5053\n",
      "Epoch 220/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0071 - acc: 0.9962 - val_loss: 3.9959 - val_acc: 0.5028\n",
      "Epoch 221/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0054 - acc: 0.9978 - val_loss: 4.0801 - val_acc: 0.5022\n",
      "Epoch 222/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0053 - acc: 0.9980 - val_loss: 4.1145 - val_acc: 0.5036\n",
      "Epoch 223/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0077 - acc: 0.9974 - val_loss: 4.0503 - val_acc: 0.5039\n",
      "Epoch 224/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0096 - acc: 0.9962 - val_loss: 3.9942 - val_acc: 0.5054\n",
      "Epoch 225/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0161 - acc: 0.9950 - val_loss: 3.9407 - val_acc: 0.5058\n",
      "Epoch 226/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0083 - acc: 0.9966 - val_loss: 4.0418 - val_acc: 0.5022\n",
      "Epoch 227/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0061 - acc: 0.9976 - val_loss: 4.0418 - val_acc: 0.5094\n",
      "Epoch 228/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0061 - acc: 0.9978 - val_loss: 4.1104 - val_acc: 0.5073\n",
      "Epoch 229/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0065 - acc: 0.9980 - val_loss: 4.0335 - val_acc: 0.5046\n",
      "Epoch 230/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0084 - acc: 0.9960 - val_loss: 3.9857 - val_acc: 0.5031\n",
      "Epoch 231/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0075 - acc: 0.9968 - val_loss: 4.0161 - val_acc: 0.5069\n",
      "Epoch 232/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0050 - acc: 0.9984 - val_loss: 3.9268 - val_acc: 0.5062\n",
      "Epoch 233/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0115 - acc: 0.9962 - val_loss: 3.9964 - val_acc: 0.5047\n",
      "Epoch 234/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0052 - acc: 0.9974 - val_loss: 4.0108 - val_acc: 0.5030\n",
      "Epoch 235/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0050 - acc: 0.9980 - val_loss: 4.0531 - val_acc: 0.5065\n",
      "Epoch 236/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0068 - acc: 0.9976 - val_loss: 4.0133 - val_acc: 0.5076\n",
      "Epoch 237/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0054 - acc: 0.9978 - val_loss: 4.1227 - val_acc: 0.5045\n",
      "Epoch 238/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0049 - acc: 0.9982 - val_loss: 4.1565 - val_acc: 0.5028\n",
      "Epoch 239/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0074 - acc: 0.9978 - val_loss: 4.1655 - val_acc: 0.5058\n",
      "Epoch 240/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0082 - acc: 0.9974 - val_loss: 4.0994 - val_acc: 0.5067\n",
      "Epoch 241/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0061 - acc: 0.9974 - val_loss: 4.1369 - val_acc: 0.5032\n",
      "Epoch 242/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0043 - acc: 0.9980 - val_loss: 4.2031 - val_acc: 0.5029\n",
      "Epoch 243/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0057 - acc: 0.9976 - val_loss: 4.2072 - val_acc: 0.4998\n",
      "Epoch 244/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0109 - acc: 0.9954 - val_loss: 4.1876 - val_acc: 0.5040\n",
      "Epoch 245/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0057 - acc: 0.9974 - val_loss: 4.1233 - val_acc: 0.5042\n",
      "Epoch 246/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0081 - acc: 0.9962 - val_loss: 4.0861 - val_acc: 0.5004\n",
      "Epoch 247/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0142 - acc: 0.9956 - val_loss: 4.1908 - val_acc: 0.5031\n",
      "Epoch 248/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0068 - acc: 0.9976 - val_loss: 4.1070 - val_acc: 0.5032\n",
      "Epoch 249/250\n",
      "5000/5000 [==============================] - 12s 2ms/step - loss: 0.0070 - acc: 0.9970 - val_loss: 4.0085 - val_acc: 0.5025\n",
      "Epoch 250/250\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0097 - acc: 0.9962 - val_loss: 4.0484 - val_acc: 0.5060\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=250,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))\n",
    "model.save_weights('pre_trained_glove_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
