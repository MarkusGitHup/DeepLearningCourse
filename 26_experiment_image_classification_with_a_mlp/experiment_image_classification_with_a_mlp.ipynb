{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction:-Trying-the-impossible!\" data-toc-modified-id=\"Introduction:-Trying-the-impossible!-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction: Trying the impossible!</a></span></li><li><span><a href=\"#Are-all-libraries-that-are-needed-available?\" data-toc-modified-id=\"Are-all-libraries-that-are-needed-available?-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Are all libraries that are needed available?</a></span></li><li><span><a href=\"#Prepare-an-image-provider-class\" data-toc-modified-id=\"Prepare-an-image-provider-class-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Prepare an image provider class</a></span></li><li><span><a href=\"#Build-a-MLP-model\" data-toc-modified-id=\"Build-a-MLP-model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Build a MLP model</a></span></li><li><span><a href=\"#Train-the-MLP-with-images\" data-toc-modified-id=\"Train-the-MLP-with-images-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Train the MLP with images</a></span></li><li><span><a href=\"#Test-the-final-MLP-on-training-data\" data-toc-modified-id=\"Test-the-final-MLP-on-training-data-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Test the final MLP on training data</a></span></li><li><span><a href=\"#Generating-new-test-images\" data-toc-modified-id=\"Generating-new-test-images-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Generating new test images</a></span></li><li><span><a href=\"#Conclusions\" data-toc-modified-id=\"Conclusions-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Conclusions</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Trying the impossible!\n",
    "\n",
    "A Convolutional Neural Network (NN) consists of two parts. A feature hierarchy with CONV and MAX-Pooling layers and a classificator on top of this feature hierarchy, usually a Multi Layer Perceptron (MLP).\n",
    "\n",
    "But do we really need this feature hierarchy? Why not use images directly as input for a MLP? Never say never. So let's try the \"impossible\": learn to classify images without the feature hierarchy. Just using a MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are all libraries that are needed available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your NumPy version is: 1.13.3\n",
      "Your TensorFlow version is: 1.5.1\n",
      "Your Keras version is: 2.2.3\n",
      "Your OpenCV version is: 3.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import cv2\n",
    "\n",
    "print( \"Your NumPy version is: \" + np.__version__ )\n",
    "print( \"Your TensorFlow version is: \" + tf.__version__)\n",
    "print( \"Your Keras version is: \" + keras.__version__ )\n",
    "print( \"Your OpenCV version is: \" + cv2.__version__ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare an image provider class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isdir, isfile, join\n",
    "\n",
    "IMG_SIZE = (100,100)\n",
    "    \n",
    "class image_provider:\n",
    "    \n",
    "    #\n",
    "    # Traverses all subfolders of the specified root_folder\n",
    "    # and generates a list of the form:\n",
    "    #\n",
    "    # [ [\"data/bikes/jfksdj43.jpg\", \"bikes\",\n",
    "    #   [\"data/cars/bvcnm401.jpg\", \"cars\"],\n",
    "    #   ...\n",
    "    # ]\n",
    "    #\n",
    "    def __init__(self, root_folder):\n",
    "        \n",
    "        self.all_training_items = []\n",
    "       \n",
    "        class_names = \\\n",
    "            [d for d in listdir(root_folder)\n",
    "             if isdir(os.path.join(root_folder,d))]\n",
    "\n",
    "        print(\"Under folder\", root_folder, \"I found the following subfolders/classes:\")\n",
    "        print(class_names)\n",
    "        \n",
    "        nr_classes = len(class_names)\n",
    "        \n",
    "        # For each subfolder ...\n",
    "        for class_id, class_name in enumerate(class_names):\n",
    "            \n",
    "            subfolder_name = root_folder + \"/\" + class_name + \"/\"\n",
    "            \n",
    "            filenames = \\\n",
    "                [subfolder_name + f\n",
    "                 for f in listdir(subfolder_name) if isfile(join(subfolder_name, f))]\n",
    "            \n",
    "            print(\"{} files in subfolder {}\".format(len(filenames), subfolder_name) )\n",
    "            \n",
    "            # For each image filename in current subfolder ...\n",
    "            for filename in filenames:\n",
    "                \n",
    "                teacher_vec = np.zeros( nr_classes )\n",
    "                teacher_vec[class_id] = 1.0\n",
    "                \n",
    "                self.all_training_items.append(\n",
    "                    [filename, class_id, class_name, teacher_vec] )              \n",
    "        \n",
    "        self.nr_images = len(self.all_training_items)\n",
    "        print(\"There are {} images in total available.\".format(self.nr_images))\n",
    "        \n",
    "    \n",
    "    \n",
    "    #   \n",
    "    # Given an absolute filename,\n",
    "    # load the image in using OpenCV,\n",
    "    # then convert it to usual RGB color channel order\n",
    "    # and scale values to be in range [0,1]\n",
    "    #\n",
    "    def load_image(self, absolute_filename):\n",
    "        \n",
    "        image = cv2.imread(absolute_filename)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # invert image\n",
    "        image = 255 - image\n",
    "        \n",
    "        image = cv2.resize(image, IMG_SIZE, interpolation=cv2.INTER_AREA)\n",
    "        image = image * (1.0 / 255.0)\n",
    "        \n",
    "        return image\n",
    "        \n",
    "        \n",
    "       \n",
    "    #\n",
    "    # Return the image from the dataset\n",
    "    # with the specified index\n",
    "    #\n",
    "    def get_specific_image(self, idx):\n",
    "        \n",
    "        image_filename  = self.all_training_items[idx][0]\n",
    "        class_id        = self.all_training_items[idx][1]\n",
    "        class_name      = self.all_training_items[idx][2]\n",
    "        teacher_vec     = self.all_training_items[idx][3]\n",
    "        \n",
    "        image = self.load_image(image_filename)\n",
    "        \n",
    "        return image, class_id, class_name, teacher_vec\n",
    "    \n",
    "    \n",
    "    #\n",
    "    # Return an OpenCV image and the class label\n",
    "    # where the image is chosen randomly from the\n",
    "    # list of all images.\n",
    "    #\n",
    "    def get_random_image(self):\n",
    "        \n",
    "        rnd_idx = np.random.randint(0, self.nr_images)\n",
    "        return self.get_specific_image( rnd_idx )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the image provider class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under folder data_digits I found the following subfolders/classes:\n",
      "['1', '2']\n",
      "10 files in subfolder data_digits/1/\n",
      "10 files in subfolder data_digits/2/\n",
      "There are 20 images in total available.\n"
     ]
    }
   ],
   "source": [
    "my_image_provider = image_provider( \"data_digits\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us retrieve randomly one of the images and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image has type <class 'numpy.ndarray'>\n",
      "image has shape (100, 100, 3)\n",
      "teacher vec: [ 1.  0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3yxTLn6zTLn4zTLl4jfL1P8DzhwfOiVJehUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x205509c3b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image, class_id, class_name, teacher_vec = \\\n",
    "    my_image_provider.get_random_image()\n",
    "print(\"image has type\", type(image))\n",
    "print(\"image has shape\", image.shape)\n",
    "print(\"teacher vec:\", teacher_vec)\n",
    "plt.imshow(image)\n",
    "plt.title(\"One of the images with label {}\".format(class_name))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vectors for this MLP will have length 30000\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 2)                 60002     \n",
      "=================================================================\n",
      "Total params: 60,002\n",
      "Trainable params: 60,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers.core import Dense\n",
    "from keras import backend as K\n",
    "\n",
    "nr_channels = 3\n",
    "input_vec_dim = IMG_SIZE[0] * IMG_SIZE[1] * nr_channels\n",
    "print(\"Input vectors for this MLP will have length\", input_vec_dim)\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(2, activation=\"linear\", input_dim=input_vec_dim))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the MLP with images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained on 0 images so far...\n",
      "Trained on 100 images so far...\n",
      "Trained on 200 images so far...\n",
      "Trained on 300 images so far...\n",
      "Trained on 400 images so far...\n",
      "Trained on 500 images so far...\n",
      "Trained on 600 images so far...\n",
      "Trained on 700 images so far...\n",
      "Trained on 800 images so far...\n",
      "Trained on 900 images so far...\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "NR_TRAIN_IMAGES = 1000\n",
    "\n",
    "for train_img_idx in range(0,NR_TRAIN_IMAGES):\n",
    "    \n",
    "    if train_img_idx % 100 == 0:\n",
    "        print(\"Trained on {} images so far...\".\n",
    "             format(train_img_idx))\n",
    "\n",
    "    # Get a random image from the image provider\n",
    "    image, class_id, class_name, teacher_vec = \\\n",
    "        my_image_provider.get_random_image()\n",
    "    \n",
    "    # Show the training image?\n",
    "    if False:\n",
    "        plt.imshow(image)\n",
    "        plt.title(\"Training image with label {}\".format(class_name))\n",
    "        plt.show()\n",
    "    \n",
    "    # Flatten the 3D input image to a 1D input vector\n",
    "    input_vec = image.flatten()\n",
    "        \n",
    "    #print(\"image has shape\", image.shape)\n",
    "    #print(\"input_vec has shape\", input_vec.shape)\n",
    "    #print(\"teacher_vec is\", teacher_vec)\n",
    "    #print(input_vec)\n",
    "    \n",
    "    input_vec = input_vec.reshape( (1, input_vec.shape[0]) )\n",
    "    teacher_vec = teacher_vec.reshape( (1, teacher_vec.shape[0]) )\n",
    "    #print(\"input_vec has shape\", input_vec.shape)\n",
    "    #print(\"teacher_vec has shape\", teacher_vec.shape)\n",
    "            \n",
    "    model.fit(input_vec, teacher_vec, epochs=1, verbose=0)\n",
    "    \n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Test the final MLP on training data\n",
    "\n",
    "Now let us see how good the trained MLP performs on the same data on which we trained it: the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with training image 0\n",
      "output neuron values are: [[  9.99995530e-01   3.16277146e-06]]\n",
      "predicted: 0 vs. real: 0\n",
      "\n",
      "Testing with training image 1\n",
      "output neuron values are: [[  1.00000012e+00  -2.68220901e-07]]\n",
      "predicted: 0 vs. real: 0\n",
      "\n",
      "Testing with training image 2\n",
      "output neuron values are: [[  9.99997079e-01   2.18302011e-06]]\n",
      "predicted: 0 vs. real: 0\n",
      "\n",
      "Testing with training image 3\n",
      "output neuron values are: [[  9.99999225e-01   5.55068254e-07]]\n",
      "predicted: 0 vs. real: 0\n",
      "\n",
      "Testing with training image 4\n",
      "output neuron values are: [[  9.99994576e-01   3.71783972e-06]]\n",
      "predicted: 0 vs. real: 0\n",
      "\n",
      "Testing with training image 5\n",
      "output neuron values are: [[  9.99983609e-01   1.25207007e-05]]\n",
      "predicted: 0 vs. real: 0\n",
      "\n",
      "Testing with training image 6\n",
      "output neuron values are: [[  9.99999285e-01   6.22123480e-07]]\n",
      "predicted: 0 vs. real: 0\n",
      "\n",
      "Testing with training image 7\n",
      "output neuron values are: [[  1.00000930e+00  -6.52298331e-06]]\n",
      "predicted: 0 vs. real: 0\n",
      "\n",
      "Testing with training image 8\n",
      "output neuron values are: [[  9.99998033e-01   1.36345625e-06]]\n",
      "predicted: 0 vs. real: 0\n",
      "\n",
      "Testing with training image 9\n",
      "output neuron values are: [[  9.99968290e-01   2.30148435e-05]]\n",
      "predicted: 0 vs. real: 0\n",
      "\n",
      "Testing with training image 10\n",
      "output neuron values are: [[ -1.79558992e-06   1.00000131e+00]]\n",
      "predicted: 1 vs. real: 1\n",
      "\n",
      "Testing with training image 11\n",
      "output neuron values are: [[ -8.49366188e-07   1.00000060e+00]]\n",
      "predicted: 1 vs. real: 1\n",
      "\n",
      "Testing with training image 12\n",
      "output neuron values are: [[ -2.37673521e-06   1.00000167e+00]]\n",
      "predicted: 1 vs. real: 1\n",
      "\n",
      "Testing with training image 13\n",
      "output neuron values are: [[ -1.40815973e-06   1.00000107e+00]]\n",
      "predicted: 1 vs. real: 1\n",
      "\n",
      "Testing with training image 14\n",
      "output neuron values are: [[ -1.50501728e-06   1.00000107e+00]]\n",
      "predicted: 1 vs. real: 1\n",
      "\n",
      "Testing with training image 15\n",
      "output neuron values are: [[ -2.07871199e-06   1.00000179e+00]]\n",
      "predicted: 1 vs. real: 1\n",
      "\n",
      "Testing with training image 16\n",
      "output neuron values are: [[ -2.14576721e-06   1.00000167e+00]]\n",
      "predicted: 1 vs. real: 1\n",
      "\n",
      "Testing with training image 17\n",
      "output neuron values are: [[ -5.96046448e-06   1.00000429e+00]]\n",
      "predicted: 1 vs. real: 1\n",
      "\n",
      "Testing with training image 18\n",
      "output neuron values are: [[ -2.11596489e-06   1.00000155e+00]]\n",
      "predicted: 1 vs. real: 1\n",
      "\n",
      "Testing with training image 19\n",
      "output neuron values are: [[ -8.12113285e-07   1.00000060e+00]]\n",
      "predicted: 1 vs. real: 1\n",
      "\n",
      "---\n",
      "Correctly classified 20 of 20 images.\n"
     ]
    }
   ],
   "source": [
    "# No images correctly classified so far\n",
    "correctly_classified = 0\n",
    "\n",
    "for img_idx in range(0, my_image_provider.nr_images):\n",
    "    \n",
    "    print(\"\\nTesting with training image {}\".\n",
    "             format(img_idx))\n",
    "\n",
    "    # Get image from image provider\n",
    "    image, gt_class_id, gt_class_name, teacher_vec = \\\n",
    "        my_image_provider.get_specific_image( img_idx )\n",
    "    \n",
    "    # Flatten the 3D input image to a 1D input vector\n",
    "    input_vec = image.flatten()\n",
    "    \n",
    "    # Inputs for predit method have to be 2D\n",
    "    input_vec = input_vec.reshape( (1, input_vec.shape[0]) )\n",
    "    teacher_vec = teacher_vec.reshape( (1, teacher_vec.shape[0]) )\n",
    "    \n",
    "    # Let the MLP predict the class!\n",
    "    neuron_outputs = model.predict(input_vec)\n",
    "    print(\"output neuron values are:\", neuron_outputs)\n",
    "    \n",
    "    # Get final prediction result:\n",
    "    # Which of the n output neurons has the largest output?\n",
    "    predicted_class_id = np.argmax(neuron_outputs.reshape(-1))\n",
    "        \n",
    "    # Show comparison of predicted vs. ground-truth label\n",
    "    print(\"predicted: {} vs. real: {}\".\n",
    "          format(predicted_class_id, gt_class_id))\n",
    "    \n",
    "    # Compute correct classification rate\n",
    "    if predicted_class_id==gt_class_id:\n",
    "        correctly_classified += 1\n",
    "        \n",
    "print(\"\\n---\\nCorrectly classified {} of {} images.\".\n",
    "      format(correctly_classified, my_image_provider.nr_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Generating new test images\n",
    "\n",
    "In order to generate new test images, we just shift the digits some pixels to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested on 0 test images so far...\n",
      "Tested on 100 test images so far...\n",
      "Tested on 200 test images so far...\n",
      "Tested on 300 test images so far...\n",
      "Tested on 400 test images so far...\n",
      "Tested on 500 test images so far...\n",
      "Tested on 600 test images so far...\n",
      "Tested on 700 test images so far...\n",
      "Tested on 800 test images so far...\n",
      "Tested on 900 test images so far...\n",
      "\n",
      "---\n",
      "Correctly classified 488 of 1000 images.\n"
     ]
    }
   ],
   "source": [
    "# No images correctly classified so far\n",
    "correctly_classified = 0\n",
    "\n",
    "NR_TEST_IMAGES = 1000\n",
    "\n",
    "for test_img_idx in range(0,NR_TEST_IMAGES):\n",
    "    \n",
    "    if test_img_idx % 100 == 0:\n",
    "        print(\"Tested on {} test images so far...\".\n",
    "             format(test_img_idx))\n",
    "    \n",
    "    # Get a random image from the image provider\n",
    "    image, class_id, class_name, teacher_vec = \\\n",
    "        my_image_provider.get_random_image()\n",
    "        \n",
    "    # Shift this original image a little bit to the right\n",
    "    # in order to generate a new random image\n",
    "    image2 = np.zeros( image.shape )\n",
    "    MIN_SHIFT = 1\n",
    "    MAX_SHIFT = 10\n",
    "    rnd_shift = np.random.randint(MIN_SHIFT, MAX_SHIFT)\n",
    "    nr_rows = image.shape[0]\n",
    "    nr_cols = image.shape[1]\n",
    "    for row_idx in range(0,nr_rows):\n",
    "        image2[row_idx][rnd_shift:] = image[row_idx][0:nr_cols-rnd_shift]\n",
    "        \n",
    "    # Show the original and the new image?\n",
    "    if False:\n",
    "        plt.imshow(image)\n",
    "        plt.title(\"Testing image with label {}\".format(class_name))\n",
    "        plt.show()\n",
    "        plt.imshow(image2)\n",
    "        plt.title(\"Testing image with label {}\".format(class_name))\n",
    "        plt.show()\n",
    "    \n",
    "    # Prepare input for the MLP\n",
    "    input_vec = image.flatten()    \n",
    "    input_vec = input_vec.reshape( (1, input_vec.shape[0]) )\n",
    "    teacher_vec = teacher_vec.reshape( (1, teacher_vec.shape[0]) )\n",
    "            \n",
    "    # Let the MLP predict the class!\n",
    "    neuron_outputs = model.predict(input_vec)\n",
    "    #print(\"output neuron values are:\", neuron_outputs)\n",
    "    \n",
    "    # Get final prediction result:\n",
    "    # Which of the n output neurons has the largest output?\n",
    "    predicted_class_id = np.argmax(neuron_outputs.reshape(-1))\n",
    "        \n",
    "    # Show comparison of predicted vs. ground-truth label\n",
    "    #print(\"predicted: {} vs. real: {}\".\n",
    "    #      format(predicted_class_id, gt_class_id))\n",
    "    \n",
    "    # Compute correct classification rate\n",
    "    if predicted_class_id==gt_class_id:\n",
    "        correctly_classified += 1\n",
    "        \n",
    "print(\"\\n---\\nCorrectly classified {} of {} images.\".\n",
    "      format(correctly_classified, NR_TEST_IMAGES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "The test of this MLP for image classification shows.\n",
    "\n",
    "It performs perfectly on the training data, since it used it huge sets of parameters to directly map the few input images to the desired output values.\n",
    "\n",
    "However, on new images (just translated variants of the original image), the MLP is not besser than randomly guessing the class of the image.\n",
    "\n",
    "By this we show that image classification with a MLP is not a good idea. Instead, we need another model that is able to come up with a representation of the image that is (mostly) invariant to translations (and all the other transformations, e.g. rotation).\n",
    "\n",
    "This will lead us to the Convolutional Neural Networks (CNN).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
