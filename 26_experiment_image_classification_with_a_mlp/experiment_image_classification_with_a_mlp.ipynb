{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction:-Trying-the-impossible!\" data-toc-modified-id=\"Introduction:-Trying-the-impossible!-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction: Trying the impossible!</a></span></li><li><span><a href=\"#Are-all-libraries-that-are-needed-available?\" data-toc-modified-id=\"Are-all-libraries-that-are-needed-available?-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Are all libraries that are needed available?</a></span></li><li><span><a href=\"#Prepare-an-image-provider-class\" data-toc-modified-id=\"Prepare-an-image-provider-class-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Prepare an image provider class</a></span></li><li><span><a href=\"#Build-a-MLP-model\" data-toc-modified-id=\"Build-a-MLP-model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Build a MLP model</a></span></li><li><span><a href=\"#Train-the-MLP-with-images\" data-toc-modified-id=\"Train-the-MLP-with-images-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Train the MLP with images</a></span></li><li><span><a href=\"#Test-the-final-MLP-on-training-data\" data-toc-modified-id=\"Test-the-final-MLP-on-training-data-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Test the final MLP on training data</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Trying the impossible!\n",
    "\n",
    "A Convolutional Neural Network (NN) consists of two parts. A feature hierarchy with CONV and MAX-Pooling layers and a classificator on top of this feature hierarchy, usually a Multi Layer Perceptron (MLP).\n",
    "\n",
    "But do we really need this feature hierarchy? Why not use images directly as input for a MLP? Never say never. So let's try the \"impossible\": learn to classify images without the feature hierarchy. Just using a MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are all libraries that are needed available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your NumPy version is: 1.13.3\n",
      "Your TensorFlow version is: 1.5.1\n",
      "Your Keras version is: 2.2.3\n",
      "Your OpenCV version is: 3.4.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import cv2\n",
    "\n",
    "print( \"Your NumPy version is: \" + np.__version__ )\n",
    "print( \"Your TensorFlow version is: \" + tf.__version__)\n",
    "print( \"Your Keras version is: \" + keras.__version__ )\n",
    "print( \"Your OpenCV version is: \" + cv2.__version__ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare an image provider class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isdir, isfile, join\n",
    "\n",
    "IMG_SIZE = (100,100)\n",
    "    \n",
    "class image_provider:\n",
    "    \n",
    "    #\n",
    "    # Traverses all subfolders of the specified root_folder\n",
    "    # and generates a list of the form:\n",
    "    #\n",
    "    # [ [\"data/bikes/jfksdj43.jpg\", \"bikes\",\n",
    "    #   [\"data/cars/bvcnm401.jpg\", \"cars\"],\n",
    "    #   ...\n",
    "    # ]\n",
    "    #\n",
    "    def __init__(self, root_folder):\n",
    "        \n",
    "        self.all_images = []\n",
    "       \n",
    "        class_names = \\\n",
    "            [d for d in listdir(root_folder)\n",
    "             if isdir(os.path.join(root_folder,d))]\n",
    "\n",
    "        print(\"Under folder\", root_folder, \"I found the following subfolders/classes:\")\n",
    "        print(class_names)\n",
    "        \n",
    "        nr_classes = len(class_names)\n",
    "        \n",
    "        # For each subfolder ...\n",
    "        for class_id, class_name in enumerate(class_names):\n",
    "            \n",
    "            subfolder_name = root_folder + \"/\" + class_name + \"/\"\n",
    "            \n",
    "            filenames = \\\n",
    "                [subfolder_name + f\n",
    "                 for f in listdir(subfolder_name) if isfile(join(subfolder_name, f))]\n",
    "            \n",
    "            print(\"{} files in subfolder {}\".format(len(filenames), subfolder_name) )\n",
    "            \n",
    "            # For each image filename in current subfolder ...\n",
    "            for filename in filenames:\n",
    "                \n",
    "                teacher_vec = np.zeros( nr_classes )\n",
    "                teacher_vec[class_id] = 1.0\n",
    "                \n",
    "                self.all_images.append( [filename, class_id, class_name, teacher_vec] )              \n",
    "        \n",
    "        self.nr_images = len(self.all_images)\n",
    "        print(\"There are {} images in total available.\".format(self.nr_images))\n",
    "        \n",
    "    \n",
    "    \n",
    "    #   \n",
    "    # Given an absolute filename,\n",
    "    # load the image in using OpenCV,\n",
    "    # then convert it to usual RGB color channel order\n",
    "    # and scale values to be in range [0,1]\n",
    "    #\n",
    "    def load_image(self, absolute_filename):\n",
    "        \n",
    "        image = cv2.imread(absolute_filename)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # invert image\n",
    "        image = 255 - image\n",
    "        \n",
    "        image = cv2.resize(image, IMG_SIZE, interpolation=cv2.INTER_AREA)\n",
    "        image = image * (1.0 / 255.0)\n",
    "        \n",
    "        return image\n",
    "        \n",
    "        \n",
    "       \n",
    "    #\n",
    "    # Return the image from the dataset\n",
    "    # with the specified index\n",
    "    #\n",
    "    def get_specific_image(self, idx):\n",
    "        \n",
    "        image_filename  = self.all_images[idx][0]\n",
    "        class_id        = self.all_images[idx][1]\n",
    "        class_name      = self.all_images[idx][2]\n",
    "        teacher_vec     = self.all_images[idx][3]\n",
    "        \n",
    "        image = self.load_image(image_filename)\n",
    "        \n",
    "        return image, class_id, class_name, teacher_vec\n",
    "    \n",
    "    \n",
    "    #\n",
    "    # Return an OpenCV image and the class label\n",
    "    # where the image is chosen randomly from the\n",
    "    # list of all images.\n",
    "    #\n",
    "    def get_random_image(self):\n",
    "        \n",
    "        rnd_idx = np.random.randint(0, self.nr_images)\n",
    "        return self.get_specific_image( rnd_idx )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the image provider class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under folder data_digits I found the following subfolders/classes:\n",
      "['1', '2']\n",
      "10 files in subfolder data_digits/1/\n",
      "10 files in subfolder data_digits/2/\n",
      "There are 20 images in total available.\n"
     ]
    }
   ],
   "source": [
    "my_image_provider = image_provider( \"data_digits\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us retrieve randomly one of the images and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image has type <class 'numpy.ndarray'>\n",
      "image has shape (100, 100, 3)\n",
      "teacher vec: [ 1.  0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nysVvlikXv1mmXPxmmfp/ZEjxS1Y1O0IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a668ff12e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image, class_id, class_name, teacher_vec = \\\n",
    "    my_image_provider.get_random_image()\n",
    "print(\"image has type\", type(image))\n",
    "print(\"image has shape\", image.shape)\n",
    "print(\"teacher vec:\", teacher_vec)\n",
    "plt.imshow(image)\n",
    "plt.title(\"One of the images with label {}\".format(class_name))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vectors for this MLP will have length 30000\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 2)                 60002     \n",
      "=================================================================\n",
      "Total params: 60,002\n",
      "Trainable params: 60,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers.core import Dense\n",
    "from keras import backend as K\n",
    "\n",
    "nr_channels = 3\n",
    "input_vec_dim = IMG_SIZE[0] * IMG_SIZE[1] * nr_channels\n",
    "print(\"Input vectors for this MLP will have length\", input_vec_dim)\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(2, activation=\"linear\", input_dim=input_vec_dim))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the MLP with images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained on 0 images so far...\n",
      "Trained on 100 images so far...\n",
      "Trained on 200 images so far...\n",
      "Trained on 300 images so far...\n",
      "Trained on 400 images so far...\n",
      "Trained on 500 images so far...\n",
      "Trained on 600 images so far...\n",
      "Trained on 700 images so far...\n",
      "Trained on 800 images so far...\n",
      "Trained on 900 images so far...\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "NR_TEST_IMAGES = 1000\n",
    "\n",
    "for test_img_nr in range(0,NR_TEST_IMAGES):\n",
    "    \n",
    "    if test_img_nr % 100 == 0:\n",
    "        print(\"Trained on {} images so far...\".\n",
    "             format(test_img_nr))\n",
    "\n",
    "    # Get a random image from the image provider\n",
    "    image, class_id, class_name, teacher_vec = \\\n",
    "        my_image_provider.get_random_image()\n",
    "    \n",
    "    # Show the training image?\n",
    "    if False:\n",
    "        plt.imshow(image)\n",
    "        plt.title(\"Training image with label {}\".format(class_name))\n",
    "        plt.show()\n",
    "    \n",
    "    # Flatten the 3D input image to a 1D input vector\n",
    "    input_vec = image.flatten()\n",
    "        \n",
    "    #print(\"image has shape\", image.shape)\n",
    "    #print(\"input_vec has shape\", input_vec.shape)\n",
    "    #print(\"teacher_vec is\", teacher_vec)\n",
    "    #print(input_vec)\n",
    "    \n",
    "    input_vec = input_vec.reshape( (1, input_vec.shape[0]) )\n",
    "    teacher_vec = teacher_vec.reshape( (1, teacher_vec.shape[0]) )\n",
    "    #print(\"input_vec has shape\", input_vec.shape)\n",
    "    #print(\"teacher_vec has shape\", teacher_vec.shape)\n",
    "            \n",
    "    model.fit(input_vec, teacher_vec, epochs=1, verbose=0)\n",
    "    \n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Test the final MLP on training data\n",
    "\n",
    "Now let us see how good the trained MLP performs on the same data on which we trained it: the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with training image 0\n",
      "output neuron values are: [[  1.00000250e+00   7.45058060e-08]]\n",
      "predicted: 0 vs. real: 0\n",
      "\n",
      "Testing with training image 1\n",
      "output neuron values are: [[  9.99998093e-01  -1.19209290e-07]]\n",
      "predicted: 0 vs. real: 0\n",
      "\n",
      "Testing with training image 2\n",
      "output neuron values are: [[  9.99998093e-01  -7.07805157e-08]]\n",
      "predicted: 0 vs. real: 0\n",
      "\n",
      "Testing with training image 3\n",
      "output neuron values are: [[  9.99998212e-01  -1.49011612e-08]]\n",
      "predicted: 0 vs. real: 0\n",
      "\n",
      "Testing with training image 4\n",
      "output neuron values are: [[  9.99996424e-01  -5.21540642e-08]]\n",
      "predicted: 0 vs. real: 0\n",
      "\n",
      "Testing with training image 5\n",
      "output neuron values are: [[  1.00000000e+00   9.31322575e-08]]\n",
      "predicted: 0 vs. real: 0\n",
      "\n",
      "Testing with training image 6\n",
      "output neuron values are: [[  9.99999285e-01  -2.98023224e-08]]\n",
      "predicted: 0 vs. real: 0\n",
      "\n",
      "Testing with training image 7\n",
      "output neuron values are: [[  9.99961257e-01  -1.02072954e-06]]\n",
      "predicted: 0 vs. real: 0\n",
      "\n",
      "Testing with training image 8\n",
      "output neuron values are: [[  1.00000060e+00   2.98023224e-08]]\n",
      "predicted: 0 vs. real: 0\n",
      "\n",
      "Testing with training image 9\n",
      "output neuron values are: [[  1.00001264e+00   3.65078449e-07]]\n",
      "predicted: 0 vs. real: 0\n",
      "\n",
      "Testing with training image 10\n",
      "output neuron values are: [[  2.68220901e-07   1.00000000e+00]]\n",
      "predicted: 1 vs. real: 1\n",
      "\n",
      "Testing with training image 11\n",
      "output neuron values are: [[ -1.64657831e-06   9.99999881e-01]]\n",
      "predicted: 1 vs. real: 1\n",
      "\n",
      "Testing with training image 12\n",
      "output neuron values are: [[ -1.86264515e-07   9.99999940e-01]]\n",
      "predicted: 1 vs. real: 1\n",
      "\n",
      "Testing with training image 13\n",
      "output neuron values are: [[ -8.41915607e-07   9.99999940e-01]]\n",
      "predicted: 1 vs. real: 1\n",
      "\n",
      "Testing with training image 14\n",
      "output neuron values are: [[ -7.89761543e-07   1.00000000e+00]]\n",
      "predicted: 1 vs. real: 1\n",
      "\n",
      "Testing with training image 15\n",
      "output neuron values are: [[ -1.04308128e-06   9.99999881e-01]]\n",
      "predicted: 1 vs. real: 1\n",
      "\n",
      "Testing with training image 16\n",
      "output neuron values are: [[ -5.58793545e-07   1.00000000e+00]]\n",
      "predicted: 1 vs. real: 1\n",
      "\n",
      "Testing with training image 17\n",
      "output neuron values are: [[ -1.49011612e-07   9.99999940e-01]]\n",
      "predicted: 1 vs. real: 1\n",
      "\n",
      "Testing with training image 18\n",
      "output neuron values are: [[ -2.16066837e-07   1.00000000e+00]]\n",
      "predicted: 1 vs. real: 1\n",
      "\n",
      "Testing with training image 19\n",
      "output neuron values are: [[ -1.19209290e-07   1.00000000e+00]]\n",
      "predicted: 1 vs. real: 1\n",
      "\n",
      "---\n",
      "Correctly classified 20 of 20 images.\n"
     ]
    }
   ],
   "source": [
    "# No images correctly classified so far\n",
    "correctly_classified = 0\n",
    "\n",
    "for img_nr in range(0, my_image_provider.nr_images):\n",
    "    \n",
    "    print(\"\\nTesting with training image {}\".\n",
    "             format(img_nr))\n",
    "\n",
    "    # Get a random image from the image provider\n",
    "    image, gt_class_id, gt_class_name, teacher_vec = \\\n",
    "        my_image_provider.get_specific_image( img_nr )\n",
    "    \n",
    "    # Flatten the 3D input image to a 1D input vector\n",
    "    input_vec = image.flatten()\n",
    "    \n",
    "    # Inputs for predit method have to be 2D\n",
    "    input_vec = input_vec.reshape( (1, input_vec.shape[0]) )\n",
    "    teacher_vec = teacher_vec.reshape( (1, teacher_vec.shape[0]) )\n",
    "    \n",
    "    # Let the MLP predict the class!\n",
    "    neuron_outputs = model.predict(input_vec)\n",
    "    print(\"output neuron values are:\", neuron_outputs)\n",
    "    \n",
    "    # Get final prediction result:\n",
    "    # Which of the n output neurons has the largest output?\n",
    "    predicted_class_id = np.argmax(neuron_outputs.reshape(-1))\n",
    "        \n",
    "    # Show comparison of predicted vs. ground-truth label\n",
    "    print(\"predicted: {} vs. real: {}\".\n",
    "          format(predicted_class_id, gt_class_id))\n",
    "    \n",
    "    # Compute correct classification rate\n",
    "    if predicted_class_id==gt_class_id:\n",
    "        correctly_classified += 1\n",
    "        \n",
    "print(\"\\n---\\nCorrectly classified {} of {} images.\".\n",
    "      format(correctly_classified, my_image_provider.nr_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
